import numpy as np
import pandas as pd
from scipy.stats import jarque_bera
from statsmodels.tsa.stattools import adfuller
import matplotlib.pyplot as plt
import statsmodels.stats.diagnostic as sm
import statsmodels.api as smi
import datetime as dt

#è¿å¤–ç½‘
import os
os.environ['HTTP_PROXY'] = 'http://127.0.0.1:7890'
os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'

# import research as rs

#%%
#æ³¨ï¼šä¹¦é‡Œæ˜¯ç”¨python2å†™çš„ä»£ç ã€‚ç°åœ¨ç”¨çš„æ˜¯python3ï¼Œæœ‰æŒºå¤šç»†èŠ‚æ˜¯ä¸ä¸€æ ·çš„ã€‚

#ä¸€äº›ä¹¦é‡Œç”¨åˆ°çš„å‡½æ•°,ä¹¦é‡Œæ˜¯åŸºäºpython2çš„ï¼Œéœ€è¦è½¬ä¸ºpython3ï¼Œ
#ä½¿ç”¨å¤šæ ¸cpuæ‰§è¡Œå‡½æ•°
# def mpPandasObj(func,pdObj,numThreads=24,mpBatches=1,linMols=True,**kargs): 
#     ''' 
#     Parallelize jobs, return a DataFrame or Series 
#     + func: function to be parallelized. Returns a DataFrame 
#     + pdObj[0]: Name of argument used to pass the molecule 
#     + pdObj[1]: List of atoms that will be grouped into molecules 
#     + kargs: any other argument needed by func
#     Example: df1=mpPandasObj(func,(â€™moleculeâ€™,df0.index),24,**kargs)
#     '''
#     import pandas as pd 
#     if linMols:
#         parts=linParts(len(pdObj[1]),numThreads*mpBatches) 
#     else:
#         parts=nestedParts(len(pdObj[1]),numThreads*mpBatches) 
#     jobs=[] 
#     for i in range(1,len(parts)): 
#         job={pdObj[0]:pdObj[1][parts[i-1]:parts[i]],'func':func} 
#         job.update(kargs) 
#         jobs.append(job) 
#     if numThreads==1:
#         out=processJobs_(jobs) 
#     else:
#         out=processJobs(jobs,numThreads=numThreads) 
#     if isinstance(out[0],pd.DataFrame):
#         df0=pd.DataFrame() 
#     elif isinstance(out[0],pd.Series):
#         df0=pd.Series() 
#     else:
#         return out 
#     for i in out:
#         df0=df0.append(i) 
#     return df0.sort_index()

# #mpPandasObjé™„å±å‡½æ•°1
# def nestedParts(numAtoms,numThreads,upperTriang=False):
#     # partition of atoms with an inner loop
#     parts,numThreads_=[0],min(numThreads,numAtoms)
#     for num in xrange(numThreads_):
#         part=1 + 4*(parts[-1]**2+parts[-1]+numAtoms*(numAtoms+1.)/numThreads_)
#         part=(-1+part**.5)/2.
#         parts.append(part)
#     parts=np.round(parts).astype(int)
#     if upperTriang: # the first rows are the heaviest
#         parts=np.cumsum(np.diff(parts)[::-1])
#         parts=np.append(np.array([0]),parts)
#     return parts

# #mpPandasObjé™„å±å‡½æ•°2
# def linParts(numAtoms,numThreads):
#     # partition of atoms with a single loop
#     parts=np.linspace(0,numAtoms,min(numThreads,numAtoms)+1)
#     parts=np.ceil(parts).astype(int)
#     return parts

#ä¸‹é¢æ˜¯è½¬ä¸ºpython3ç‰ˆæœ¬çš„ä»£ç 
import numpy as np
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
import multiprocessing

# ==============================
# 1. linParts: çº¿æ€§å‡åŒ€åˆ†å—
# ==============================
def linParts(numAtoms, numThreads):
    """å°† numAtoms ä¸ªåŸå­å‡åŒ€åˆ’åˆ†ä¸º numThreads ä»½ï¼ˆçº¿æ€§åˆ†å—ï¼‰"""
    # è‡³å°‘ 1 ä»½ï¼Œæœ€å¤š numAtoms ä»½
    n_parts = min(numThreads, numAtoms)
    parts = np.linspace(0, numAtoms, n_parts + 1)
    parts = np.ceil(parts).astype(int)
    return parts


# ==============================
# 2. nestedParts: éå‡åŒ€åˆ†å—ï¼ˆç”¨äºè®¡ç®—è´Ÿè½½ä¸å‡åœºæ™¯ï¼‰
# ==============================
def nestedParts(numAtoms, numThreads, upperTriang=False):
    """
    åµŒå¥—åˆ†å—ï¼šå‰é¢çš„å—æ›´å¤§ï¼ˆé€‚ç”¨äºå‰æ®µè®¡ç®—æ›´é‡çš„æƒ…å†µï¼‰
    """
    parts = [0]
    numThreads_ = min(numThreads, numAtoms)
    
    for num in range(numThreads_):  # â† Python 3: xrange â†’ range
        # æ³¨æ„ï¼šåŸå…¬å¼ä¸­çš„é™¤æ³•åœ¨ Python 3 ä¸­å·²æ˜¯ float é™¤æ³•
        part = 1 + 4 * (parts[-1]**2 + parts[-1] + numAtoms * (numAtoms + 1.) / numThreads_)
        part = (-1 + np.sqrt(part)) / 2.
        parts.append(part)
    
    parts = np.round(parts).astype(int)
    
    if upperTriang:
        # åè½¬å—å¤§å°ï¼šç¬¬ä¸€ä¸ªå—æœ€é‡
        diffs = np.diff(parts)[::-1]
        parts = np.cumsum(diffs)
        parts = np.concatenate([[0], parts])
    
    return parts


# ==============================
# 3. è¾…åŠ©å‡½æ•°ï¼šæ‰§è¡Œå•ä¸ª job
# ==============================
def _run_job(job_dict):
    """è¿è¡Œå•ä¸ªä»»åŠ¡ï¼šè°ƒç”¨ func(**job_dict)"""
    func = job_dict.pop('func')
    return func(**job_dict)


# ==============================
# 4. å•çº¿ç¨‹æ‰§è¡Œå™¨
# ==============================
def processJobs_(jobs):
    """å•çº¿ç¨‹é¡ºåºæ‰§è¡Œä»»åŠ¡ï¼ˆç”¨äºè°ƒè¯•æˆ– numThreads=1ï¼‰"""
    return [_run_job(job) for job in jobs]


# ==============================
# 5. å¤šçº¿ç¨‹æ‰§è¡Œå™¨
# ==============================
def processJobs(jobs, numThreads=24):
    """å¤šçº¿ç¨‹å¹¶è¡Œæ‰§è¡Œä»»åŠ¡"""
    if not jobs:
        return []
    
    max_workers = min(numThreads, multiprocessing.cpu_count(), len(jobs))
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(_run_job, job) for job in jobs]
        results = [f.result() for f in futures]
    return results


# ==============================
# 6. ä¸»å‡½æ•°ï¼šmpPandasObjï¼ˆPython 3 å…¼å®¹ç‰ˆï¼‰
# ==============================
def mpPandasObj(func, pdObj, numThreads=24, mpBatches=1, linMols=True, **kargs):
    """
    å¹¶è¡ŒåŒ–å¤„ç† pandas å¯¹è±¡ï¼ˆDataFrame/Seriesï¼‰ï¼Œè¿”å›åˆå¹¶åçš„ç»“æœã€‚
    ä¸é€‚ç”¨äºï¼šéœ€è¦èšåˆæ‰€æœ‰æ•°æ®æ‰èƒ½è®¡ç®—çš„å…¨å±€ç»Ÿè®¡é‡ï¼ˆå¦‚æ•´ä½“ mean/std/corrï¼‰
    æ˜¯å°†æ¯ä¸ªâ€œmoleculeâ€ç‹¬ç«‹äº§å‡ºä¸€ä¸ªç»“æœ
    
    å‚æ•°:
        func: è¦å¹¶è¡ŒåŒ–çš„å‡½æ•°ï¼Œå¿…é¡»æ¥å—ä¸€ä¸ªåä¸º pdObj[0] çš„å‚æ•°ï¼ˆå¦‚ 'molecule'ï¼‰
        pdObj: tuple (arg_name: str, atoms: array-like)ï¼Œä¾‹å¦‚ ('molecule', df.index)
        numThreads: å¹¶è¡Œçº¿ç¨‹æ•°
        mpBatches: æ‰¹æ¬¡æ•°ï¼ˆå¢å¤§å¯æé«˜è´Ÿè½½å‡è¡¡ï¼‰
        linMols: True=çº¿æ€§åˆ†å—ï¼ŒFalse=åµŒå¥—åˆ†å—
        **kargs: ä¼ é€’ç»™ func çš„å…¶ä»–å‚æ•°
    
    ç¤ºä¾‹:
        result = mpPandasObj(my_func, ('molecule', df.index), numThreads=8, data=df, clf=clf)
    """
    arg_name, atoms = pdObj
    num_atoms = len(atoms)

    # åˆ†å—
    if linMols:
        parts = linParts(num_atoms, numThreads * mpBatches)
    else:
        parts = nestedParts(num_atoms, numThreads * mpBatches)

    # æ„å»ºä»»åŠ¡åˆ—è¡¨
    jobs = []
    for i in range(1, len(parts)):
        # æ¯ä¸ªä»»åŠ¡åŒ…å«ï¼šåˆ†å­ï¼ˆç´¢å¼•å­é›†ï¼‰+ å‡½æ•° + å…¶ä»–å‚æ•°
        job = {
            arg_name: atoms[parts[i-1]:parts[i]],
            'func': func
        }
        job.update(kargs)
        jobs.append(job)

    # æ‰§è¡Œä»»åŠ¡
    if numThreads == 1:
        out = processJobs_(jobs)
    else:
        out = processJobs(jobs, numThreads=numThreads)

    # åˆå¹¶ç»“æœ
    if not out:
        raise ValueError("No results returned from parallel jobs.")

    first_result = out[0]
    if isinstance(first_result, pd.DataFrame):
        result = pd.concat(out, axis=0)
    elif isinstance(first_result, pd.Series):
        result = pd.concat(out, axis=0)
    else:
        # é pandas å¯¹è±¡ï¼Œç›´æ¥è¿”å›åˆ—è¡¨
        return out

    return result.sort_index()


#%%
#2-5ç« æ˜¯æ•°æ®åˆ†æã€‚å¯¹è·å¾—åˆ°çš„æ•°æ®è¿›è¡Œåˆ†æï¼Œæ¸…æ´—ï¼Œå¤„ç†è‡³å¯ç”¨
#AFML ç¬¬äºŒç« 
##########åˆ¶é€ åŸºç¡€æ•°æ®
def create_price_data(start_price: float = 1000.00,
                      mu: float = .0,
                      var: float = 1.0,
                      n_samples: int = 1000000):
                      


    i = np.random.normal(mu, var, n_samples)
    df0 = pd.date_range(periods=n_samples,
                        freq=pd.tseries.offsets.Minute(),
                        end=pd.Timestamp.now())
                        
    X = pd.Series(i, index=df0, name = "close").to_frame()
    X.close.iat[0] = start_price
    X.cumsum().plot.line()
    return X.cumsum()
df=create_price_data()
df.to_csv(r'D:\Git\book\ASML\dollar_bars.csv')   #åˆ¶é€ åŸºç¡€çš„æ•°æ®
df=create_price_data()
df.to_csv(r'D:\Git\book\ASML\volume_bars.csv')   #åˆ¶é€ åŸºç¡€çš„æ•°æ®
df=create_price_data()
df.to_csv(r'D:\Git\book\ASML\tick_bars.csv')   #åˆ¶é€ åŸºç¡€çš„æ•°æ®






################# æŸ¥è¯¢ä¸æ­£æ€åˆ†å¸ƒçš„ç›¸ä¼¼ç¨‹åº¦â€”â€”é€šè¿‡ååº¦å’Œå³°å€¼çš„æ£€éªŒã€‚è¿™æ ·å°±ä¸éœ€è¦ç”»å›¾äº†
from scipy import stats

p(stats.jarque_bera(dollar['close'].pct_change().dropna())[0],
stats.jarque_bera(volume['close'].pct_change().dropna())[0],
stats.jarque_bera(tick['close'].pct_change().dropna())[0])




'''
2.1 On a series of E-mini S&P 500 futures tick data:
(a) Form tick, volume, and dollar bars. Use the ETF trick to deal with the roll.
(b) Count the number of bars produced by tick, volume, and dollar bars on a
weekly basis. Plot a time series of that bar count. What bar type produces
the most stable weekly count? Why?
(c) Compute the serial correlation of returns for the three bar types. What bar
method has the lowest serial correlation?
(d) Partition the bar series into monthly subsets. Compute the variance of returns
for every subset of every bar type. Compute the variance of those variances.
What method exhibits the smallest variance of variances?
(e) Apply the Jarque-Bera normality test on returns from the three bar types.
What method achieves the lowest test statistic
'''

#Jarque-Bera normality test å¯¹æ¥è¿‘æ­£æ€åˆ†å¸ƒè¿›è¡Œæ£€æµ‹
data=pd.read_csv(r'D:\Git\book\ASML\dollar_bars.csv',index_col=[0])

# ä½¿ç”¨å¯¹æ•°æ”¶ç›Šç‡
data['returns'] = np.log(data['close'] / data['close'].shift(1))

# åˆ é™¤ç¼ºå¤±å€¼ï¼ˆç¬¬ä¸€è¡Œä¼šå˜æˆ NaNï¼‰
returns = data['returns'].dropna()



#c ç»Ÿè®¡æ»åç›¸å…³æ€§ï¼š

returns = data['returns'].dropna()

# 2. è®¡ç®—åºåˆ—çš„è‡ªç›¸å…³ç³»æ•°ï¼ˆå¯ä»¥é€‰æ‹©ä¸åŒæ»åæœŸæ•°ï¼Œè¿™é‡Œå–1æœŸæ»åï¼‰
lag = 1  # æ»åæœŸæ•°
serial_corr = returns.autocorr(lag=lag)

print(f"æ”¶ç›Šç‡çš„{lag}æœŸæ»åè‡ªç›¸å…³ç³»æ•°: {serial_corr}")

#d ï¼šæ¯æœˆå­é›†çš„æ”¶ç›Šç‡æ–¹å·®
data.index = pd.to_datetime(data.index)
data['month'] = data.index.to_period('M')  # ä½¿ç”¨ç´¢å¼•ä¸­çš„æ—¥æœŸæå–æœˆä»½

# æŒ‰æœˆä»½åˆ†ç»„å¹¶è®¡ç®—æ–¹å·®
monthly_variance = data.groupby('month')['returns'].var()
# è¾“å‡ºæ¯ä¸ªæœˆçš„æ–¹å·®
print(monthly_variance)


# ç»˜åˆ¶æ¯ä¸ªæœˆçš„æ”¶ç›Šç‡æ–¹å·®
plt.figure(figsize=(10, 6))
monthly_variance.plot(kind='bar')
plt.title('Monthly Variance of Returns')
plt.xlabel('Month')
plt.ylabel('Variance')
plt.xticks(rotation=45)
plt.show()

#e ï¼š Jarque-Bera æ­£æ€æ€§æ£€éªŒ

jb_stat, jb_pvalue = jarque_bera(returns)
print("Jarque-Bera ç»Ÿè®¡é‡:", jb_stat)
print("p-value:", jb_pvalue)

# 4. åˆ¤æ–­æ­£æ€æ€§
if jb_pvalue > 0.05:
    print("ä¸èƒ½æ‹’ç»æ­£æ€æ€§å‡è®¾ï¼Œæ”¶ç›Šç‡å¯ä»¥è§†ä¸ºè¿‘ä¼¼æ­£æ€åˆ†å¸ƒ")
else:
    print("æ‹’ç»æ­£æ€æ€§å‡è®¾ï¼Œæ”¶ç›Šç‡ä¸ç¬¦åˆæ­£æ€åˆ†å¸ƒ")

'''
2.2 On a series of E-mini S&P 500 futures tick data, compute dollar bars
and dollar imbalance bars. What bar type exhibits greater serial correlation?
Why?

'''



######################## ç®€å•çš„æ„å»ºdollar bar  ,äº¤æ˜“é‡‘é¢ç´¯è®¡è¾¾åˆ°ä¸€å®šçš„é—¨æ§›å°±resampleä¸ºæ–°çš„bar
def dd_bars(data: pd.DataFrame, m: int = None):
    '''
    params: data => dataframe of close series
    params: column => column of data sample; vol, dollar etc  ç´¯è®¡é˜ˆå€¼é—¨æ§›ï¼Œè¾¾åˆ°å°±é‡é‡‡æ ·
    '''    
    ts, idx = 0, []
    for i, x in enumerate(data):
        ts += x
        if ts >= m:
            ts = 0; idx.append(i)
            continue
    return data.iloc[idx]
tb = dd_bars(data = data.close, m = 1000000) # assuming 10% of daily transacted volme is 1,000,000


################### æ„å»ºå¤æ‚çš„imbalance baræ ·ä¾‹ï¼Œæ¯”å¦‚è€ƒè™‘æ¶¨è·Œè¿›è¡ŒåŠ æƒçš„æƒ…å†µï¼Œè¿˜å¯ä»¥åŠ ä¸Šè¿ç»­æ­£æ”¶ç›Šè®¡æ•°
def returns(data, tickers):
    b_t = []
    _ = data[tickers].pct_change()
    _.dropna(inplace=True)
    for i, value in enumerate(_):
        b_t.append(value)
    return b_t

def ema_tick(imbalance, weighted_count, weighted_sum, weighted_sum_T, limit, alpha, T_count):
    weighted_sum_T = limit + (1 - alpha) * weighted_sum_T
    weighted_sum = limit / (1.0 * T_count) + (1 - alpha) * weighted_sum
    weighted_count = 1 + (1 - alpha) * weighted_count
    imbalance = weighted_sum_T * weighted_sum/ weighted_count ** 2
    return imbalance, weighted_count, weighted_sum, weighted_sum_T

def imbalance_bar(data, tickers, set_limit, alpha):
    b_t = returns(data, tickers)
    bt_arr = []
    imb_arr = []
    weighted_sum_T = 0
    weighted_sum = 0
    weighted_count = 0
    bt_count = 0
    bt_up = 0
    b_imb_sum = 0
    b_sum = 0
    imbalance = 0
    for i, value in enumerate(b_t):
        bt_count += 1
        if value >= 0:
            b_sum += b_t[i]
            b_imb_sum += 1
            bt_up += 1
            bt_arr.append(b_sum)
        else:
            b_imb_sum -= 1
            b_sum += b_t[i]
            bt_arr.append(b_sum)
            
        upper_limit = max(b_imb_sum, bt_up)
        if upper_limit >= set_limit:
            imbalance, weighted_count, weighted_sum, weighted_sum_T = ema_tick(imbalance, 
                                                                               weighted_count,
                                                                               weighted_sum,
                                                                               weighted_sum_T,
                                                                               upper_limit,
                                                                               alpha,
                                                                               bt_count)
            imb_arr.append(imbalance) # exclude ewma without hitting threshold
            if upper_limit == bt_up:
                bt_up = 0
            else:
                b_imb_sum = 0
        else:
            imb_arr.append(0.0)    
    return bt_arr, imb_arr, b_t  #ç´¯è®¡æ”¶ç›Šç‡ï¼Œä¸å¹³è¡¡ä¿¡å·ï¼Œæ”¶ç›Šç‡listã€‚ç”Ÿæˆäº†ä¸å¹³è¡¡æ”¶ç›Šç‡è§¦å‘ä¿¡å·





'''
2.3 On dollar bar series of E-mini S&P 500 futures and Eurostoxx 50 futures:
(a) Apply Section 2.4.2 to compute the { Ì‚ğœ”t} vector used by the ETF trick. (Hint:
You will need FX values for EUR/USD at the roll dates.)
(b) Derive the time series of the S&P 500/Eurostoxx 50 spread.
(c) Confirm that the series is stationary, with an ADF test.

'''
#a ä½¿ç”¨pcaæ–¹æ³•ï¼ˆåˆ†è§£ä¸ºä¸ç›¸å…³çš„ä¸»æˆåˆ†ï¼‰å¯¹å„ä¸ªèµ„äº§çš„é£é™©è¿›è¡Œé…ç½®æƒé‡ï¼Œç„¶åä½¿ç”¨èµ„äº§ç»„åˆæ–¹å·®å…¬å¼è¿›è¡Œé…ç½®æ–¹å·®ï¼ˆax+byå¦‚æœç›¸å…³ç³»æ•°ä¸º0ï¼Œåˆ™æ–¹å·®ä¸ºaæ–¹*xæ–¹å·®+bæ–¹*yæ–¹å·®ï¼‰ï¼Œå¯ä»¥åŠ¨æ€è°ƒæ•´ä»è€Œä¿æŒæš´éœ²çš„é£é™©åœ¨åŒä¸€æ°´å¹³
def pcaWeights(cov,riskDist=None,riskTarget=1.):
    # Following the riskAlloc distribution, match riskTarget
    eVal,eVec=np.linalg.eigh(cov) # must be Hermitian
    indices=eVal.argsort()[::-1] # arguments for sorting eVal desc
    eVal,eVec=eVal[indices],eVec[:,indices]
    if riskDist is None:
        riskDist=np.zeros(cov.shape[0])
        riskDist[-1]=1.
    loads=riskTarget*(riskDist/eVal)**.5
    wghts=np.dot(eVec,np.reshape(loads,(-1,1)))
    #ctr=(loads/riskTarget)**2*eVal # verify riskDist
    return wghts


#b
# è®¡ç®—ä¸€é˜¶å·®åˆ†
data['close_diff'] = data['close'].diff()

# åˆ é™¤ NaN å€¼ï¼ˆå·®åˆ†åç¬¬ä¸€ä¸ªå€¼ä¼šä¸º NaNï¼‰
data = data.dropna()

#c ADFæ£€éªŒï¼Œç”¨äºæ£€éªŒæ—¶é—´åºåˆ—æ˜¯å¦å¹³ç¨³ã€‚ï¼ˆå¸¸ç”¨äºç»Ÿè®¡å¥—åˆ©-ä»·å·®å›å½’ï¼‰
# æ‰§è¡Œ ADF æ£€éªŒ  å¹³ç¨³çš„æ•°æ®æœ‰ç¨³å®šçš„ç»Ÿè®¡ç‰¹æ€§ï¼šå‡å€¼ï¼Œæ–¹å·®ï¼Œåæ–¹å·®ç­‰ç­‰æ˜¯ç¨³å®šçš„ï¼Œèƒ½å¤Ÿé¢„æµ‹æ•°æ®
adf_result = adfuller(data['close_diff'])  #å¤§æ•°æ®å¾ˆå¡ï¼Œå¾ˆåƒå†…å­˜

# è¾“å‡ºæ£€éªŒç»“æœ
print('ADF Statistic:', adf_result[0])
print('p-value:', adf_result[1])
print('Critical Values:', adf_result[4])

'''
2.4 Form E-mini S&P 500 futures dollar bars:
(a) Compute Bollinger bands of width 5% around a rolling moving average.
Count how many times prices cross the bands out (from within the bands
to outside the bands).
(b) Now sample those bars using a CUSUM filter, where {yt} are returns and
h = 0.05. How many samples do you get?
(c) Compute the rolling standard deviation of the two-sampled series. Which
one is least heteroscedastic? What is the reason for these results

2.5 Using the bars from exercise 4:
(a) Sample bars using the CUSUM filter, where {yt} are absolute returns and
h = 0.05.
(b) Compute the rolling standard deviation of the sampled bars.
(c) Compare this result with the results from exercise 4. What procedure delivered the least heteroscedastic sample? Why?

'''



#a 5%å¸ƒæ—å¸¦åŠå…¶å†…ç©¿å¤–
window = 20  # è®¡ç®—å¸ƒæ—å¸¦æ‰€ç”¨çš„ç§»åŠ¨çª—å£
std_multiplier = 0.05  # 5%æ ‡å‡†å·®å€æ•°

tb=pd.DataFrame(tb)
tb['close'] = pd.to_numeric(tb['close'], errors='coerce')  # å¼ºåˆ¶è½¬æ¢ä¸ºæ•°å€¼ç±»å‹

# tb=tb.iloc[:-2,:]
# è®¡ç®—ç®€å•ç§»åŠ¨å¹³å‡ (SMA) å’Œæ ‡å‡†å·®
tb['SMA'] = tb['close'].rolling(window=window).mean()  # è®¡ç®—SMA
tb['Std'] = tb['close'].rolling(window=window).std()  # è®¡ç®—æ ‡å‡†å·®

# è®¡ç®—å¸ƒæ—å¸¦çš„ä¸Šè½¨å’Œä¸‹è½¨
tb['Upper_Band'] = tb['SMA'] + (std_multiplier * tb['Std'])
tb['Lower_Band'] = tb['SMA'] - (std_multiplier * tb['Std'])

# æ£€æŸ¥ç©¿è¶Šæ¬¡æ•°
cross_up = 0  # ä»å†…ç©¿åˆ°å¤–ä¸Šè½¨
cross_down = 0  # ä»å†…ç©¿åˆ°å¤–ä¸‹è½¨

# éå†æ—¶é—´åºåˆ—ï¼Œæ£€æŸ¥æ¯ä¸ªç‚¹æ˜¯å¦ç©¿è¶Šäº†å¸ƒæ—å¸¦
for i in range(1, len(tb)):
    # åˆ¤æ–­æ˜¯å¦ä»å†…åˆ°å¤–ç©¿è¶Š
    if tb['close'][i] > tb['Upper_Band'][i] and tb['close'][i-1] <= tb['Upper_Band'][i-1]:
        cross_up += 1
    elif tb['close'][i] < tb['Lower_Band'][i] and tb['close'][i-1] >= tb['Lower_Band'][i-1]:
        cross_down += 1

# è¾“å‡ºç©¿è¶Šæ¬¡æ•°
print(f"ä»å†…åˆ°å¤–ç©¿è¶Šä¸Šè½¨çš„æ¬¡æ•°: {cross_up}")
print(f"ä»å†…åˆ°å¤–ç©¿è¶Šä¸‹è½¨çš„æ¬¡æ•°: {cross_down}")

#b   CUSUM filter. ä¸¤è¾¹æ­£è´Ÿå¯¹ç§°ç‰ˆã€‚ï¼ˆæ­£è´Ÿç´¯åŠ å€¼å¯ä»¥ä¸å¯¹ç§°é˜ˆå€¼ï¼Œè¿˜å¯ä»¥åŠ é¢„æœŸæ”¶ç›Šå€¼è¿›è¡Œä»‹å…¥ï¼Œè§ç« èŠ‚2.5.2.1ï¼‰
#èµ·åˆ°äº†æ»¤æ³¢å™¨çš„æ•ˆæœï¼Œå¯¹éœ‡è¡è¡Œæƒ…èƒ½å¤Ÿè¿‡æ»¤ã€‚
#CUSUM filter åˆ°åº•æ˜¯å¯¹ä»·æ ¼çš„å˜åŒ–ï¼Œè¿˜æ˜¯å¯¹æ”¶ç›Šç‡çš„å˜åŒ–ç´¯è®¡å‡ºæ¥çš„ç»“æœæ›´æœ‰æ•ˆå‘¢ï¼Ÿéœ€è¦åœ¨è¿™é‡Œçš„2.4ï¼Œ2.5è¿›è¡Œæ£€éªŒä¸€ä¸‹ã€‚
#æ‰€ä»¥è¿™é‡Œæœ‰ä¸‰ä¸ªï¼šä»·æ ¼çš„å˜åŒ–ï¼Œæ”¶ç›Šå˜åŒ–ï¼Œæ”¶ç›Šç‡çš„å·®å˜åŒ–ã€‚
#å¯¹æ”¶ç›Šç‡çš„å·®å˜åŒ–è¿›è¡ŒCUSUM filterè¿˜å¯ä»¥è¿›è¡ŒåŒºåˆ†å¸‚åœºæ˜¯å‡ºäºå›å½’è¿˜æ˜¯è¶‹åŠ¿ç±»çŠ¶æ€ã€‚

# åŸºäºä»·æ ¼å·®çš„CUSUM filter
def cumsum_events(df: pd.Series, limit: float):
    idx, _up, _dn = [], 0, 0
    diff = df.diff()
    for i in diff.index[1:]:
        _up, _dn = max(0, _up + diff.loc[i]), min(0, _dn + diff.loc[i])
        if _up > limit:
            _up = 0; idx.append(i)
        elif _dn < - limit:
            _dn = 0; idx.append(i)
        
    return pd.DatetimeIndex(idx)

# åŸºäºæ”¶ç›Šç‡å·®çš„CUSUM filter
def cumsum_events1(df: pd.Series, limit: float):
    idx, _up, _dn = [], 0, 0
    diff = df.pct_change()
    for i in diff.index[1:]:
        _up, _dn = max(0, _up + diff.loc[i]), min(0, _dn + diff.loc[i])
        if _up > limit:
            _up = 0; idx.append(i)
        elif _dn < - limit:
            _dn = 0; idx.append(i)
        
    return pd.DatetimeIndex(idx)


#äº‹ä»¶æ„å»ºçš„é˜ˆå€¼ä¹Ÿå¯ä»¥åŸºäºæ ‡å‡†åå·®æ¥æ„å»ºï¼Œè€Œä¸æ˜¯ä¸»è§‚è‡†æ–­
#æŒ‰ç…§ç™¾åˆ†æ¯”å’Œæ ‡å‡†åå·®æ„å»ºçš„äº‹ä»¶èƒ½å¤Ÿé€šè¿‡æ€€ç‰¹æ£€éªŒï¼Œæ˜¯åŒæ–¹å·®ï¼Œè€Œbenchmarkè¿™æ ·ä¸»è§‚å›ºå®šæ•°å€¼æ„å»ºçš„äº‹ä»¶æ— æ³•é€šè¿‡ã€‚
# ä½¿ç”¨å¯¹æ•°æ”¶ç›Šç‡
tb['returns'] = np.log(tb['close'] / tb['close'].shift(1))
tb = tb.dropna()


event = cumsum_events(tb['close'], limit = 0.005) # benchmark
event_pct = cumsum_events1(tb['close'], limit = 0.005) #åŸºäºç™¾åˆ†æ¯”æ„å»ºäº‹ä»¶
event_abs = cumsum_events(tb['close'], limit = tb['Std'].mean()) # åŸºäºæ ‡å‡†+æ ‡å‡†å·®é˜ˆå€¼
event_pct2 = cumsum_events(tb['returns'], limit = 0.005) #åŸºæ”¶ç›Šç‡å·®

tb.index = pd.to_datetime(tb.index)

event_count0 = tb.reindex(event)
event_count1 = tb.reindex(event_abs)
event_count2 = tb.reindex(event_pct)
event_count3 = tb.reindex(event_pct2)


#White Test åŒæ–¹å·®æ£€éªŒ
def white_test(data: pd.DataFrame, window: int = 21):
    data['std1'] = data['close'].rolling(21).std()
    data.dropna(inplace= True)
    X = smi.tools.tools.add_constant(data['close'])
    results = smi.regression.linear_model.OLS(data['std1'], X).fit()
    resid = results.resid
    exog = results.model.exog
    print("White-Test p-Value: {0}".format(sm.het_white(resid, exog)[1]))
    if sm.het_white(resid, exog)[1] > 0.05:
        print("White test outcome at 5% signficance: åŒæ–¹å·®")
    else:
        print("White test outcome at 5% signficance: å¼‚æ–¹å·®")


white_test(event_count0)  #å¼‚æ–¹å·®
white_test(event_count1)  #åŒæ–¹å·®ï¼Œpå€¼0.35
white_test(event_count2)  #å¼‚æ–¹å·®
white_test(event_count3)  #åŒæ–¹å·®,på€¼0.66



#%%
'''
ç¬¬ä¸‰ç« ï¼šå…ƒæ ‡ç­¾ï¼ˆmeta labelï¼‰
åœ¨æœ‰ä¸‹æ³¨æ–¹å‘æœ‰å¦‚ä½•ç¡®å®šæ˜¯å¦ä¸‹æ³¨

'''

import numpy as np
import pandas as pd
# import research as rs
import matplotlib.pyplot as plt



'''
3.1 Form dollar bars for E-mini S&P 500 futures:
(a) Apply a symmetric CUSUM filter (Chapter 2, Section 2.5.2.1) where the
threshold is the standard deviation of daily returns (Snippet 3.1).
(b) Use Snippet 3.4 on a pandas series t1, where numDays=1.
(c) On those sampled features, apply the triple-barrier method, where
ptSl=[1,1] and t1 is the series you created in point 1.b.
(d) Apply getBins to generate the labels.

'''
#a  æ‰¾åˆ°äº‹ä»¶
dollar = pd.read_csv(r'D:\Git\book\ASML\dollar_bars.csv'   ,
                     parse_dates=True,      # è§£ææ—¥æœŸåˆ—
                     index_col=[0]  # å°† 'date_time' åˆ—ä½œä¸ºç´¢å¼•
                     )
tb = dd_bars(data = dollar.close, m = 1000000)
#a 5%å¸ƒæ—å¸¦åŠå…¶å†…ç©¿å¤–
window = 20  # è®¡ç®—å¸ƒæ—å¸¦æ‰€ç”¨çš„ç§»åŠ¨çª—å£
tb=pd.DataFrame(tb)
tb['close'] = pd.to_numeric(tb['close'], errors='coerce')  # å¼ºåˆ¶è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
tb['returns'] = np.log(tb['close'] / tb['close'].shift(1))
tb = tb.dropna()
# è®¡ç®—ç§»åŠ¨æ ‡å‡†å·®
def getDailyVol(close,span0=100):
# daily vol, reindexed to close
    df0=close.index.searchsorted(close.index-pd.Timedelta(days=1))
    df0=df0[df0>0]
    df0=pd.Series(close.index[df0 - 1], index=close.index[close.shape[0]-df0.shape[0]:])
    df0=close.loc[df0.index]/close.loc[df0.values].values-1 # daily returns
    df0=df0.ewm(span=span0).std()
    return df0
std=getDailyVol(tb.close,span0=100)
std=pd.DataFrame(std).rename(columns={'close': 'daily_vol'})
# æŒ‰ index åˆå¹¶
tb = tb.join(std)
def cumsum_events3(df: pd.Series, limit: pd.Series):
    idx, _up, _dn = [], 0, 0
    diff = df.diff()
    
    # ç¡®ä¿ limit å’Œ df æœ‰ç›¸åŒçš„ç´¢å¼•
    if not df.index.equals(limit.index):
        raise ValueError("The index of 'limit' must match the index of 'df'.")
    
    for i in diff.index[1:]:
        # ä½¿ç”¨ä¸ df å½“å‰ç´¢å¼•å¯¹åº”çš„ limit å€¼
        current_limit = limit.loc[i]
        
        _up = max(0, _up + diff.loc[i])
        _dn = min(0, _dn + diff.loc[i])
        
        # å¦‚æœç´¯è®¡å€¼è¶…è¿‡ limitï¼Œé‡ç½®ç´¯è®¡å€¼
        if _up > current_limit:
            _up = 0
            idx.append(i)
        elif _dn < -current_limit:
            _dn = 0
            idx.append(i)
        
    return pd.DatetimeIndex(idx)

event_pct3=cumsum_events3(tb.returns, tb.daily_vol)

#b ç»™äº‹ä»¶åŠ ä¸Š1å¤©çš„æ—¶é—´é•¿åº¦
numDays=1
close=tb.close
tEvents=event_pct3
t1=close.index.searchsorted(tEvents+pd.Timedelta(days=numDays)) #è¿”å›ç¬¬ä¸€ä¸ªå¤§äºç­‰äºæ¯ä¸ªè°ƒæ•´åçš„äº‹ä»¶æ—¶é—´çš„ä½ç½®ï¼Œå³å¤§äº1å¤©çš„ç¬¬ä¸€ä¸ªcloseçš„æ—¶é—´
t1=t1[t1<close.shape[0]]
t1=pd.Series(close.index[t1],index=tEvents[:t1.shape[0]])
t1.name = 't1'

#c the triple-barrier method ï¼Œæ‰¾åˆ°äº‹ä»¶åœ¨æŒ‡å®šç®±ä½“å†…å…ˆç¢°åˆ°å“ªæ¡è¾¹ï¼Œç„¶åè·å–å¯¹åº”çš„æ—¶é—´ï¼Œ
#å¾—æ ¹æ®èµ·ç‚¹çš„çŠ¶æ€æ„å»ºç›®æ ‡ä¸Šä¸‹é™ï¼Œå¦åˆ™æœ‰æœªæ¥å‡½æ•°
#åŸæ–‡æ˜¯åŸºäºæ”¶ç›Šçš„ï¼Œè€Œä¸æ˜¯åŸºäºä»·æ ¼ã€‚åŸæ–‡è¿˜æœ‰ä¸€ä¸ªä¹°å…¥æ–¹å‘æ¥åˆ¤æ–­å¤šç©ºï¼Œæˆ‘è¿™é‡Œä¹Ÿæ²¡åˆ¤æ–­ï¼Œå°±é»˜è®¤ä¸ºå…¨æ˜¯åšå¤šã€‚
def applyPtSlOnT1(close:pd.Series, events:pd.DataFrame, ptSl:list, daily_vol:pd.Series):
    out = events[['t1']].copy()  # å¤åˆ¶äº‹ä»¶æ•°æ®æ¡†ï¼Œåªä¿ç•™t1åˆ—
    out['pt_time'] = pd.NaT  # åˆå§‹åŒ–pt_timeåˆ—
    out['sl_time'] = pd.NaT  # åˆå§‹åŒ–sl_timeåˆ—
    
    for loc, row in events.iterrows():
        t1 = row['t1']
        
        # è·å–äº‹ä»¶çš„èµ·å§‹æ—¶é—´å’Œç»ˆæ­¢æ—¶é—´
        start_time = loc
        end_time = t1
        
        # è·å–äº‹ä»¶çš„ä»·æ ¼æ•°æ®ï¼ˆèµ·å§‹æ—¶é—´åˆ°ç»ˆæ­¢æ—¶é—´ä¹‹é—´ï¼‰
        price_data = close[start_time:end_time]
        
        # è®¡ç®—ä¸Šä¸‹é™
        upper_limit = close[start_time] * (1 + daily_vol[start_time]) * ptSl[0]
        lower_limit = close[start_time] * (1 - daily_vol[start_time]) * ptSl[1]
        
        # æŸ¥æ‰¾é¦–æ¬¡è§¦åŠä¸Šé™å’Œä¸‹é™çš„æ—¶é—´ç‚¹
        pt_time = price_data[price_data >= upper_limit].index.min()  # ä¸Šé™è§¦åŠæ—¶é—´
        sl_time = price_data[price_data <= lower_limit].index.min()  # ä¸‹é™è§¦åŠæ—¶é—´
        
        # å¦‚æœè§¦åŠæ—¶é—´å­˜åœ¨ï¼Œåˆ™è®°å½•ï¼Œè‹¥æ²¡æœ‰è§¦åŠåˆ™ä¿æŒä¸ºNaT
        out.loc[loc, 'pt_time'] = pd.to_datetime(pt_time, errors='coerce') if pd.notna(pt_time) else pd.NaT
        out.loc[loc, 'sl_time'] =  pd.to_datetime(sl_time, errors='coerce') if pd.notna(sl_time) else pd.NaT
    
    return out

ptSl=[1,1]
events=pd.DataFrame(t1)
result = applyPtSlOnT1(tb.close, events, ptSl, tb.daily_vol)


#d è·å–äº‹ä»¶å¯¹åº”çš„ç®±å‹æ ‡ç­¾,å°±ä¸ä»…ä»…æ˜¯è·å–æ—¶é—´äº†ã€‚åˆ°è¿™ä¸€æ­¥å°±æ˜¯å°†æ³¢åŠ¨ç‡è¾ƒä½çš„äº‹ä»¶ç»™å‰”é™¤ï¼Œç„¶åå†å‰©ä½™çš„äº‹ä»¶é‡Œé¢æ‰§è¡Œç®±å‹è§„åˆ™ï¼Œå¹¶ä¸”è®°å½•åˆ°æ¯ä¸ªäº‹ä»¶çš„æœ€ç»ˆç»ˆç‚¹æ—¶é—´ï¼ˆä¸ç®¡æ˜¯ç®±å‹è§„åˆ™ä¸‰è¾¹çš„é‚£ä¸€è¾¹ï¼‰ï¼Œä»¥åŠå¯¹åº”çš„trgt
#trgtæ˜¯äº‹ä»¶çš„æ°´å¹³éšœç¢ç›®æ ‡ï¼ˆæ­¢ç›ˆæ­¢æŸï¼‰ï¼Œç»å¯¹æ”¶ç›Šï¼Œè¿™é‡Œå¯ä»¥ç”¨stdæ›¿ä»£
#æŒ‡æœ€å°æ”¶ç›Šç‡ï¼Œåœ¨è¿™é‡Œçš„ä½œç”¨æ˜¯å°†æ³¢åŠ¨/æ”¶ç›Šè¾ƒå°çš„äº‹ä»¶ç»™è¿‡æ»¤æ‰
def getEvents(close,tEvents,ptSl,trgt,minRet,t1=False):
    #1) get target
    trgt=trgt.loc[tEvents]
    trgt=trgt[trgt>minRet] # minRet
    #2) get t1 (max holding period)
    if t1 is False:
        t1=pd.Series(pd.NaT,index=tEvents)
    #3) form events object, apply stop loss on t1
    side_=pd.Series(1.,index=trgt.index)
    events=pd.concat({'t1':t1,'trgt':trgt,'side':side_},axis=1).dropna(subset=[('trgt')])
    #df0=mpPandasObj(func=applyPtSlOnT1,pdObj=('molecule',events.index),numThreads=numThreads,close=close,events=events,ptSl=[ptSl,ptSl])
    df0=applyPtSlOnT1(close, events, ptSl, events.trgt)
    events['t1']=df0.dropna(how='all').min(axis=1) # pd.min ignores nan
    events=events.drop('side',axis=1)
    return events

trgt=std['daily_vol']
minRet=0.035
result_getEvents= getEvents(close,tEvents,ptSl,trgt,minRet,t1)

#è·å–å¯¹åº”äº‹ä»¶çš„å®é™…æ”¶ç›Šä¸å®é™…è¦æ‰§è¡Œçš„æ­£ç¡®æ–¹å‘ã€‚
#ä¹Ÿå°±æ˜¯è¿™ä¸ªäº‹ä»¶æœ€ç»ˆåº”è¯¥æ˜¯labelä¸ºä»€ä¹ˆæ ·çš„æ–¹å‘
def getBins(events,close):
    #1) prices aligned with events
    events_=events.dropna(subset=['t1'])
    px=events_.index.union(events_['t1'].values).drop_duplicates()
    px=close.reindex(px,method='bfill')
    #2) create out object
    out=pd.DataFrame(index=events_.index)
    out['ret']=px.loc[events_['t1'].values].values/px.loc[events_.index]-1
    out['bin']=np.sign(out['ret'])  #æ•°æ®æ ¹æ®åŸæ¥çš„æ•°å€¼è½¬ä¸ºã€-1ï¼Œ0ï¼Œ1ã€‘ä¸­çš„ä¸€ä¸ª
    return out

result_getBins=getBins(events,close)

'''
3.2 From exercise 3.1, use Snippet 3.8 to drop rare labels.

'''
#å»æ‰å‡ºç°é¢‘ç‡ä½äº0.05çš„æ ‡ç­¾ã€‚
#å»æ‰æç«¯æ ‡ç­¾ï¼Œæœ‰åˆ©äºæœºå™¨å­¦ä¹ è¯†åˆ«
def dropLabels(events,minPtc=0.05):
# apply weights, drop labels with insufficient examples
    while True:
        df0=events['bin'].value_counts(normalize=True)
        if df0.min()>minPtc or df0.shape[0]<3:
            break
        print ('dropped label',df0.argmin(),df0.min())
        events=events[events['bin']!=df0.argmin()]
    return events

result_getBins=dropLabels(events=result_getBins,minPtc=0.05)



'''
3.3 Adjust the getBins function (Snippet 3.5) to return a 0 whenever the vertical
barrier is the one touched first
'''
#åœ¨è·å–åˆ°result_getEventsçš„åŸºç¡€ä¸Šè¿›è¡Œæ”¹é€ ï¼Œå³èµ·ç‚¹å’Œç»ˆç‚¹ã€‚
def getBins2(events,t1,close):
    #1) prices aligned with events
    events_=events.dropna(subset=['t1'])
    px=events_.index.union(events_['t1'].values).drop_duplicates()
    px=close.reindex(px,method='bfill')
    #2) create out object
    out=pd.DataFrame(index=events_.index)
    out['ret']=px.loc[events_['t1'].values].values/px.loc[events_.index]-1
    out['bin']=np.sign(out['ret'])  #æ•°æ®æ ¹æ®åŸæ¥çš„æ•°å€¼è½¬ä¸ºã€-1ï¼Œ0ï¼Œ1ã€‘ä¸­çš„ä¸€ä¸ª
    #å¦‚æœäº‹ä»¶ç»ˆç‚¹çš„å€¼ç­‰äºå‚ç›´éšœç¢ï¼Œåˆ™èµ‹å€¼ä¸º0 
    out=out.merge(events_['t1'],how='inner', left_index=True, right_index=True)
    out=out.merge(t1,how='inner', left_index=True, right_index=True)
    out['bin']= np.where(out['t1_x'] == out['t1_y'], 0, out['bin'])
    return out

result_getBins2=getBins2(events=result_getEvents,t1=t1,close=close)




'''
3.4 Develop a trend-following strategy based on a popular technical analysis statistic
(e.g., crossing moving averages). For each observation, the model suggests a side,
but not a size of the bet.
(a) Derive meta-labels for ptSl=[1,2] and t1 where numDays=1. Use as
trgt the daily standard deviation as computed by Snippet 3.1.
(b) Train a random forest to decide whether to trade or not. Note: The decision
is whether to trade or not, {0,1}, since the underlying  model (the crossing moving average) has decided the side, {âˆ’1,1}.

'''

#meta-labels:å¯¹æ¨¡å‹å‘å‡ºçš„ä¿¡å·è¿›è¡Œç¡®è®¤ã€‚è´Ÿè´£è§£å†³â€œå½“ä¸»æ¨¡å‹å‘å‡ºä¿¡å·æ—¶ï¼Œæˆ‘åˆ°åº•è¯¥ä¸è¯¥ç›¸ä¿¡å®ƒï¼Ÿâ€œ,é€šè¿‡åˆ†ç±»å­¦ä¹ ä¸»æ¨¡å‹ä¿¡å·æ˜¯å¦ç›ˆåˆ©æ¥æå‡æ¦‚ç‡/ç½®ä¿¡ã€‚å³ä¿¡å·å‘å‡ºåç½®ä¿¡åŒºé—´å¤§äºé˜ˆå€¼å†æ‰§è¡Œï¼Œå¦‚æ¦‚ç‡å¤§äº0.6å†æ‰§è¡Œ
#å¯ä»¥ç‹¬ç«‹çš„ä¼˜åŒ–ä¸»æ¨¡å‹ä¸å…ƒæ¨¡å‹ï¼ˆmetaæ¨¡å‹ï¼‰ï¼Œä¸»æ¨¡å‹æ˜¯æ— è®ºæ˜¯ç»Ÿè®¡æ¨¡å‹ã€æœºå™¨å­¦ä¹ æ¨¡å‹è¿˜æ˜¯åŸºäºè§„åˆ™çš„ç³»ç»Ÿéƒ½å¯ä»¥åº”ç”¨ï¼Œå…ƒæ¨¡å‹ä¹Ÿå¯ä»¥ç»§ç»­ä½¿ç”¨å¸‚åœºç‰¹å¾ï¼ˆä»·æ ¼ï¼Œäº¤æ˜“é‡ç­‰ï¼‰ï¼Œæˆ–è€…å› å­ç­‰ä½œä¸ºç‰¹å¾é›†ã€‚
#è¿˜å¯ä»¥ä½¿ç”¨å…ƒæ¨¡å‹æ¥ç¡®å®šä¹°å…¥çš„å¤´å¯¸å¤§å°ï¼Œè¿™æ ·å°±æ„æˆäº†ä¸»æ¨¡å‹ç¡®å®šæ–¹å‘ï¼Œå…ƒæ¨¡å‹ç¡®å®šå¤´å¯¸ã€‚
# å…·ä½“ï¼šDeepseekè¯´ä¸‹æ³¨çš„å¤§å°æ˜¯è·Ÿä¸‹æ³¨æˆåŠŸæ¦‚ç‡æŒ‚é’©ï¼Œå¯ä»¥åŠ å…¥é¢„æœŸç›ˆäºæ¯”å¼•å…¥å‡¯åˆ©å…¬å¼ï¼Œä¹Ÿå¯ä»¥å°†è¿™ä¸ªæ¦‚ç‡ç»“åˆæ³¢åŠ¨ç‡ï¼Œç›¸å…³æ€§ç­‰ç»“åˆï¼Œæ˜¯é£é™©ç®¡ç†æ­¢ç›ˆæ­¢æŸéƒ½å¯ä»¥ç”¨çš„ä¸Šçš„ã€‚éƒ½æ˜¯åœ¨metalabelè·å–ä¿¡å·æˆåŠŸæ¦‚ç‡åŸºç¡€ä¸Šè¿›è¡Œçš„ã€‚


#a
#ä½¿ç”¨dollar baré‡‘å‰æ­»å‰æ„å»ºç®€å•çš„äº¤æ˜“ã€‚
# è®¡ç®—ç§»åŠ¨å¹³å‡çº¿
short_ma = close.rolling(window=5).mean()
long_ma = close.rolling(window=60).mean()

# åˆ›å»ºä¿¡å·DataFrame
df = pd.DataFrame(index=close.index)
df['close'] = close
df['short_ma'] = short_ma
df['long_ma'] = long_ma
# ç”Ÿæˆäº¤æ˜“ä¿¡å·
# é‡‘å‰: çŸ­å‡çº¿ä¸Šç©¿é•¿å‡çº¿ (ä¹°å…¥ä¿¡å·)
# æ­»å‰: çŸ­å‡çº¿ä¸‹ç©¿é•¿å‡çº¿ (å–å‡ºä¿¡å·)
df['signal'] = 0  # 0è¡¨ç¤ºæ— ä¿¡å·
# è®¡ç®—é‡‘å‰å’Œæ­»å‰
golden_cross = (df['short_ma'] > df['long_ma']) & (df['short_ma'].shift(1) <= df['long_ma'].shift(1))
death_cross = (df['short_ma'] < df['long_ma']) & (df['short_ma'].shift(1) >= df['long_ma'].shift(1))
# æ ‡è®°ä¿¡å·
df.loc[golden_cross, 'signal'] = 1  # ä¹°å…¥ä¿¡å·
df.loc[death_cross, 'signal'] = -1  # å–å‡ºä¿¡å·
# æå–æ‰€æœ‰äº¤æ˜“ä¿¡å·çš„æ—¶é—´ç‚¹
buy_signals = df[df['signal'] == 1]
sell_signals = df[df['signal'] == -1]

print("ä¹°å…¥ä¿¡å·å‘ç”Ÿæ—¶é—´:")
print(len(buy_signals.index))
print("\nå–å‡ºä¿¡å·å‘ç”Ÿæ—¶é—´:")
print(len(sell_signals.index))

# # å¯è§†åŒ–
# plt.figure(figsize=(12, 8))
# plt.plot(df.index, df['close'], label='Close Price', alpha=0.5)
# plt.plot(df.index, df['short_ma'], label='5-period MA', alpha=0.7)
# plt.plot(df.index, df['long_ma'], label='60-period MA', alpha=0.7)
# # æ ‡è®°ä¹°å…¥ä¿¡å·
# plt.scatter(buy_signals.index, buy_signals['close'], 
#             color='green', marker='^', s=100, label='Buy Signal')

# # æ ‡è®°å–å‡ºä¿¡å·
# plt.scatter(sell_signals.index, sell_signals['close'], 
#             color='red', marker='v', s=100, label='Sell Signal')
# plt.title('Golden Cross/Death Cross Trading Strategy')
# plt.legend()
# plt.grid(True)
# plt.show()

# è®¡ç®—ç›¸é‚»æ—¶é—´ç‚¹çš„æ—¶é—´å·®
time_diffs = df[df['signal'] != 0].index.to_series().diff().dropna()

avg_interval_seconds = time_diffs.dt.total_seconds().mean()
avg_interval = avg_interval_seconds/86400 # è½¬æ¢ä¸ºå¤©æ•°

numDays = avg_interval
# 
ptSl=[1,2]
std = getDailyVol(df['close'], span0=100)
std = pd.DataFrame(std).rename(columns={'close': 'daily_vol'})
# å°†ç»“æœåˆå¹¶å›åŸDataFrame
df = df.join(std)

t1 = df['close'].index.searchsorted(df['close'].index + pd.Timedelta(days=numDays))
t1 = t1[t1 < df.shape[0]]  # ç¡®ä¿ä¸è¶…å‡ºèŒƒå›´
t1 = pd.Series(df['close'].index[t1], index=df['close'].index[:t1.shape[0]])
t1.name = 't1'
# å°†t1åˆ—æ·»åŠ åˆ°DataFrame
df = df.join(t1)
df_events = df[df['signal'] != 0]
df_events.head()

def generate_metalabels(df_events: pd.DataFrame, close: pd.Series, daily_vol: pd.Series, ptSl: list) -> pd.DataFrame:
    """
    ç”Ÿæˆä¸‰é‡éšœç¢æ³•çš„meta-labels
    æ³¨ï¼šå¯ä»¥æ”¹è¿›ä¸ºå‘é‡åŒ–æ“ä½œå†å åŠ åˆ†å¸ƒå¼è®¡ç®—åº”ç”¨äºå¤§æ•°æ®é›†ã€‚
    
    å‚æ•°:
        df_events: åŒ…å«t1åˆ—çš„äº‹ä»¶DataFrame
        close: æ”¶ç›˜ä»·Series
        daily_vol: æ¯æ—¥æ³¢åŠ¨ç‡Series
        ptSl: [æ­¢ç›ˆå€æ•°, æ­¢æŸå€æ•°]
    
    è¿”å›:
        æ·»åŠ äº†metallabelåˆ—çš„DataFrame
    """
    # å¤åˆ¶ä¸€ä»½é¿å…ä¿®æ”¹åŸæ•°æ®
    df = df_events.copy()
    df['metallabel'] = 0  # åˆå§‹åŒ–ä¸º0
    
    for idx, row in df.iterrows():
        start_time = idx
        end_time = row['t1']
        
        # è·å–ä»·æ ¼åºåˆ—
        price_series = close[start_time:end_time]
        if price_series.empty:
            continue
            
        # è®¡ç®—ä¸Šä¸‹é™
        if ptSl[0]==0:
            upper_limit = np.inf  # å¦‚æœæ­¢ç›ˆä¸º0ï¼Œåˆ™è®¾ä¸ºæ— ç©·å¤§
        else:
            upper_limit = close[start_time] * (1 + ptSl[0] * daily_vol[start_time])
        
        if ptSl[1]==0:
            lower_limit = -np.inf  # å¦‚æœæ­¢æŸä¸º0ï¼Œåˆ™è®¾ä¸ºè´Ÿæ— ç©·å¤§
        else:
            lower_limit = close[start_time] * (1 - ptSl[1] * daily_vol[start_time])
        
        # æ£€æŸ¥æ˜¯å¦è§¦åŠæ­¢ç›ˆ
        pt_touch = price_series[price_series >= upper_limit]
        # æ£€æŸ¥æ˜¯å¦è§¦åŠæ­¢æŸ
        sl_touch = price_series[price_series <= lower_limit]
        
        # ç¡®å®šæœ€å…ˆè§¦åŠçš„éšœç¢
        if not pt_touch.empty and not sl_touch.empty:
            # ä¸¤è€…éƒ½è§¦åŠï¼Œçœ‹å“ªä¸ªå…ˆå‘ç”Ÿ
            if pt_touch.index[0] < sl_touch.index[0]:
                df.loc[idx, 'metallabel'] = 1
            else:
                df.loc[idx, 'metallabel'] = -1
        elif not pt_touch.empty:
            df.loc[idx, 'metallabel'] = 1
        elif not sl_touch.empty:
            df.loc[idx, 'metallabel'] = -1
        # å¦‚æœéƒ½æ²¡è§¦åŠï¼Œä¿æŒä¸º0
        
    return df


df_events_with_labels = generate_metalabels(df_events, df['close'], df_events['daily_vol'], ptSl)


#b å†™ä¸€ä¸ªéšæœºæ£®æ—æ¨¡å‹å­¦ä¹ æ˜¯å¦è¯¥ä¸‹æ³¨
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# 1. å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
# åˆ›å»ºç›®æ ‡å˜é‡ï¼šsignalæ˜¯å¦ç­‰äºmetallabelï¼ˆæ­£ç¡®æ–¹å‘ï¼‰
df_events_with_labels['correct_direction'] = (df_events_with_labels['signal'] == df_events_with_labels['metallabel']).astype(int)

# 2. ç‰¹å¾å·¥ç¨‹
def prepare_features(df):
    """å‡†å¤‡ç”¨äºè®­ç»ƒçš„ç‰¹å¾æ•°æ®"""
    features = pd.DataFrame(index=df.index)
    
    # 1. ä»·æ ¼åŠ¨é‡ç‰¹å¾
    features['returns'] = df['close'].pct_change() 
    features['returns_5'] = df['close'].pct_change(5)
    features['returns_20'] = df['close'].pct_change(20)
    
    # 2. ç§»åŠ¨å¹³å‡ç‰¹å¾
    features['ma_ratio'] = df['short_ma'] / df['long_ma'] - 1
    features['ma_dist'] = (df['close'] - df['long_ma']) / df['long_ma']
    
    # 3. æ³¢åŠ¨ç‡ç‰¹å¾
    features['volatility'] = df['daily_vol']
    features['volatility_ratio'] = df['daily_vol'] / df['daily_vol'].rolling(20).mean()
    
    # 4. ä¿¡å·ç‰¹å¾
    features['signal'] = df['signal']

    #åŸå§‹ç‰¹å¾
    features['short_ma'] = df['short_ma']
    features['long_ma'] = df['long_ma']
    features['close'] = df['close']
    
    # åˆ é™¤ç¼ºå¤±å€¼
    features = features.dropna()
    
    return features

features = prepare_features(df)
features=features.loc[df_events_with_labels.index]  # åªä¿ç•™äº‹ä»¶å¯¹åº”çš„ç‰¹å¾

X = features
y = df_events_with_labels['correct_direction']

# 3. åˆ†å‰²æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# 4. è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
rf = RandomForestClassifier(
    n_estimators=100,
    max_depth=5,
    random_state=42,
    class_weight='balanced'  # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
)
rf.fit(X_train, y_train)

# 5. è¯„ä¼°æ¨¡å‹
print(classification_report(y_test, rf.predict(X_test)))

df_x_test = X_test.copy()


df_x_test['correct_direction'] = y_test  # å®é™…æ­£ç¡®æ–¹å‘
df_x_test['metallabel'] = df_events_with_labels.loc[y_test.index, 'metallabel']  # åŸå§‹metallabel

# 3. æ·»åŠ é¢„æµ‹ç»“æœ
df_x_test['predicted_direction'] = rf.predict(X_test)  # é¢„æµ‹æ–¹å‘

# 4. è®¡ç®—é¢„æµ‹å‡†ç¡®ç‡
accuracy = (df_x_test['predicted_direction'] == df_x_test['correct_direction']).mean()
print(f"æ¨¡å‹é¢„æµ‹å‡†ç¡®ç‡: {accuracy:.2%}")

# 5. å¯é€‰ï¼šæ·»åŠ é¢„æµ‹æ¦‚ç‡
df_x_test['probability'] = rf.predict_proba(X_test)[:, 1]  # é¢„æµ‹ä¸º1çš„æ¦‚ç‡

# 6. æŸ¥çœ‹ç»“æœ
display(df_x_test.head())  

#ç»“è®ºï¼Œè¿™ä¸ªä¿¡å·æ¨¡å‹æ•ˆæœæå·®ï¼Œmetaæ¨¡å‹åé¦ˆå‡ºæ¥çš„ç»“æœéƒ½æ˜¯ä¸è¿›è¡Œä¸‹æ³¨ï¼Œä¸ç®¡å¤šç©ºæ–¹å‘éƒ½æ˜¯ã€‚




'''
3.5 Develop a mean-reverting strategy based on Bollinger bands. For each observation, the model suggests a side, but not a size of the bet.
(a) Derive meta-labels for ptSl=[0,2] and t1 where numDays=1. Use as trgt the daily standard deviation as computed by Snippet 3.1.
(b) Train a random forest to decide whether to trade or not. Use as features: volatility, serial correlation, and the crossing moving averages from
exercise 2.
(c) What is the accuracy of predictions from the primary model (i.e., if the secondary model does not filter the bets)? What are the precision, recall, and
F1-scores?
(d) What is the accuracy of predictions from the secondary model? What are the precision, recall, and F1-scores?
'''

#a

def bollinger_strategy(close, window=20, num_std=2):
    """
    åŸºäºå¸ƒæ—å¸¦çš„å‡å€¼å›å½’ç­–ç•¥
    :param close: æ”¶ç›˜ä»·åºåˆ—
    :param window: ç§»åŠ¨å¹³å‡çª—å£
    :param num_std: æ ‡å‡†å·®å€æ•°
    :return: åŒ…å«ä¿¡å·åˆ—çš„DataFrame
    """
    # è®¡ç®—å¸ƒæ—å¸¦
    rolling_mean = close.rolling(window=window).mean()
    rolling_std = close.rolling(window=window).std()
    
    upper_band = rolling_mean + num_std * rolling_std
    lower_band = rolling_mean - num_std * rolling_std
    
    # ç”Ÿæˆä¿¡å·
    signal = pd.Series(0, index=close.index)
    signal[close < lower_band] = 1    # ä½äºä¸‹è½¨ï¼Œä¹°å…¥ä¿¡å·
    signal[close > upper_band] = -1   # é«˜äºä¸Šè½¨ï¼Œå–å‡ºä¿¡å·
    
    # åˆ›å»ºç»“æœDataFrame
    result = pd.DataFrame({
        'close': close,
        'rolling_mean': rolling_mean,
        'upper_band': upper_band,
        'lower_band': lower_band,
        'signal': signal
    })
    
    return result

# ä½¿ç”¨ç¤ºä¾‹
df = bollinger_strategy(df['close'], window=20, num_std=2)
display(df.head())


import matplotlib.pyplot as plt
# åˆ›å»ºå›¾è¡¨
plt.figure(figsize=(12, 6))

# ç»˜åˆ¶æ”¶ç›˜ä»·å’Œå¸ƒæ—å¸¦
plt.plot(df.index, df['close'], label='Close Price', color='black', linewidth=1)
plt.plot(df.index, df['rolling_mean'], label='Moving Average', color='blue', linestyle='--')
plt.plot(df.index, df['upper_band'], label='Upper Band', color='red', linestyle=':')
plt.plot(df.index, df['lower_band'], label='Lower Band', color='green', linestyle=':')

# æ ‡è®°ä¹°å…¥ä¿¡å· (signal == 1)
buy_signals = df[df['signal'] == 1]
plt.scatter(buy_signals.index, buy_signals['close'], 
           label='Buy Signal', color='green', marker='^', s=100)

# æ ‡è®°å–å‡ºä¿¡å· (signal == -1)
sell_signals = df[df['signal'] == -1]
plt.scatter(sell_signals.index, sell_signals['close'], 
           label='Sell Signal', color='red', marker='v', s=100)

# å¡«å……å¸ƒæ—å¸¦åŒºåŸŸ
plt.fill_between(df.index, df['upper_band'], df['lower_band'], 
               color='gray', alpha=0.2, label='Bollinger Band')

# æ·»åŠ å›¾ä¾‹å’Œæ ‡é¢˜
plt.title('Bollinger Band Strategy Signals')
plt.xlabel('Date')
plt.ylabel('Price')
plt.legend()

# æ˜¾ç¤ºå›¾è¡¨
plt.show()


# è®¡ç®—ç›¸é‚»æ—¶é—´ç‚¹çš„æ—¶é—´å·® ä½œä¸ºå¹³å‡æŒä»“æ—¶é—´
time_diffs = df[df['signal'] != 0].index.to_series().diff().dropna()

avg_interval_seconds = time_diffs.dt.total_seconds().mean()
avg_interval = avg_interval_seconds/86400 # è½¬æ¢ä¸ºå¤©æ•°

numDays = avg_interval

ptSl=[0,2]
std = getDailyVol(df['close'], span0=100)
std = pd.DataFrame(std).rename(columns={'close': 'daily_vol'})
# å°†ç»“æœåˆå¹¶å›åŸDataFrame
df = df.join(std)

t1 = df['close'].index.searchsorted(df['close'].index + pd.Timedelta(days=numDays))
t1 = t1[t1 < df.shape[0]]  # ç¡®ä¿ä¸è¶…å‡ºèŒƒå›´
t1 = pd.Series(df['close'].index[t1], index=df['close'].index[:t1.shape[0]])
t1.name = 't1'
# å°†t1åˆ—æ·»åŠ åˆ°DataFrame
df = df.join(t1)
df_events = df[df['signal'] != 0]
df_events.head()

df_events_with_labels = generate_metalabels(df_events, df['close'], df_events['daily_vol'], ptSl)

#b ä½¿ç”¨éšæœºæ£®æ—æ¨¡å‹å»éªŒè¯ä¿¡å·æ•ˆæœã€‚è²Œä¼¼è®­ç»ƒé¢˜é‡Œé¢æåˆ°çš„ç‰¹å¾æ˜¯ä¸åŒ…å«ä¸»æ¨¡å‹signalçš„ã€‚
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def prepare_features(close, window_short=5, window_long=60, corr_window=20):
    """
    æ‰©å±•ç‰¹å¾å·¥ç¨‹å‡½æ•°
    :param close: æ”¶ç›˜ä»·åºåˆ—
    :param window_short: çŸ­æœŸç§»åŠ¨å¹³å‡çª—å£
    :param window_long: é•¿æœŸç§»åŠ¨å¹³å‡çª—å£
    :param corr_window: åºåˆ—ç›¸å…³æ€§è®¡ç®—çª—å£
    :return: åŒ…å«æ–°ç‰¹å¾çš„DataFrame
    """
    # åŸºç¡€ç‰¹å¾
    features = pd.DataFrame(index=close.index)
    features['returns'] = close.pct_change()
    
    # 1. åºåˆ—ç›¸å…³æ€§ç‰¹å¾
    features['autocorr_1'] = features['returns'].rolling(corr_window).apply(
        lambda x: x.autocorr(lag=1), raw=False)
    features['autocorr_5'] = features['returns'].rolling(corr_window).apply(
        lambda x: x.autocorr(lag=5), raw=False)
    
    # 2. ç§»åŠ¨å¹³å‡ç‰¹å¾
    features['ma_short'] = close.rolling(window_short).mean()
    features['ma_long'] = close.rolling(window_long).mean()
    
    # 3. ç§»åŠ¨å¹³å‡æ¯”ç‡
    features['ma_ratio'] = features['ma_short'] / features['ma_long']
    
    # 4. ä»·æ ¼ä¸ç§»åŠ¨å¹³å‡åç¦»åº¦
    features['dev_short'] = close / features['ma_short'] - 1
    features['dev_long'] = close / features['ma_long'] - 1
    
    # 5. ç§»åŠ¨å¹³å‡äº¤å‰ä¿¡å·
    features['ma_cross'] = np.where(features['ma_short'] > features['ma_long'], 1, -1)
    
    return features.dropna()

# ä½¿ç”¨ç¤ºä¾‹
features = prepare_features(df['close'])
# display(features.head())
merged_df = df_events_with_labels.join(features, how='inner')


# å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
X = merged_df[['close', 'rolling_mean', 'upper_band', 'lower_band', 'signal',
       'daily_vol', 'returns', 'autocorr_1', 'autocorr_5',
       'ma_short', 'ma_long', 'ma_ratio', 'dev_short', 'dev_long', 'ma_cross']]
y = merged_df['metallabel']

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# åˆå§‹åŒ–éšæœºæ£®æ—åˆ†ç±»å™¨
rf = RandomForestClassifier(n_estimators=100, random_state=42)

# è®­ç»ƒæ¨¡å‹
rf.fit(X_train, y_train)


print(classification_report(y_test, rf.predict(X_test)))

df_x_test = X_test.copy()

df_x_test['correct_direction'] = y_test  # å®é™…æ­£ç¡®æ–¹å‘
df_x_test['metallabel'] = merged_df.loc[y_test.index, 'metallabel']  # åŸå§‹metallabel

# 3. æ·»åŠ é¢„æµ‹ç»“æœ
df_x_test['predicted_direction'] = rf.predict(X_test)  # é¢„æµ‹æ–¹å‘

# 4. è®¡ç®—é¢„æµ‹å‡†ç¡®ç‡
accuracy = (df_x_test['predicted_direction'] == df_x_test['correct_direction']).mean()
print(f"æ¨¡å‹é¢„æµ‹å‡†ç¡®ç‡: {accuracy:.2%}")

# 5. å¯é€‰ï¼šæ·»åŠ é¢„æµ‹æ¦‚ç‡
df_x_test['probability'] = rf.predict_proba(X_test)[:, 1]  # é¢„æµ‹ä¸º1çš„æ¦‚ç‡

# 6. æŸ¥çœ‹ç»“æœ
display(df_x_test.head())  

df_x_test[['signal','metallabel','predicted_direction','probability']]


#c  ä¸»æ¨¡å‹çš„å‡†ç¡®ç‡ï¼Œç²¾ç¡®ç‡ï¼Œå¬å›ç‡å’Œf1scoreï¼Œå³æ²¡æœ‰å…ƒæ¨¡å‹è¿›è¡Œè¿‡æ»¤çš„è¯

print("ä¸»æ¨¡å‹è¯„ä¼°æŠ¥å‘Š:")
print(classification_report(y_test, X_test['signal']))



#d  åŠ å…¥å…ƒæ¨¡å‹åçš„å‡†ç¡®ç‡ï¼Œç²¾ç¡®ç‡ï¼Œå¬å›ç‡å’Œf1score
print("åŠ å…¥å…ƒæ¨¡å‹è¯„ä¼°æŠ¥å‘Š:")
print(classification_report(y_test, df_x_test['predicted_direction']))

# ç»“è®ºï¼šå‡†ç¡®ç‡ï¼Œç²¾ç¡®ç‡ï¼Œå¬å›ç‡å’Œf1scoreè¿™äº›æŒ‡æ ‡éƒ½æœ‰å›å‡ï¼Œæé«˜äº†ä¿¡å·çš„å¯ä¿¡åº¦

#ç”»å›¾
# 1. è·å–X_testæ—¶é—´æ®µå†…çš„æ”¶ç›˜ä»·
start_date = df_x_test.index.min()
end_date = df_x_test.index.max()
closes_in_period = close.loc[start_date:end_date]

# 2. åˆ›å»ºå›¾è¡¨
plt.figure(figsize=(14, 7))

# ç»˜åˆ¶æ”¶ç›˜ä»·
plt.plot(closes_in_period.index, closes_in_period, 
        label='Close Price', color='black', linewidth=1.5)

# 3. æ ‡æ³¨åŸå§‹signalä¿¡å·
signal_mask = df_x_test['signal'] != 0
buy_signals = df_x_test[(df_x_test['signal'] == 1) & signal_mask]
sell_signals = df_x_test[(df_x_test['signal'] == -1) & signal_mask]

plt.scatter(buy_signals.index, 
           closes_in_period.loc[buy_signals.index],
           label='Original Buy Signal', 
           color='blue', marker='^', s=100)
plt.scatter(sell_signals.index, 
           closes_in_period.loc[sell_signals.index],
           label='Original Sell Signal', 
           color='red', marker='v', s=100)

# 4. æ ‡æ³¨é¢„æµ‹predicted_directionä¿¡å·
pred_buy = df_x_test[df_x_test['predicted_direction'] == 1]
pred_sell = df_x_test[df_x_test['predicted_direction'] == -1]

plt.scatter(pred_buy.index, 
           closes_in_period.loc[pred_buy.index],
           label='Predicted Buy', 
           color='green', marker='*', s=150, alpha=0.7)
plt.scatter(pred_sell.index, 
           closes_in_period.loc[pred_sell.index],
           label='Predicted Sell', 
           color='orange', marker='*', s=150, alpha=0.7)

# 5. æ·»åŠ å›¾è¡¨å…ƒç´ 
plt.title(f'Price and Signals from {start_date.date()} to {end_date.date()}')
plt.xlabel('Date')
plt.ylabel('Price')
plt.legend()
plt.grid(True, alpha=0.3)

# 6. æ˜¾ç¤ºå›¾è¡¨
plt.show()
# ç»“è®ºï¼šå¯ä»¥çœ‹åˆ°å…ƒæ¨¡å‹è¿‡æ»¤åçš„ä¿¡å·ç‚¹æ˜æ˜¾å‡å°‘ï¼Œä¸”å¤§éƒ¨åˆ†é›†ä¸­åœ¨ä»·æ ¼çš„æç«¯ä½ç½®ï¼Œæ­£ç¡®ç‡æ¯”è¾ƒé«˜ï¼Œè€Œä¸”æ”¶ç›Šæä¸ºææ€–
#3.5å¥½åƒè¿˜å¿˜è®°è¿‡æ»¤æœ€å°æ”¶ç›Šç‡äº†ï¼Œå¯ä»¥é€‰æ‹©åœ¨ä¿¡å·sideç”Ÿæˆåç”¨CUSUM filterè¿‡æ»¤æ‰æ”¶ç›Šè¾ƒä½çš„ä¿¡å·ã€‚æ•ˆæœå¯èƒ½ä¼šæ›´å¥½

#æ”¹è¿›æ–¹å‘ï¼š
#First, we build a model that achieve high recall, even if precision is not particularly high.
#Second correct for low precision by applying meta-label to the positives predicted by the primary model."
#Advances in Financial Machine Learning, page 52
# ä¹Ÿå°±æ˜¯å…ˆè®©æ¨¡å‹å°½å¯èƒ½å¤šçš„é¢„æµ‹å‡ºæ­£ç±»ï¼ˆé«˜å¬å›ç‡ï¼‰ï¼Œç„¶åå†ç”¨å…ƒæ¨¡å‹å»è¿‡æ»¤æ‰é”™è¯¯çš„æ­£ç±»ï¼ˆé«˜ç²¾å‡†ç‡ï¼‰ï¼Œä»è€Œæå‡æ•´ä½“çš„ç²¾ç¡®ç‡



#%%
#4.æ ·æœ¬æƒé‡
#è¿™ç« æ¯”è¾ƒåæ•°å­¦ï¼Œçœ‹çš„æˆ‘äº‘é‡Œé›¾é‡Œçš„

'''
4.1 In Chapter 3, we denoted as t1 a pandas series of timestamps where the first
barrier was touched, and the index was the timestamp of the observation. This
was the output of the getEvents function.
(a) Compute a t1 series on dollar bars derived from E-mini S&P 500 futures
tick data.
(b) Apply the function mpNumCoEvents to compute the number of overlapping
outcomes at each point in time.
(c) Plot the time series of the number of concurrent labels on the primary axis,
and the time series of exponentially weighted moving standard deviation of
returns on the secondary axis.
(d) Produce a scatterplot of the number of concurrent labels (x-axis) and the
exponentially weighted moving standard deviation of returns (y-axis). Can
you appreciate a relationship

'''

#a è·å–äº‹ä»¶èµ·æ­¢æ—¶é—´ t1
dollar = pd.read_csv(r'D:\Git\book\ASML\dollar_bars.csv'   ,
                     parse_dates=True,      # è§£ææ—¥æœŸåˆ—
                     index_col=[0]  # å°† 'date_time' åˆ—ä½œä¸ºç´¢å¼•
                     )
close = dd_bars(data = dollar.close, m = 100000) #dollar barçš„series


short_ma = close.rolling(window=5).mean()
long_ma = close.rolling(window=30).mean()

# åˆ›å»ºä¿¡å·DataFrame
df = pd.DataFrame(index=close.index)
df['close'] = close
df['short_ma'] = short_ma
df['long_ma'] = long_ma
# ç”Ÿæˆäº¤æ˜“ä¿¡å·
# é‡‘å‰: çŸ­å‡çº¿ä¸Šç©¿é•¿å‡çº¿ (ä¹°å…¥ä¿¡å·)
# æ­»å‰: çŸ­å‡çº¿ä¸‹ç©¿é•¿å‡çº¿ (å–å‡ºä¿¡å·)
df['signal'] = 0  # 0è¡¨ç¤ºæ— ä¿¡å·
# è®¡ç®—é‡‘å‰å’Œæ­»å‰
golden_cross = (df['short_ma'] > df['long_ma']) & (df['short_ma'].shift(1) <= df['long_ma'].shift(1))
death_cross = (df['short_ma'] < df['long_ma']) & (df['short_ma'].shift(1) >= df['long_ma'].shift(1))
# æ ‡è®°ä¿¡å·
df.loc[golden_cross, 'signal'] = 1  # ä¹°å…¥ä¿¡å·
df.loc[death_cross, 'signal'] = -1  # å–å‡ºä¿¡å·
# æå–æ‰€æœ‰äº¤æ˜“ä¿¡å·çš„æ—¶é—´ç‚¹
buy_signals = df[df['signal'] == 1]
sell_signals = df[df['signal'] == -1]

print("ä¹°å…¥ä¿¡å·å‘ç”Ÿæ—¶é—´:")
print(len(buy_signals.index))
print("\nå–å‡ºä¿¡å·å‘ç”Ÿæ—¶é—´:")
print(len(sell_signals.index))

# è®¡ç®—ç›¸é‚»æ—¶é—´ç‚¹çš„æ—¶é—´å·®
time_diffs = df[df['signal'] != 0].index.to_series().diff().dropna()

avg_interval_seconds = time_diffs.dt.total_seconds().mean() *1.5
avg_interval = avg_interval_seconds/86400 # è½¬æ¢ä¸ºå¤©æ•°

numDays = avg_interval
# 
ptSl=[1,1]
std = getDailyVol(df['close'], span0=100)
std = pd.DataFrame(std).rename(columns={'close': 'daily_vol'})
# å°†ç»“æœåˆå¹¶å›åŸDataFrame
df = df.join(std)

t1 = df['close'].index.searchsorted(df['close'].index + pd.Timedelta(days=numDays))
t1 = t1[t1 < df.shape[0]]  # ç¡®ä¿ä¸è¶…å‡ºèŒƒå›´
t1 = pd.Series(df['close'].index[t1], index=df['close'].index[:t1.shape[0]])
t1.name = 't1'
# å°†t1åˆ—æ·»åŠ åˆ°DataFrame
df = df.join(t1)
df_events = df[df['signal'] != 0]
df_events_with_labels = generate_metalabels(df_events, df['close'], df_events['daily_vol'], ptSl)
# df_events_with_labels.head()

#b è®¡ç®—æ¯ä¸ªæ—¶é—´ç‚¹ï¼ˆbarï¼‰çš„é‡å äº‹ä»¶æ•°é‡  éœ€è¦åœ¨aä¸­ç”Ÿæˆé‡å çš„äº‹ä»¶
def mpNumCoEvents(event,close):
    close=close[event.index.min():event.index.max()] # align
    count=pd.Series(0,index=close.index)
    for loc,row in event.iterrows():
        count.loc[loc:row['t1']]+=1 # count events
    return count
event_count_cloes=mpNumCoEvents(df_events_with_labels,close)
event_count_cloes.head()


#c ç»˜åˆ¶é‡å äº‹ä»¶æ•°é‡ä¸æ”¶ç›Šæ³¢åŠ¨ç‡çš„æ—¶é—´åºåˆ—å›¾
# è®¡ç®—æ”¶ç›Šç‡å’ŒæŒ‡æ•°åŠ æƒç§»åŠ¨æ ‡å‡†å·®
returns = close.pct_change()  # æ”¶ç›Šç‡
ewm_std = returns.ewm(span=20).std()  # 20å¤©åŠè¡°æœŸçš„æŒ‡æ•°åŠ æƒç§»åŠ¨æ ‡å‡†å·®
# ç¡®ä¿æ—¶é—´ç´¢å¼•å¯¹é½
ewm_std = ewm_std[event_count_cloes.index.min():event_count_cloes.index.max()] 

# åˆ›å»ºç”»å¸ƒå’Œä¸»åæ ‡è½´
fig, ax1 = plt.subplots(figsize=(12, 6))

# ç»˜åˆ¶ä¸»åæ ‡è½´ï¼ˆå¹¶å‘æ ‡ç­¾æ•°é‡ï¼‰
ax1.plot(event_count_cloes, color='blue', label='Concurrent Labels')
ax1.set_xlabel('Time')
ax1.set_ylabel('Number of Concurrent Labels', color='blue')
ax1.tick_params(axis='y', labelcolor='blue')

# åˆ›å»ºæ¬¡åæ ‡è½´å¹¶ç»˜åˆ¶æ³¢åŠ¨ç‡
ax2 = ax1.twinx()
ax2.plot(ewm_std, color='red', label='EWM Std of Returns')
ax2.set_ylabel('EWM Std of Returns', color='red')
ax2.tick_params(axis='y', labelcolor='red')

# æ·»åŠ å›¾ä¾‹å’Œæ ‡é¢˜
ax1.legend(loc='upper left')
ax2.legend(loc='upper right')
plt.title('Concurrent Labels vs. Returns Volatility')
plt.show()


#d ç”»æ•£ç‚¹å›¾ï¼Œä»¥å¹¶å‘æ•°é‡ä¸ºxè½´ï¼Œæ”¶ç›Šæ³¢åŠ¨ç‡ä¸ºyè½´
plt.figure(figsize=(10, 6))
plt.scatter(event_count_cloes, ewm_std, alpha=0.5)  # alphaè®¾ç½®é€æ˜åº¦
plt.xlabel('Number of Concurrent Labels (event_count_cloes)')
plt.ylabel('EWM Std of Returns')
plt.title('Scatter Plot: Concurrent Labels vs Returns Volatility')
plt.grid(True)  # æ·»åŠ ç½‘æ ¼çº¿
plt.show()

'''
4.2 Using the function mpSampleTW, compute the average uniqueness of each label.
What is the first-order serial correlation, AR(1), of this time series? Is it statistically significant? Why?

'''
#è®¡ç®—æ¯ä¸ªæ ‡ç­¾çš„å¹³å‡å”¯ä¸€æ€§ã€‚å¹³å‡å”¯ä¸€æ€§æ˜¯æŒ‡åœ¨æŸä¸ªæ—¶é—´ç‚¹ä¸Šï¼Œæ‰€æœ‰è¦†ç›–è¯¥æ—¶é—´ç‚¹çš„äº‹ä»¶çš„å”¯ä¸€æ€§åŠ æƒçš„å¹³å‡å€¼ã€‚å¤§äº0ï¼Œå°äº1.
# mpSampleTWè¿™é‡Œåªè®¡ç®—äº†æ¯ä¸ªäº‹ä»¶çš„å¹³å‡å”¯ä¸€æ€§åŠ æƒã€‚
#ä¸Šè¿°å¾—åˆ°äº†æ¯ä¸ªæ—¶é—´çš„å”¯ä¸€å€¼uniquenessï¼Œæ˜¯äº‹ä»¶çº§åˆ«çš„ï¼Œè€Œä¸æ˜¯barçº§åˆ«çš„ã€‚æ‰€ä»¥å¯¹äºæ¯ä¸ªbarï¼Œ å°†è¦†ç›–è¯¥barçš„æ‰€æœ‰äº‹ä»¶çš„uniquenesså–å¹³å‡å€¼ ï¼Œå°±å¾—åˆ°äº†è¯¥bar çš„å”¯ä¸€å€¼
def mpSampleTW(event,close):
    '''
    #è®¡ç®—æ¯ä¸ªäº‹ä»¶çš„å”¯ä¸€æ€§åŠ æƒï¼Œè¿”å›æ¯ä¸ªäº‹ä»¶çš„æƒé‡
    '''
    close=close[event.index.min():event.index.max()] # align
    count=pd.Series(0,index=close.index)
    for loc,row in event.iterrows():
        count.loc[loc:row['t1']]+=1 # count events
    
    # è®¡ç®—æ¯ä¸ªäº‹ä»¶çš„æƒé‡ï¼ˆæŒç»­æœŸé—´çš„å¹³å‡å¹¶å‘äº‹ä»¶å€’æ•°ï¼‰
    wght = pd.Series(index=event.index)
    for tIn, tOut in event['t1'].items():
        wght.loc[tIn] = (1./count.loc[tIn:tOut]).mean()
    
    return wght

uniqueness = mpSampleTW(df_events_with_labels, close)
uniqueness.head()
def calculate_bar_uniqueness(events, uniqueness, bar_timestamps):
    """
    è®¡ç®—æ¯ä¸ªbarçš„å”¯ä¸€å€¼ï¼ˆè¦†ç›–è¯¥barçš„æ‰€æœ‰äº‹ä»¶çš„uniquenesså¹³å‡å€¼ï¼‰
    
    å‚æ•°:
        events: äº‹ä»¶DataFrameï¼ŒåŒ…å«'t1'åˆ—ï¼ˆäº‹ä»¶ç»“æŸæ—¶é—´ï¼‰
        uniqueness: Seriesï¼Œäº‹ä»¶çº§åˆ«çš„å”¯ä¸€å€¼ï¼Œç´¢å¼•ä¸ºäº‹ä»¶å¼€å§‹æ—¶é—´
        bar_timestamps: barçš„æ—¶é—´æˆ³åˆ—è¡¨/Index
    """
    bar_uniqueness = pd.Series(index=bar_timestamps, dtype=float)
    
    for bar_time in bar_timestamps:
        # æ‰¾å‡ºè¦†ç›–å½“å‰barçš„æ‰€æœ‰äº‹ä»¶ï¼ˆäº‹ä»¶å¼€å§‹æ—¶é—´ <= bar_time <= äº‹ä»¶ç»“æŸæ—¶é—´ï¼‰
        overlapping_events = events[(events.index <= bar_time) & (events['t1'] >= bar_time)]
        
        if not overlapping_events.empty:
            # è®¡ç®—è¿™äº›äº‹ä»¶çš„uniquenesså¹³å‡å€¼
            bar_uniqueness[bar_time] = uniqueness[overlapping_events.index].mean()
        else:
            bar_uniqueness[bar_time] = 1  # æ— è¦†ç›–äº‹ä»¶æ—¶è®¾ä¸º1
    
    return bar_uniqueness
# è®¡ç®—æ¯ä¸ªæ ‡ç­¾çš„å¹³å‡å”¯ä¸€æ€§

bar_uniqueness = calculate_bar_uniqueness(df_events_with_labels, uniqueness, close.index)


# è®¡ç®—AR(1)
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller

# å¯¹bar_uniquenessè¿›è¡Œå•ä½æ ¹æµ‹è¯• 
#å¹³ç¨³è¯´æ˜åºåˆ—çš„ç»Ÿè®¡ç‰¹æ€§ï¼ˆå¦‚å‡å€¼å’Œæ–¹å·®ï¼‰ä¸ä¼šéšç€æ—¶é—´å‘ç”Ÿå˜åŒ–
result = adfuller(bar_uniqueness.dropna())
print('ADF Statistic:', result[0])
print('p-value:', result[1]) ## p<0.05åˆ™å¹³ç¨³
print('Critical Values:')
for key, value in result[4].items():
    print(f'   {key}: {value}')


'''
4.3 Fit a random forest to a financial dataset where  (âˆ‘I i=1 uÌ„i)I â‰ª 1. #Iæ˜¯äº‹ä»¶æ•°é‡ï¼ŒuÌ„iæ˜¯äº‹ä»¶çš„å¹³å‡å”¯ä¸€æ€§
(a) What is the mean out-of-bag accuracy?
(b) What is the mean accuracy of k-fold cross-validation (without shuffling) on
the same dataset?
(c) Why is out-of-bag accuracy so much higher than cross-validation accuracy?
Which one is more correct / less biased? What is the source of this bias?
'''

#a å½“å‰ä½¿ç”¨çš„æ•°æ®é›†å°±æ»¡è¶³ (âˆ‘I i=1 uÌ„i)I â‰ª 1ï¼Œå› ä¸ºæ‰€ä»¥çš„barå¹³å‡å”¯ä¸€æ€§éƒ½æ˜¯å¤§äº0å°äº1çš„
#è¢‹å¤–ç²¾ç¡®åº¦å‡å€¼ ï¼šåœ¨è®­ç»ƒä¸­æ²¡æœ‰é€‰æ‹©è¯¥æ•°æ®ç‚¹çš„æ‰€æœ‰å†³ç­–æ ‘å¯¹è¯¥æ•°æ®ç‚¹çš„é¢„æµ‹å‡†ç¡®åº¦ï¼Œæ‰€æœ‰æ•°æ®ç‚¹çš„è¢‹å¤–å‡†ç¡®åº¦çš„å¹³å‡å€¼å°±æ˜¯è¢‹å¤–ç²¾ç¡®åº¦å‡å€¼
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.model_selection import cross_val_score, TimeSeriesSplit
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold

#å»¶ç»­ä½¿ç”¨4.1çš„æ•°æ®é›†
# å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡ 'metallabel'ä½œä¸ºç›®æ ‡å€¼
X = df_events_with_labels[['close', 'short_ma', 'long_ma', 'signal', 'daily_vol']]
y = df_events_with_labels['metallabel']
# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)   
# åˆå§‹åŒ–éšæœºæ£®æ—åˆ†ç±»å™¨ï¼Œå¯ç”¨è¢‹å¤–è¯„åˆ†
rf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)
# è®­ç»ƒæ¨¡å‹
rf.fit(X_train, y_train)
# æ‰“å°è¢‹å¤–ç²¾ç¡®åº¦å‡å€¼
print("è¢‹å¤–ç²¾ç¡®åº¦å‡å€¼:", rf.oob_score_)  #0.528344671201814

#b kæŠ˜äº¤å‰éªŒè¯ç²¾ç¡®åº¦å‡å€¼ ï¼šåœ¨kæŠ˜äº¤å‰éªŒè¯ä¸­ï¼Œæ¯æ¬¡å°†æ•°æ®é›†åˆ†ä¸ºkä¸ªå­é›†ï¼Œæ¯æ¬¡ä½¿ç”¨k-1ä¸ªå­é›†è®­ç»ƒæ¨¡å‹ï¼Œç”¨å‰©ä¸‹çš„å­é›†éªŒè¯æ¨¡å‹ï¼Œæœ€åå–kæ¬¡éªŒè¯ç»“æœçš„å¹³å‡å€¼ä½œä¸ºæ¨¡å‹çš„ç²¾ç¡®åº¦å‡å€¼ã€‚
# åˆå§‹åŒ–éšæœºæ£®æ—åˆ†ç±»å™¨ï¼Œå¯ç”¨kæŠ˜äº¤å‰éªŒè¯
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# è®¾ç½®k-foldäº¤å‰éªŒè¯ï¼ˆä¸æ´—ç‰Œï¼‰
k = 5  # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´æŠ˜æ•°
kf = KFold(n_splits=k, shuffle=False, random_state=None)

# è®¡ç®—äº¤å‰éªŒè¯å‡†ç¡®ç‡
cv_scores = cross_val_score(rf_classifier, X, y, 
                           cv=kf, scoring='accuracy', n_jobs=-1)

# è¾“å‡ºç»“æœ
print(f"K-foldäº¤å‰éªŒè¯å‡†ç¡®ç‡ (k={k}): {cv_scores}")
print(f"å¹³å‡å‡†ç¡®ç‡: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})") #0.4784

#c è¢‹å¤–ç²¾ç¡®åº¦å‡å€¼æ¯” kæŠ˜äº¤å‰éªŒè¯ç²¾ç¡®åº¦å‡å€¼é«˜ã€‚
#Kfold åªæ˜¯å°†æ•°æ®æ‹†æˆæ ·æœ¬ï¼Œä¸ä¼šæ›¿æ¢æˆ–é‡æ–°é€‰æ‹©ä»–ä»¬ã€‚è€Œéšæœºæ£®æ—oobé™…ä¸Šé€‰æ‹©å’Œæ›¿æ¢æ ·æœ¬ï¼ˆBootstrap = Trueé»˜è®¤ï¼‰ï¼Œå½“æ•°æ®é›†çš„å”¯ä¸€æ€§ä¸é«˜æ—¶ï¼Œç”±äºå¹¶å‘äº‹ä»¶ï¼Œè¿™å°†ä½¿OOBåˆ†æ•°ä¸in-bagæ ·æœ¬éå¸¸ç›¸åŒï¼Œå¹¶ä¸”å½¼æ­¤å†—ä½™ã€‚
#å‚è§ã€Šé‡‘èæœºå™¨å­¦ä¹ è¿›å±•ã€‹ï¼Œç¬¬62 - 63é¡µï¼Œç¬¬4.5èŠ‚ã€‚


'''
4.4 Modify the code in Section 4.7 to apply an exponential time-decay factor.
'''
#é™¤äº†å¯¹å¹³å‡å”¯ä¸€æ€§çš„åº”ç”¨å¤–ï¼Œæ ·æœ¬æ•ˆæœéšæ—¶é—´è¡°å‡ä¹Ÿæ˜¯é‡è¦çš„åº”ç”¨
def getTimeDecay(tW,clfLastW=1.):
    # apply piecewise-linear decay to observed uniqueness (tW)
    # newest observation gets weight=1, oldest observation gets weight=clfLastW
    clfW=tW.sort_index().cumsum()
    if clfLastW>=0:
        slope=(1.-clfLastW)/clfW.iloc[-1]
    else:
        slope=1./((clfLastW+1)*clfW.iloc[-1])
    const=1.-slope*clfW.iloc[-1]
    clfW=const+slope*clfW
    clfW[clfW<0]=0
    print (const,slope)
    return clfW

bar_uniqueness_decay1=getTimeDecay(bar_uniqueness, clfLastW=1)
bar_uniqueness_decay2=getTimeDecay(bar_uniqueness, clfLastW=0.6)
bar_uniqueness_decay3=getTimeDecay(bar_uniqueness, clfLastW=0.3)
bar_uniqueness_decay4=getTimeDecay(bar_uniqueness, clfLastW=0)
bar_uniqueness_decay5=getTimeDecay(bar_uniqueness, clfLastW=-0.5)
bar_uniqueness_decay6=getTimeDecay(bar_uniqueness, clfLastW=-0.9)
#ç”»å›¾
import matplotlib.pyplot as plt
plt.figure(figsize=(12, 8))
# plt.plot(bar_uniqueness.index, bar_uniqueness, label='Original')
plt.plot(bar_uniqueness.index, bar_uniqueness_decay1, label='Decay 1')
plt.plot(bar_uniqueness.index, bar_uniqueness_decay2, label='Decay 0.6')
plt.plot(bar_uniqueness.index, bar_uniqueness_decay3, label='Decay 0.3')
plt.plot(bar_uniqueness.index, bar_uniqueness_decay4, label='Decay 0')
plt.plot(bar_uniqueness.index, bar_uniqueness_decay5, label='Decay -0.5')
plt.plot(bar_uniqueness.index, bar_uniqueness_decay6, label='Decay -0.9')
plt.legend()
plt.title('Bar Uniqueness with Different Decay Factors')
plt.xlabel('Date')
plt.ylabel('Uniqueness')
plt.show()


'''
4.5 Consider you have applied meta-labels to events determined by a trend-following
model. Suppose that two thirds of the labels are 0 and one third of the labels
are 1.
(a) What happens if you fit a classifier without balancing class weights?
(b) A label 1 means a true positive, and a label 0 means a false positive. By
applying balanced class weights, we are forcing the classifier to pay more
attention to the true positives, and less attention to the false positives. Why
does that make sense?
(c) What is the distribution of the predicted labels, before and after applying
balanced class weights?
'''


#a ä¸å¹³è¡¡ç±»æƒé‡çš„åˆ†ç±»å™¨ä¼šå°†æ›´å¤šçš„æƒé‡åˆ†é…ç»™å¤šæ•°ç±»ï¼ˆ0ï¼‰ï¼Œä»è€Œå¯¼è‡´æ¨¡å‹å¯¹å°‘æ•°ç±»ï¼ˆ1ï¼‰çš„é¢„æµ‹èƒ½åŠ›ä¸‹é™ã€‚

#b é€šè¿‡åº”ç”¨å¹³è¡¡ç±»æƒé‡ï¼Œæ¨¡å‹ä¼šæ›´åŠ å…³æ³¨å°‘æ•°ç±»ï¼ˆ1ï¼‰ï¼Œä»è€Œæé«˜å¯¹æ­£ç±»çš„è¯†åˆ«èƒ½åŠ›ã€‚
#è€ŒçœŸæ­£ç±»çœŸæ˜¯èƒ½å¤Ÿå¸¦æ¥ç›ˆåˆ©çš„ç‚¹ï¼Œå‡æ•´æ­£ç±»å¹¶ä¸ä¼šæé«˜ç›ˆåˆ©æ°´å¹³ï¼Œåªæ˜¯æé«˜äº†æ¨¡å‹ç»¼åˆå¾—åˆ†ã€‚
#æ‰€ä»¥åœ¨metalabelçš„è¯†åˆ«è¦æ—¢è¦å°†ç„¦ç‚¹èšç„¦åˆ°çœŸæ­£ç±»çš„æé«˜å°±å¤Ÿäº†ã€‚
#åº”è¯¥å¦‚ä½•å¹³è¡¡ç±»æƒé‡å‘¢ï¼Ÿ 1.class_weight='balanced'ï¼ˆæˆ–è€…'balance_subsample'ï¼‰,  # è‡ªåŠ¨å¹³è¡¡æƒé‡ è®¾ç½®class_weightå°±æ˜¯è¦æ±‚MLæ¨¡å‹æ›´åŠ å…³æ³¨å°‘æ•°ç±»
# #2.class_weight={0:1, 1:3}  # æ‰‹åŠ¨è®¾ç½®æƒé‡ 3.å¯¹æ ·æœ¬è¿›è¡Œé‡é‡‡æ ·

#c  åˆ†å¸ƒä¼šæ›´åŠ å‡è¡¡ï¼Œå°‘æ•°ç±»ï¼ˆ1ï¼‰çš„æ¯”ä¾‹ä¼šå¢åŠ ï¼Œä»è€Œä½¿æ¨¡å‹åœ¨é¢„æµ‹æ—¶æ›´æœ‰å¯èƒ½è¾“å‡º1ã€‚


'''
4.6 Update the draw probabilities for the final draw in Section 4.5.3
4.7 In Section 4.5.3, suppose that number 2 is picked again in the second draw. What
would be the updated probabilities for the third draw
'''
#çº¯æ•°å­¦ï¼Œæ¦‚ç‡å†å¹³è¡¡çš„é¡ºåºæŠ½æ ·ï¼ˆsequential bootstrapï¼‰ åšä¸å‡ºï¼Œå…ˆè·³è¿‡

'''
ç¬¬å››ç« æ€»ç»“ï¼š
1.æœ¬ç« æ•°å­¦å«é‡æå¤§ï¼Œç»ˆäºçŸ¥é“ä¸ºä»€ä¹ˆåšé‡åŒ–éœ€è¦æ‰¾æ¸…åŒ—æ•°å­¦ç³»çš„äº†ã€‚ ä½†æ˜¯æ²¡å…³ç³»ï¼Œä¸éœ€è¦åšåˆ°é‚£ä¹ˆæé™çš„æ•°å­¦ä¹Ÿè¶³å¤Ÿäº†
2.è¿™ç« ä¸»è¦æ˜¯å…³æ³¨äº†æ ·æœ¬æƒé‡çš„é—®é¢˜ã€‚ä¸€æ˜¯æ ·æœ¬çš„å”¯ä¸€æ€§ï¼ˆäº‹ä»¶ä½¿ç”¨çš„baré‡å ï¼‰ï¼ŒäºŒæ˜¯æ ·æœ¬çš„æ—¶é—´è¡°å‡æ€§ï¼Œä¸‰æ˜¯æ ·æœ¬ï¼ˆäº‹ä»¶ï¼‰ç±»åˆ«ä¸å¹³è¡¡æ€§


'''


#%%
#ç¬¬äº”ç«  Fractionally Differentiated Features åˆ†æ•°é˜¶å¾®åˆ†ç‰¹å¾ å³åˆ†æ•°çº§å·®åˆ†
'''
#é‡‘èæ•°æ®çŸ­æœŸå†…ç”±äºå¥—åˆ©ç­‰é«˜é¢‘æ“ä½œä½¿å¾—ä¿¡å™ªæ¯”å¾ˆä½ï¼Œä»·æ ¼åºåˆ—é€šå¸¸æ˜¯éå¹³ç¨³çš„ï¼Œè€Œä¸”é€šå¸¸å…·æœ‰è®°å¿†æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ•´æ•°å·®åˆ†åçš„åºåˆ—ï¼Œå¦‚æ”¶ç›Šç‡ï¼Œå…¶è®°å¿†æ˜¯æœ‰é™çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå†å²æ•°æ®åœ¨æœ‰é™æ ·æœ¬çª—å£ä¹‹å¤–å°†è¢«å®Œå…¨å¿½ç•¥
#ä¸€æ—¦å¹³ç¨³æ€§å˜æ¢æŠ¹å»äº†æ•°æ®ä¸­çš„æ‰€æœ‰è®°å¿†ï¼Œå°±ä¼šæ±‚åŠ©äºå¤æ‚çš„æ•°å­¦æŒ‡æ ‡æ¥æå–ä¿¡å·
#è€Œåˆ†æ•°é˜¶æŸ¥åˆ†åœ¨ç¡®ä¿æ•°æ®å¹³ç¨³çš„åŒæ—¶å°½å¯èƒ½ä¿ç•™è®°å¿†ã€‚

ä¸¤ä¸ªç‰¹æ€§å¾ˆé‡è¦ï¼šå¹³ç¨³æ€§ä¸è®°å¿†æ€§
1.å¹³ç¨³æ€§æŒ‡æ•°æ®çš„ç»Ÿè®¡ç‰¹æ€§ï¼ˆå¦‚å‡å€¼å’Œæ–¹å·®ï¼‰ä¸ä¼šéšç€æ—¶é—´å‘ç”Ÿå˜åŒ–ã€‚ä¹Ÿå°±æ˜¯ä¸ç®¡èµ·ç‚¹åœ¨å“ªéƒ½å…·å¤‡ç±»ä¼¼çš„ç»Ÿè®¡ç‰¹æ€§ã€‚
    é€šè¿‡æ•°æ®è½¬å˜æ˜¯èƒ½å¤Ÿä½¿æ•°æ®å˜ä¸ºå¹³ç¨³åºåˆ—çš„ï¼Œæ¯”å¦‚å¯¹æ•°ï¼Œå·®åˆ†ç­‰ï¼Œä½†æ˜¯è¿™äº›æ“ä½œå¾ˆå®¹æ˜“å°±å°†è®°å¿†æ€§æŠ¹é™¤äº†
2.è®°å¿†æ€§æŒ‡æ•°æ®ä¸­å­˜åœ¨é•¿æœŸä¾èµ–å…³ç³»ï¼Œè¿‡å»çš„äº‹ä»¶ä¼šå½±å“æœªæ¥çš„äº‹ä»¶ã€‚è®°å¿†æ€§èƒ½å¤Ÿå¸®åŠ©æˆ‘ä»¬æ•æ‰æ•°æ®ä¸­çš„æ¨¡å¼å’Œè¶‹åŠ¿ï¼Œä»è€Œæé«˜é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚
    ä¸€èˆ¬æ¥è¯´æ”¶ç›Šæ˜¯å¹³ç¨³çš„ï¼Œä½†æ²¡æœ‰è®°å¿†ï¼›ä»·æ ¼æ˜¯æœ‰è®°å¿†çš„ï¼Œä½†ä¸å¹³ç¨³ã€‚


åˆ†æ•°å·®åˆ†ä¼šå°†æ»åé˜¶çš„æ•°æ®ç³»æ•°èµ‹å€¼ä¸ºé0 ï¼Œæ‰€ä»¥å…·æœ‰è®°å¿†æ€§ã€‚å½“ä¸ºæ•´æ•°é˜¶å·®åˆ†çš„æ—¶å€™ï¼Œåé¢çš„æ»åé¡¹çš„ç³»æ•°ä¼šç½®0.

æœ¬ç« ç²¾é«“ï¼š
ç¡®å®šæœ€å°çš„ dï¼ˆåˆ†æ•°å·®åˆ†é˜¶ï¼‰ï¼Œä½¿å¾— FFD(d) ä¸Š ADF ç»Ÿè®¡é‡çš„ p å€¼ä½äº 5%ï¼Œå³95%æ˜¾è‘—å¹³ç¨³ã€‚
ç„¶åï¼Œä½¿ç”¨ FFD(d) åºåˆ—ä½œä¸ºé¢„æµ‹ç‰¹å¾ã€‚
'''


'''
5.1 Generate a time series from an IID Gaussian random process. This is a memoryless, stationary series:
(a) Compute the ADF statistic on this series. What is the p-value?
(b) Compute the cumulative sum of the observations. This is a non-stationary
series without memory.
(i) What is the order of integration of this cumulative series?
(ii) Compute the ADF statistic on this series. What is the p-value?
(c) Differentiate the series twice. What is the p-value of this over-differentiated
series?

'''

#a ç”Ÿæˆä¸€ä¸ªç‹¬ç«‹åŒåˆ†å¸ƒçš„é«˜æ–¯éšæœºè¿‡ç¨‹æ—¶é—´åºåˆ—ï¼Œå¹¶è®¡ç®—å…¶ADFç»Ÿè®¡é‡å’Œpå€¼
import numpy as np
import pandas as pd
from statsmodels.tsa.stattools import adfuller
import matplotlib.pyplot as plt
def create_price_data51(start_price: float = 1000.00,
                      mu: float = .0,
                      var: float = 1.0,
                      n_samples: int = 1000):
                      
    i = np.random.normal(mu, var, n_samples)
    df0 = pd.date_range(periods=n_samples,
                        freq=pd.tseries.offsets.Minute(),
                        end=pd.Timestamp.now())
                        
    X = pd.Series(i, index=df0, name = "close").to_frame()
    # X.close.iat[0] = start_price
    return X
data51 = create_price_data51()
# è®¡ç®—ADFç»Ÿè®¡é‡å’Œpå€¼
adf_result = adfuller(data51)
print('ADF Statistic: %f' % adf_result[0])
print('p-value: %f' % adf_result[1])


#b è®¡ç®—è§‚å¯Ÿå€¼çš„ç´¯ç§¯å’Œï¼Œå¹¶åˆ†æå…¶å¹³ç¨³æ€§
cumsum_series = data51['close'].cumsum()
# è®¡ç®—ADFç»Ÿè®¡é‡å’Œpå€¼
adf_result = adfuller(cumsum_series)
print('ADF Statistic: %f' % adf_result[0])
print('p-value: %f' % adf_result[1])

#å˜æˆäº†éå¹³ç¨³äº†ã€‚data51ç›¸å½“äºæ—¥æ”¶ç›˜ä»·çš„å·®ï¼Œè€Œç´¯è®¡å’Œç±»ä¼¼äºæ¯æ—¥æ”¶ç›˜ä»·ã€‚æ¯æ—¥æ”¶ç›˜ä»·æ˜¯éå¹³ç¨³çš„ï¼Œæ‰€ä»¥å¿…é¡»è¦è¿›è¡Œå¤„ç†

#b1 è¯¥ç´¯ç§¯åºåˆ—çš„ç§¯åˆ†é˜¶æ•°ä¸º1ï¼Œå› ä¸ºå®ƒæ˜¯é€šè¿‡å¯¹åŸå§‹åºåˆ—è¿›è¡Œä¸€æ¬¡ç´¯ç§¯å¾—åˆ°çš„ã€‚
#ç§¯åˆ†é˜¶æŒ‡ä½¿ä¸€ä¸ªéå¹³ç¨³è¿‡ç¨‹å˜ä¸ºå¹³ç¨³æ‰€éœ€çš„ç´¯ç§¯ï¼ˆæˆ–å·®åˆ†ï¼‰çš„æ¬¡æ•°ï¼Œæ‰€ä»¥è¿™é‡Œæ˜¯1

#c å¯¹åŸç”Ÿæˆåºåˆ—è¿›è¡Œä¸¤æ¬¡å·®åˆ†ï¼Œå¹¶è®¡ç®—å…¶ADFç»Ÿè®¡é‡å’Œpå€¼
diff_series = data51.diff().diff().dropna()
# è®¡ç®—ADFç»Ÿè®¡é‡å’Œpå€¼
adf_result = adfuller(diff_series)
print('ADF Statistic: %f' % adf_result[0])
print('p-value: %f' % adf_result[1])

#ä»æ—§æ˜¯å¹³ç¨³çš„ï¼Œå·®åˆ†æ²¡æœ‰æ”¹å˜å¹³ç¨³æ€§ã€‚


'''
5.2 Generate a time series that follows a sinusoidal function. This is a stationary
series with memory.
(a) Compute the ADF statistic on this series. What is the p-value?
(b) Shift every observation by the same positive value. Compute the cumulative
sum of the observations. This is a non-stationary series with memory.
(i) Compute the ADF statistic on this series. What is the p-value?
(ii) Apply an expanding window fracdiff, with ğœ = 1E âˆ’ 2. For what minimum d value do you get a p-value below 5%?
(iii) Apply FFD, with ğœ = 1E âˆ’ 5. For what minimum d value do you get a p-value below 5%?

'''

#a ç”Ÿæˆä¸€ä¸ªéµå¾ªæ­£å¼¦å‡½æ•°çš„æ—¶é—´åºåˆ—ï¼Œå¹¶è®¡ç®—å…¶ADFç»Ÿè®¡é‡å’Œpå€¼
def create_sinusoidal_data52(amplitude: float = 1.0,
                             frequency: float = 1.0,
                             phase: float = 0.0,
                             n_samples: int = 50000,
                             n_periods: int = 5):  # n_periods ç”¨æ¥è¡¨ç¤ºå‘¨æœŸæ•°
    # è®¡ç®—æ—¶é—´ tï¼Œè·¨è¶Šå¤šä¸ªå‘¨æœŸ
    t = np.linspace(0, 2 * np.pi * n_periods, n_samples)
    
    # ç”Ÿæˆæ­£å¼¦æ³¢æ•°æ®
    X = amplitude * np.sin(frequency * t + phase)
    
    # ç”Ÿæˆæ—¥æœŸæ—¶é—´ç´¢å¼•
    df0 = pd.date_range(periods=n_samples,
                        freq=pd.tseries.offsets.Minute(),
                        end=pd.Timestamp.now())
    
    # å°†æ­£å¼¦æ³¢æ•°æ®è½¬æ¢ä¸º pandas DataFrame
    X = pd.Series(X, index=df0, name="close").to_frame()
    
    return X

# ç”ŸæˆåŒ…å«å¤šä¸ªå‘¨æœŸçš„æ­£å¼¦æ³¢æ•°æ®
data52 = create_sinusoidal_data52(n_periods=100)  

# ç»˜å›¾
plt.figure(figsize=(10, 6))
plt.plot(data52.index, data52['close'], label='Sinusoidal Wave', color='b')
plt.title('Sinusoidal Wave with Multiple Periods')
plt.xlabel('Time')
plt.ylabel('Amplitude')
plt.grid(True)
plt.legend()
plt.show()
# è®¡ç®—ADFç»Ÿè®¡é‡å’Œpå€¼
adf_result = adfuller(data52)
print('ADF Statistic: %f' % adf_result[0])
print('p-value: %f' % adf_result[1])

#b å°†æ¯ä¸ªè§‚å¯Ÿå€¼å¹³ç§»ä¸€ä¸ªæ­£å€¼ï¼Œå¹¶è®¡ç®—å…¶ç´¯ç§¯å’Œçš„ADFç»Ÿè®¡é‡å’Œpå€¼
shifted_series = data52 + 3  # å¹³ç§»ä¸€ä¸ªæ­£å€¼
shifted_series = shifted_series['close'].cumsum()
# è®¡ç®—ADFç»Ÿè®¡é‡å’Œpå€¼
adf_result = adfuller(shifted_series)
print('ADF Statistic: %f' % adf_result[0])
print('p-value: %f' % adf_result[1])
#ç»è¿‡è½¬æ¢åæ•°æ®å˜æˆäº†éå¹³ç¨³çš„ã€‚å‡è®¾ä»·æ ¼æ˜¯åœ¨æŸä¸ªå‡ä»·ä¸Šä¸‹æ³¢åŠ¨ï¼Œè€Œä¸”å…·æœ‰è®°å¿†æ€§ã€‚ä½†æ˜¯åˆæˆçš„ç´¯è®¡å’Œä»·æ ¼åºåˆ—æ˜¯éå¹³ç¨³çš„

#b2 åº”ç”¨æ‰©å±•çª—å£åˆ†æ•°å·®åˆ†ï¼Œæ‰¾åˆ°ä½¿på€¼ä½äº5%çš„æœ€å°då€¼ï¼Œä¸¢å¼ƒé˜ˆå€¼0.01
#åªéœ€è¦æ‰¾åˆ°æ˜¾è‘—å¹³ç¨³çš„æœ€å°då€¼å³å¯ï¼Œdè¶Šå¤§ï¼Œè¶Šæ¥è¿‘äº1ï¼Œè®°å¿†æ€§æ˜¯è¶Šå·®çš„ã€‚åœ¨ã€0,1ã€‘è¿™ä¸ªèŒƒå›´å†…
def getWeights(d,size):
    # thres>0 drops insignificant weights
    w=[1.]
    for k in range(1,size):
        w_=-w[-1]/k*(d-k+1)
        w.append(w_)
    w=np.array(w[::-1]).reshape(-1,1)
    return w
def fracDiff_ew(series,d,thres=.01):
    '''
    Increasing width window, with treatment of NaNs
    Note 1: For thres=1, nothing is skipped.
    Note 2: d can be any positive fractional, not necessarily bounded [0,1].
    '''
    #1) Compute weights for the longest series
    w=getWeights(d,series.shape[0])
    #2) Determine initial calcs to be skipped based on weight-loss threshold
    w_=np.cumsum(abs(w))
    w_/=w_[-1]
    skip=w_[w_>thres].shape[0]
    #3) Apply weights to values
    df={}
    series=pd.DataFrame(series)
    for name in series.columns:
        seriesF,df_=series[[name]].ffill().dropna(),pd.Series()  
        for iloc in range(skip,seriesF.shape[0]):
            loc=seriesF.index[iloc]
            if not np.isfinite(series.loc[loc,name]):continue # exclude NAs
            df_[loc]=np.dot(w[-(iloc+1):,:].T,seriesF.loc[:loc])[0,0]
        df[name]=df_.copy(deep=True)
    df=pd.concat(df,axis=1)
    return df

expending_windows=fracDiff_ew(shifted_series,d=0.1,thres=1E-2)
# è®¡ç®—ADFç»Ÿè®¡é‡å’Œpå€¼
adf_result = adfuller(expending_windows)
print('ADF Statistic: %f' % adf_result[0])
print('p-value: %f' % adf_result[1])
#ä»¥0.01ä¸ºé—´éš”ï¼Œé€æ­¥å‡å°‘då€¼ï¼Œç›´åˆ°på€¼ä½äº0.05
d_values = np.arange(0, 0.1, 0.005)
for d in d_values:
    expending_windows=fracDiff_ew(shifted_series,d=d,thres=1E-2)
    # è®¡ç®—ADFç»Ÿè®¡é‡å’Œpå€¼
    adf_result = adfuller(expending_windows)
    print(f'd={d:.3f}, ADF Statistic: {adf_result[0]:.4f}, p-value: {adf_result[1]:.4f}')
    if adf_result[1] <= 0.05:
        print(f'æœ€å°då€¼: {d:.3f}')
        break
    #ä¸€ä¸ªå‘¨æœŸæ—¶ï¼š
#æœ€å°då€¼: 0-0.5åŒºé—´0.1å°±æ˜¯å¹³ç¨³äº†ï¼Œä½†æ˜¯expending_windowsåªæœ‰40%çš„æ•°æ®äº†ã€‚åœ¨0.5-1åŒºé—´ï¼Œ0.69æ˜¯æœ€å°å¹³ç¨³å€¼ã€‚æ•°æ®æ ·æœ¬è¿˜æœ‰95%
#ä¸åŒçš„é˜ˆå€¼thresä¹Ÿå½±å“æœ€å°åˆ†æ•°é˜¶då€¼
#100ä¸ªå‘¨æœŸæ—¶ï¼š
#æœ€å°då€¼0.005ï¼Œç›´æ¥å‡º

#b31 ä¹¦ä¸­ä»£ç 
def getWeights_FFD(d,thres):
    w,k=[1.],1
    while True:
        w_=-w[-1]/k*(d-k+1)
        if abs(w_)<thres:break
        w.append(w_);k+=1
    return np.array(w[::-1]).reshape(-1,1)
def fracDiff_FFD(series,d,thres=1e-5):
    # Constant width window (new solution)
    w=getWeights_FFD(d,thres)
    width=len(w)-1
    df={}
    series=pd.DataFrame(series)
    for name in series.columns:
        seriesF,df_=series[[name]].ffill().dropna(),pd.Series()
        for iloc1 in range(width,seriesF.shape[0]):
            loc0,loc1=seriesF.index[iloc1-width],seriesF.index[iloc1]
            if not np.isfinite(series.loc[loc1,name]):continue # exclude NAs
            df_[loc1]=np.dot(w.T,seriesF.loc[loc0:loc1])[0,0]
        df[name]=df_.copy(deep=True)
    df=pd.concat(df,axis=1)
    return df,width

ffd_book,width=fracDiff_FFD(shifted_series,d=0.8,thres=1E-5)
print(f'çª—å£å®½åº¦: {width}')
# è®¡ç®—ADFç»Ÿè®¡é‡å’Œpå€¼
adf_result = adfuller(ffd_book)
print('ADF Statistic: %f' % adf_result[0])
print('p-value: %f' % adf_result[1])
#ä»¥0.01ä¸ºé—´éš”ï¼Œé€æ­¥å‡å°‘då€¼ï¼Œç›´åˆ°på€¼ä½äº0.05
d_values = np.arange(0.995, 1, 0.00002)
for d in d_values:
    ffd_book,width=fracDiff_FFD(shifted_series,d=d,thres=1E-5)
    # è®¡ç®—ADFç»Ÿè®¡é‡å’Œpå€¼
    adf_result = adfuller(ffd_book)
    print(f'd={d:.5f}, ADF Statistic: {adf_result[0]:.4f}, p-value: {adf_result[1]:.4f}, çª—å£å®½åº¦: {width}')
    if adf_result[1] <= 0.05:
        print(f'æœ€å°då€¼: {d:.5f}')
        break

#1ä¸ªå‘¨æœŸæ—¶ï¼š
#ä»¥1E-4çš„é˜ˆå€¼éƒ½éš¾ä»¥åœ¨ã€0,1ã€‘åŒºé—´æ‰¾åˆ°å¹³ç¨³çš„ï¼Œæœ€ç»ˆåœ¨d=1æ—¶è¾¾åˆ°å¹³ç¨³ã€‚å‡å°‘ç²¾åº¦ä¸º1e-2ä¹Ÿæ‰¾ä¸åˆ°åˆé€‚çš„då€¼ã€‚åªæ˜¯æ¥è¿‘1çš„æ—¶å€™å°±æ˜¯å¹³ç¨³çš„ã€‚0.9999999995-0.99999999995ä¹‹é—´æ‰è¾¾åˆ°å¹³ç¨³ï¼Œéå¸¸éº»çƒ¦
#é˜ˆå€¼æ˜¯ä¸ºäº†æ§åˆ¶ç³»æ•°çš„æƒé‡é˜ˆå€¼ï¼Œè¶Šå°çš„é˜ˆå€¼èƒ½å¤Ÿæˆªå–æ›´é•¿çš„ç³»æ•°åˆ—ï¼Œç²¾åº¦æ›´é«˜ï¼Œå³çª—å£æ›´å¤§ã€‚çª—å£æŒ‡æ¯ä¸ªxä¸åé¢widthé•¿åº¦çš„å€¼äº§ç”Ÿé€’æ¨å…³ç³»
#ä¹¦æœ¬é‡Œé¢çš„çª—å£é•¿åº¦æ˜¯è‡ªé€‚åº”é˜ˆå€¼çš„ï¼Œè¿™æ ·è¦å¤„ç†çš„ç³»æ•°å°±å°‘äº†ä¸€ä¸ªã€‚å³åˆ†æ•°é˜¶dï¼Œç²¾åº¦é˜ˆå€¼thresï¼Œå’Œçª—å£é•¿åº¦widthåªéœ€è¦ç¡®å®šä¸¤ä¸ªå°±è¡Œäº†ã€‚
#æ‰€ä»¥æœ€åè¿˜æ˜¯ä½¿ç”¨Fracdiffè¿™ä¸ªåº“æ›´å¿«æ›´æ–¹ä¾¿ã€‚
#100ä¸ªå‘¨æœŸæ—¶ï¼š
#è¿˜æ˜¯éš¾ä»¥æ‰¾åˆ°åˆé€‚çš„då€¼

#b32
#Fracdiffè¿™ä¸ªåº“æ˜¯ä¸“é—¨è®¡ç®—åˆ†æ•°é˜¶å·®åˆ†çš„åº“ï¼Œæ˜¯ä¸“é—¨ä¸ºè¿™æœ¬ä¹¦å¼€å‘çš„ä¸€ä¸ªåº“ï¼Œç”šè‡³æ”¯æŒäº†5%ä¸‹æ˜¾è‘—çš„å¹³ç¨³æ€§æœ€å¤§è®°å¿†è·å–ã€‚å³æœ€å°då€¼è·å–  https://github.com/fracdiff/fracdiff
#ä½†æ˜¯è¿™ä¸ªåº“æœ€é«˜åªæ”¯æŒpython3.9 
#ä½¿ç”¨miniforgeè£…äº†ç¯å¢ƒï¼Œéœ€è¦æ—¶åˆ‡æ¢åˆ°è¯¥ç¯å¢ƒè£…åŒ…ä½¿ç”¨ã€‚
#conda activate py39 æ¿€æ´» ç„¶åconda install fracdiff
#Fracdiff è¿™ä¸ªåº“åªå®ç°äº†å›ºå®šçª—å£æ³•ã€‚ä»¥åä½¿ç”¨éƒ½ç”¨è¿™ä¸ªï¼Œä½†æ˜¯è”ç³»è¿˜å¾—å†™æ‹“å±•çª—å£æ³•ç»ƒä¹ ã€‚
#modeå¿…é¡»é€‰æ‹©'valid'æ‰è¡Œï¼Œsameæ¨¡å¼æ˜¯å¡«å……çš„ï¼Œä»…é€‚ç”¨äºå¯è§†åŒ–ç”»å›¾
#windowæ˜¯ç¡®å®šç²¾åº¦ï¼Œå³ä¸åé¢å¤šå°‘ä½äº§ç”Ÿé€’æ¨å…³ç³»ã€‚precisionæ˜¯è¾“å‡ºçš„åˆ†æ•°é˜¶ç²¾åº¦
#ä½¿ç”¨fracdiffå¯¹å•æ¬¡æ±‚å¾®åˆ†ï¼Œä½¿ç”¨FracdiffStatå¯»æ‰¾æ˜¾è‘—æ°´å¹³ä¸‹çš„æœ€å°åˆ†æ•°é˜¶ï¼Œåªéœ€è¦ç¡®å®šwindowè¿™ä¸ªå‚æ•°å³å¯
from fracdiff.sklearn import FracdiffStat,fracdiff
series_2d = shifted_series.to_numpy().reshape(-1, 1)
ffd = FracdiffStat(window=5000 , precision=1e-6,lower=0,upper=1.0, pvalue=0.05,mode = 'valid')  
y=ffd.fit_transform(series_2d)
# è®¡ç®—ADFç»Ÿè®¡é‡å’Œpå€¼
adf_result = adfuller(y)
print('ADF Statistic: %f' % adf_result[0])
print('p-value: %f' % adf_result[1])
print(f"æœ€å°då€¼: {ffd.d_[0]:.9f}")  
#æœ€å°då€¼0.999999958ï¼Œç›´æ¥å¾—åˆ°æœ€åçš„ç»“æœã€‚æ˜¾è‘—çš„æ¯”è¯¾æœ¬é‡Œçš„ä»£ç å¿«

#ä¸€å¼€å§‹ç”Ÿæˆçš„æ•°æ®åªæœ‰æ­£å¼¦å‡½æ•°ä¸€ä¸ªå‘¨æœŸï¼Œåç»­æ”¹ä¸º100ä¸ªå‘¨æœŸï¼Œæ•°æ®ç»“æ„ä¸ä¸€æ ·äº†ï¼Œç»“è®ºä¹Ÿä¸åŒ
#100ä¸ªå‘¨æœŸï¼š åªèƒ½æ‰¾åˆ°d=1äº†ã€‚å°†çª—å£æ‰©å¤§ä¹Ÿä¸€æ ·ã€‚

'''
5.3 Take the series from exercise 2.b:
(a) Fit the series to a sine function. What is the R-squared?
(b) Apply FFD(d=1). Fit the series to a sine function. What is the R-squared?
(c) What value of d maximizes the R-squared of a sinusoidal fit on FFD(d).
Why?

'''


#5.3a æ‹Ÿåˆä¸ºæ­£å¼¦å‡½æ•°å¹¶è®¡ç®—ræ–¹
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import curve_fit
from sklearn.metrics import r2_score

def fit_sine_y_only(y, sampling_rate=1):
    """
    åªæœ‰yæ•°æ®æ—¶çš„æ­£å¼¦å‡½æ•°æ‹Ÿåˆ
    
    å‚æ•°:
    y: yå€¼æ•°æ®æ•°ç»„
    sampling_rate: é‡‡æ ·ç‡ï¼Œé»˜è®¤ä¸º1ï¼ˆå‡è®¾ç­‰é—´è·é‡‡æ ·ï¼‰
    
    è¿”å›:
    popt: æœ€ä¼˜å‚æ•° [A, f, phi, C]
    r_squared: RÂ²å€¼
    y_pred: é¢„æµ‹å€¼
    x: ç”Ÿæˆçš„xåæ ‡
    """
    
    # å‡è®¾xæ˜¯ç­‰é—´è·çš„ç´¢å¼•
    x = np.arange(len(y)) / sampling_rate
    
    # æ­£å¼¦å‡½æ•°æ¨¡å‹
    def sin_func(x, A, f, phi, C):
        return A * np.sin(2 * np.pi * f * x + phi) + C
    
    # æä¾›åˆå§‹å‚æ•°ä¼°è®¡
    # æŒ¯å¹…ä¼°è®¡
    A0 = (np.max(y) - np.min(y)) / 2
    if A0 == 0:
        A0 = 1  # é¿å…é™¤é›¶
    
    # é¢‘ç‡ä¼°è®¡ï¼ˆä½¿ç”¨FFTæ‰¾åˆ°ä¸»è¦é¢‘ç‡ï¼‰
    fft = np.fft.fft(y - np.mean(y))  # å»å‡å€¼
    freqs = np.fft.fftfreq(len(y), 1/sampling_rate)
    
    # æ‰¾åˆ°æ­£é¢‘ç‡éƒ¨åˆ†çš„æœ€å¤§æŒ¯å¹…å¯¹åº”çš„é¢‘ç‡
    positive_freq_idx = np.where(freqs > 0)[0]
    if len(positive_freq_idx) > 0:
        idx = positive_freq_idx[np.argmax(np.abs(fft[positive_freq_idx]))]
        f0 = freqs[idx]
    else:
        f0 = 1/len(y)  # é»˜è®¤é¢‘ç‡
    
    # ç›¸ä½å’Œåç§»ä¼°è®¡
    phi0 = 0
    C0 = np.mean(y)
    
    initial_guess = [A0, f0, phi0, C0]
    
    try:
        # è®¾ç½®å‚æ•°è¾¹ç•Œä»¥é¿å…ä¸åˆç†çš„å€¼
        bounds = ([0, 0, -np.pi, -np.inf], 
                 [2*A0, sampling_rate/2, np.pi, np.inf])
        
        # ä½¿ç”¨curve_fitè¿›è¡Œéçº¿æ€§æœ€å°äºŒä¹˜æ‹Ÿåˆ
        popt, pcov = curve_fit(sin_func, x, y, p0=initial_guess, 
                              bounds=bounds, maxfev=5000)
        
        # è®¡ç®—é¢„æµ‹å€¼
        y_pred = sin_func(x, *popt)
        
        # è®¡ç®—RÂ²
        r_squared = r2_score(y, y_pred)
        
        return popt, r_squared, y_pred, x
        
    except Exception as e:
        print(f"æ‹Ÿåˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        # å°è¯•ä¸ä½¿ç”¨è¾¹ç•Œ
        try:
            popt, pcov = curve_fit(sin_func, x, y, p0=initial_guess, maxfev=5000)
            y_pred = sin_func(x, *popt)
            r_squared = r2_score(y, y_pred)
            return popt, r_squared, y_pred, x
        except:
            return None, None, None, x


# å¦‚æœæ‚¨æœ‰è‡ªå·±çš„yæ•°æ®ï¼Œè¯·ä½¿ç”¨è¿™ä¸ªå‡½æ•°
def fit_your_data(y_data, sampling_rate=1):
    """
    å¯¹æ‚¨çš„yæ•°æ®è¿›è¡Œæ­£å¼¦æ‹Ÿåˆ
    
    å‚æ•°:
    y_data: æ‚¨çš„yæ•°æ®æ•°ç»„
    sampling_rate: é‡‡æ ·ç‡ï¼ˆå¦‚æœçŸ¥é“çš„è¯ï¼‰ï¼Œé»˜è®¤ä¸º1
    """
    popt, r_squared, y_pred, x = fit_sine_y_only(y_data, sampling_rate)
    
    if popt is not None:
        A, f, phi, C = popt
        
        print("=" * 50)
        print("æ‚¨çš„æ•°æ®æ‹Ÿåˆç»“æœ")
        print("=" * 50)
        print(f"æŒ¯å¹… (A): {A:.4f}")
        print(f"é¢‘ç‡ (f): {f:.4f} Hz")
        print(f"ç›¸ä½ (Ï†): {phi:.4f} rad")
        print(f"åç§» (C): {C:.4f}")
        print(f"RÂ²: {r_squared:.4f}")
        print("=" * 50)
        print(f"æ‹Ÿåˆæ–¹ç¨‹: y = {A:.4f} * sin(2Ï€*{f:.4f}*t + {phi:.4f}) + {C:.4f}")
        
        # ç»˜åˆ¶ç»“æœ
        plt.figure(figsize=(10, 6))
        plt.plot(x, y_data, 'bo', alpha=0.7, label='æ‚¨çš„æ•°æ®', markersize=4)
        plt.plot(x, y_pred, 'r-', label='æ‹Ÿåˆæ›²çº¿', linewidth=2)
        plt.xlabel('æ ·æœ¬ç´¢å¼•' if sampling_rate == 1 else 'æ—¶é—´')
        plt.ylabel('y')
        plt.legend()
        plt.title('æ‚¨çš„æ•°æ®æ­£å¼¦æ‹Ÿåˆç»“æœ')
        plt.grid(True, alpha=0.3)
        plt.show()
        
        return popt, r_squared, y_pred, x
    else:
        print("æ‹Ÿåˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®")
        return None, None, None, None

y=y.flatten()
fit_your_data(y, sampling_rate=1) 


#5.3b åº”ç”¨FFD(d=1)å¹¶æ‹Ÿåˆä¸ºæ­£å¼¦å‡½æ•°
ffd_book,width=fracDiff_FFD(shifted_series,d=1,thres=1E-2)
ffd_book=ffd_book['close']
fit_your_data(ffd_book, sampling_rate=1) 

'''
d=1
æŒ¯å¹… (A): 1.0000
é¢‘ç‡ (f): 0.0000 Hz
ç›¸ä½ (Ï†): 0.0001 rad
åç§» (C): 3.0000
RÂ²: 1.0000

d=0.9999999995
æŒ¯å¹… (A): 1.0000
é¢‘ç‡ (f): 0.0000 Hz
ç›¸ä½ (Ï†): 0.0124 rad
åç§» (C): 3.0000
RÂ²: 1.0000

æ•´ä½“å·®åˆ«ä¸å¤§
'''

'''
5.4 Take the dollar bar series on E-mini S&P 500 futures. Using the code
in Snippet 5.3, for some d âˆˆ [0, 2], compute fracDiff_FFD(fracDiff
_FFD(series,d),-d). What do you get? Why?
'''
from fracdiff import fdiff
dollar = pd.read_csv(r'D:\Git\book\ASML\dollar_bars.csv'   ,
                     parse_dates=True,      # è§£ææ—¥æœŸåˆ—
                     index_col=[0]  # å°† 'date_time' åˆ—ä½œä¸ºç´¢å¼•
                     )
tb = dd_bars(data = dollar.close, m = 100000)
d = 0.2
window = 100

d0 = fdiff(tb.values, d, window=window, mode="same")
d1 = fdiff(d0, -d, window=window, mode="same")
spx=tb
spxd = pd.Series(d0, index=spx.index)
spxi = pd.Series(d1, index=spx.index)

plt.figure(figsize=(24, 6))

plt.subplot(1, 3, 1)
plt.title("åŸå§‹")
plt.plot(spx, linewidth=0.6)

plt.subplot(1, 3, 2)
plt.title("d^{} åŸå§‹".format(d))
plt.plot(spxd, linewidth=0.6)

plt.subplot(1, 3, 3)
plt.title("d^{} d^{} åŸå§‹".format(-d, d))
plt.plot(spxi, linewidth=0.6)

plt.show()

#ä¸èƒ½ç™¾åˆ†ç™¾è¿˜åŸï¼Œå› ä¸ºçª—å£æˆªæ–­æ˜¯æœ‰è¯¯å·®çš„

'''
5.5 Take the dollar bar series on E-mini S&P 500 futures.
(a) Form a new series as a cumulative sum of log-prices.
(b) Apply FFD, with ğœ = 1E âˆ’ 5. Determine for what minimum d âˆˆ [0, 2] the
new series is stationary.
(c) Compute the correlation of the fracdiff series to the original (untransformed)
series.
(d) Apply an Engel-Granger cointegration test on the original and fracdiff series.
Are they cointegrated? Why?
(e) Apply a Jarque-Bera normality test on the fracdiff series.

'''
#5.5a 
log_prices = np.log(tb)
log_prices_cumsum = log_prices.cumsum()

#5.5b
#æ ¹æ®åˆ†æ•°é˜¶å’Œé˜ˆå€¼ç¡®å®šçª—å£å¤§å°
from fracdiff.sklearn.tol import window_from_tol_coef
window = window_from_tol_coef(0.5, 1e-5)
print('åˆé€‚çª—å£å¤§å°:',window)

X_log_cumsum = np.array(log_prices).reshape(-1, 1)
f = FracdiffStat(window=window, mode="valid", upper=2)
diff = f.fit_transform(X_log_cumsum)

print("* Order: {:.2f}".format(f.d_[0]))
adf_result = adfuller(diff)
print('ADF Statistic: %f' % adf_result[0])
print('p-value: %f' % adf_result[1])

#5.5c
#è®¡ç®—fracdiffç³»åˆ—ä¸åŸå§‹ç³»åˆ—çš„ç›¸å…³æ€§
corr = np.corrcoef(diff.flatten(), X_log_cumsum[window-1:].flatten())[0, 1]
print("* Correlation: {:.4f}".format(corr))
#ç”»å›¾ difféœ€è¦å‘å³ç§»åŠ¨window-1ä¸ªå•ä½ï¼Œè€Œä¸”diffä½¿ç”¨å³è½´ä½¿å¾—å›¾å½¢å·®ä¸å¤šé‡å 
# åˆ›å»ºå›¾å½¢å’Œåæ ‡è½´
fig, ax1 = plt.subplots(figsize=(12, 6))
# ç¡®ä¿æ•°æ®æ ¼å¼é€‚åˆç”»å›¾
if hasattr(X_log_cumsum, 'flatten'):
    X_flat = X_log_cumsum.flatten()
else:
    X_flat = X_log_cumsum
if hasattr(diff, 'flatten'):
    diff_flat = diff.flatten()
else:
    diff_flat = diff
# å‘å³ç§»åŠ¨diffæ•°æ®window-1ä¸ªå•ä½
shifted_diff = np.full_like(X_flat, np.nan)
shifted_diff[window-1:] = diff_flat
# åœ¨å·¦è½´ä¸Šç»˜åˆ¶åŸå§‹åºåˆ—
ax1.plot(X_flat, label='åŸå§‹å¯¹æ•°ç´¯è®¡åºåˆ—', color='blue')
ax1.set_xlabel('æ—¶é—´')
ax1.set_ylabel('åŸå§‹åºåˆ—å€¼', color='blue')
ax1.tick_params(axis='y', labelcolor='blue')
# åˆ›å»ºå³è½´
ax2 = ax1.twinx()
# åœ¨å³è½´ä¸Šç»˜åˆ¶ç§»åŠ¨åçš„diff
ax2.plot(shifted_diff, label='åˆ†æ•°å·®åˆ†åºåˆ— (shifted)', color='red', alpha=0.7)
ax2.set_ylabel('åˆ†æ•°å·®åˆ†åºåˆ—å€¼', color='red')
ax2.tick_params(axis='y', labelcolor='red')
# è®¾ç½®æ ‡é¢˜å’Œå›¾ä¾‹
plt.title(f'åŸå§‹å¯¹æ•°ç´¯è®¡åºåˆ—ä¸åˆ†æ•°å·®åˆ†åºåˆ—å¯¹æ¯” (window={window}, correlation={corr:.4f})')
lines1, labels1 = ax1.get_legend_handles_labels()
lines2, labels2 = ax2.get_legend_handles_labels()
ax1.legend(lines1 + lines2, labels1 + labels2, loc='best')
# æ˜¾ç¤ºç½‘æ ¼çº¿
ax1.grid(True, linestyle='--', alpha=0.7)
plt.tight_layout()
# æ˜¾ç¤ºå›¾å½¢
plt.show()


#5.5d åº”ç”¨åæ•´æ£€éªŒã€‚å³ä¸¤ä¸ªåºåˆ—æ˜¯å¦å­˜åœ¨çº¿æ€§æ•°å­¦å…³ç³»ä½¿å¾—æ–°ç»„åˆæ˜¯å¹³ç¨³çš„
from statsmodels.tsa.stattools import coint
# è¿›è¡ŒEngel-Granger cointegration test
score, pvalue, _ = coint(X_log_cumsum[window-1:].flatten(), diff.flatten())
print("* Cointegration Test p-value: {:.4f}".format(pvalue))
# è§£é‡Šç»“æœ
if pvalue < 0.05:
    print("* æ‹’ç»åŸå‡è®¾ï¼šåºåˆ—æ˜¯ åæ•´å…³ç³»")
else:
    print("* ä¸èƒ½æ‹’ç»åŸå‡è®¾ï¼šåºåˆ—ä¸æ˜¯ åæ•´å…³ç³»")


#5.5e åº”ç”¨Jarque-Beraæ£€éªŒã€‚å³æ£€éªŒåºåˆ—æ˜¯å¦æœä»æ­£æ€åˆ†å¸ƒ
from statsmodels.stats.stattools import jarque_bera
# è¿›è¡ŒJarque-Bera test
jb_stat, pvalue, _, _ = jarque_bera(diff.flatten())
print("* Jarque-Bera Test p-value: {:.4f}".format(pvalue))
# è§£é‡Šç»“æœ
if pvalue < 0.05:
    print("* æ‹’ç»åŸå‡è®¾ï¼šåºåˆ—ä¸æ˜¯ æ­£æ€åˆ†å¸ƒ")
else:
    print("* ä¸èƒ½æ‹’ç»åŸå‡è®¾ï¼šåºåˆ—æ˜¯ æ­£æ€åˆ†å¸ƒ")

#éæ­£æ€åˆ†å¸ƒã€‚æ‰€ä»¥æ—¶é—´åºåˆ—å…³ç³»é‡Œé¢ï¼Œå¹³ç¨³æ€§ä¸è®°å¿†æ€§æ›´ä¸ºé‡è¦ï¼Œæ­£æ€æ€§æ˜¯æ›´é«˜ä¸€çº§çš„è¦æ±‚ï¼Œä¸ä¸€å®šèƒ½è¾¾åˆ°ã€‚



'''
5.6 Take the fracdiff series from exercise 5.
(a) Apply a CUSUM filter (Chapter 2), where h is twice the standard deviation
of the series.
(b) Use the filtered timestamps to sample a featuresâ€™ matrix. Use as one of the
features the fracdiff value.
(c) Form labels using the triple-barrier method, with symmetric horizontal barriers of twice the daily standard deviation, and a vertical barrier of 5 days.
(d) Fit a bagging classifier of decision trees where:
(i) The observed features are bootstrapped using the sequential method
from Chapter 4.
(ii) On each bootstrapped sample, sample weights are determined using the
techniques from Chapter 4.
'''

#5.5a åº”ç”¨CUSUMè¿‡æ»¤å™¨ã€‚å³ç­›é€‰å‡ºåºåˆ—ä¸­æ˜¾è‘—çš„å˜åŒ–ç‚¹

#è®¡ç®—æ ‡å‡†å·®
diff_std=diff.std()
diff_Series=pd.Series(diff.flatten()) #è½¬pd.Series
#å°†ç´¢å¼•æ”¹ä¸ºæ—¥æœŸæ ¼å¼ï¼Œä»2025å¹´1æœˆ1æ—¥å¾€å‰å€’æ¨
end_date = pd.Timestamp('2025-01-01')
date_index = pd.date_range(end=end_date, periods=len(diff_Series), freq='h')
diff_Series.index = date_index

#åŸåºåˆ—å·²ç»æ˜¯å¹³ç¨³ä¸”æœ‰è®°å¿†äº†ï¼Œç›´æ¥ç”¨åŸåºåˆ—è®¡ç®—CUSUM filterï¼Œä¸éœ€è¦å†åŠ ç™¾åˆ†æ¯”å˜åŒ–äº†
event_diff = cumsum_events(diff_Series, limit = 2*diff_std) 

#5.6b
sample_diff=diff_Series[event_diff]

#5.6c ä½¿ç”¨ä¸‰é‡éšœç¢æ³•å½¢æˆæ ‡ç­¾
# å®šä¹‰å‚æ•°
diff_Series=pd.DataFrame(diff_Series)
diff_Series.columns=['close']
ptSl = [2, 2]  # å¯¹ç§°æ°´å¹³ barriers
std = getDailyVol(diff_Series['close'], span0=100)
std = pd.DataFrame(std).rename(columns={'close': 'daily_vol'})
# å°†ç»“æœåˆå¹¶å›åŸDataFrame
diff_Series = diff_Series.join(std)
numDays=5
t1 = diff_Series['close'].index.searchsorted(diff_Series['close'].index + pd.Timedelta(days=numDays))
t1 = t1[t1 < diff_Series.shape[0]]  # ç¡®ä¿ä¸è¶…å‡ºèŒƒå›´
t1 = pd.Series(diff_Series['close'].index[t1], index=diff_Series['close'].index[:t1.shape[0]])
t1.name = 't1'
# å°†t1åˆ—æ·»åŠ åˆ°DataFrame
diff_Series = diff_Series.join(t1)
diff_Series_event=diff_Series.loc[event_diff]
diff_Series_label=generate_metalabels(diff_Series_event, diff_Series['close'], diff_Series_event['daily_vol'], ptSl)



##########æä¸æ‡‚è¿™é‡Œ############
#5.6d1 åº”ç”¨baggingåˆ†ç±»å™¨  
#è¦æ±‚çš„æ˜¯ä½¿ç”¨é¡ºåºå¼•å¯¼æ³•è¿›è¡ŒæŠ½æ ·



#5.6d2 å¯¹ä¸Šä¸€æ­¥åº”ç”¨çš„é¡ºåºå¼•å¯¼æ³•æ ·æœ¬ç¡®å®šæƒé‡  ï¼Œæœ‰å¯èƒ½æŠ½æ ·é‡å¤ï¼Œæ‰€ä»¥éœ€è¦é‡æ–°è®¡ç®—æƒé‡

'''
ç¬¬äº”ç« æ€»ç»“ï¼š
1.æœ¬ç« å…³æ³¨çš„æ˜¯æ•°æ®çš„å¹³ç¨³æ€§ä¸è®°å¿†æ€§ã€‚å…³äºè®°å¿†æ€§çš„é—®é¢˜ï¼Œåˆ‡åˆ†æ ·æœ¬çš„æ—¶å€™æ˜¯ä¸æ˜¯å°½é‡è¦æŒ‰é¡ºåºåˆ‡åˆ†ï¼Œå¦åˆ™å°±ç ´åçš„è®°å¿†æ€§ï¼ˆè¶‹åŠ¿æ€§ï¼‰
2.ç›´æ¥ä½¿ç”¨fracdiffè¿™ä¸ªåŒ…æ‰¾åˆ°æŒ‡å®šç½®ä¿¡åº¦ï¼ˆä¸€èˆ¬5%ï¼‰ä¸‹çš„æœ€å°å·®åˆ†é˜¶dï¼Œå°±æ˜¯å…·æœ‰æœ€é«˜è®°å¿†æ€§ä¸”å¹³ç¨³çš„å·®åˆ†æ•°æ®äº†ã€‚
3.from fracdiff.sklearn import FracdiffStat å°±å¯ä»¥ç›´æ¥æ‰¾åˆ°å¯¹åº”çš„æœ€å°då€¼äº†ï¼Œä½¿ç”¨å‰å…ˆç”¨from fracdiff.sklearn.tol import window_from_tol_coef ç¡®å®šæŒ‡å®šé˜ˆå€¼ä¸‹çš„çª—å£å¤§å°ã€‚è¿™ä¸ªåŒ…çš„åº”ç”¨ä¾‹å­éƒ½åœ¨ä¸Šé¢
'''


#%%
#æ¨¡å‹ ï¼š6-9ç« æ˜¯ä»‹ç»æ¨¡å‹çš„ä½¿ç”¨
#ç¬¬å…­ç« 

#æ¨¡å‹è®¾ç½®å…³é”®å‚æ•°ï¼š
'''
éšæœºæ£®æ—æ¨¡å‹ï¼š
1.max_features è®¾ç½®å°ä¸€ç‚¹ï¼Œå¯ä»¥å¢åŠ æ ‘çš„å·®å¼‚æ€§
2.å°†æ­£åˆ™åŒ–å‚æ•° min_weight_fraction_leaf è®¾ç½®ä¸ºè¶³å¤Ÿå¤§çš„å€¼ï¼ˆä¾‹å¦‚ 5%ï¼‰ï¼Œä»¥ä½¿è¢‹å¤–å‡†ç¡®ç‡æ”¶æ•›åˆ°æ ·æœ¬å¤–ï¼ˆk æŠ˜ï¼‰å‡†ç¡®ç‡
3.ä¿®æ”¹ RF ç±»ï¼Œå°†æ ·æœ¬å–æ ·ä»æ ‡å‡†è‡ªåŠ©æ³•æ”¹ä¸ºé¡ºåºè‡ªåŠ©æ³•  ï¼ˆè§ç¬¬å››ç« ä»£ç ï¼‰
4.å¯ä»¥å…ˆå¯¹ç‰¹å¾è¿›è¡Œä¸»æˆåˆ†åˆ†æï¼ˆpcaï¼‰ï¼Œé™ä½è¿‡æ‹Ÿåˆã€‚
5.class_weight='balanced_subsample' é™ä½æ ·æœ¬ä¸å¹³è¡¡æ€§å¸¦æ¥çš„å½±å“ã€‚
6.criterion='entropy' æå‡æ¨¡å‹æ€§èƒ½
clf0=RandomForestClassifier(n_estimators=1000,class_weight='balanced_subsample',
criterion='entropy') 
'''

'''
6.1 Why is bagging based on random sampling with replacement? Would bagging
still reduce a forecastâ€™s variance if sampling were without replacement?
'''


#ä½¿ç”¨æœ‰æ”¾å›çš„éšæœºæŠ½æ ·æ˜¯ä¸ºäº†å¢åŠ æ¯æ£µæ ‘ä¹‹é—´çš„å·®å¼‚æ€§ï¼Œä»è€Œæé«˜æ•´ä½“æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å¦‚æœä½¿ç”¨æ— æ”¾å›æŠ½æ ·ï¼Œæ ·æœ¬ä¹‹é—´çš„å·®å¼‚æ€§ä¼šå‡å°‘ï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹è¿‡æ‹Ÿåˆï¼Œä»è€Œæ— æ³•æœ‰æ•ˆé™ä½é¢„æµ‹çš„æ–¹å·®ã€‚

'''
6.2 Suppose that your training set is based on highly overlap labels (i.e., with low
uniqueness, as defined in Chapter 4).
(a) Does this make bagging prone to overfitting, or just ineffective? Why?
(b) Is out-of-bag accuracy generally reliable in financial applications? Why?

'''

#a é«˜åº¦é‡å çš„æ ‡ç­¾ä¼šä½¿å¾—è¢‹è£…æ–¹æ³•å˜å¾—æ— æ•ˆï¼Œå› ä¸ºæ¨¡å‹éš¾ä»¥å­¦ä¹ åˆ°æœ‰æ„ä¹‰çš„æ¨¡å¼ï¼Œä»è€Œæ— æ³•æœ‰æ•ˆé™ä½é¢„æµ‹çš„æ–¹å·®ã€‚
#Bagging çš„æ ¸å¿ƒæœºåˆ¶æ˜¯â€œé™ä½æ–¹å·®â€ï¼Œä½†å‰ææ˜¯æ¨¡å‹æœ¬èº«èƒ½å­¦åˆ°ä¸€äº›æ¨¡å¼.å†å²æ•°æ®æœ¬èº«æ²¡æœ‰è§„å¾‹,æ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šéƒ½è¡¨ç°å¹³å¹³ï¼Œæ— æ³•è¶…è¶ŠéšæœºçŒœæµ‹.è€Œä¸æ˜¯æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šè¡¨ç°å¾ˆå¥½ï¼Œä½†åœ¨æµ‹è¯•é›†ä¸Šå¾ˆå·®çš„è¿‡æ‹Ÿåˆ
#b é‡‘èæ•°æ®å…·æœ‰å¼ºæ—¶é—´ä¾èµ–æ€§å’Œéå¹³ç¨³æ€§ï¼ŒOOB æ ·æœ¬è™½ç„¶æ˜¯â€œæœªå‚ä¸è®­ç»ƒâ€çš„ï¼Œä½†ç”±äºæ˜¯ä»åŒä¸€æ—¶é—´æ®µéšæœºæŠ½å–çš„ï¼Œå®ƒä»¬ä¸è®­ç»ƒæ ·æœ¬åœ¨æ—¶é—´ä¸Šæ˜¯æ··åˆçš„ã€‚è¿™å¯¼è‡´ OOB æ ·æœ¬å¹¶éçœŸæ­£ç‹¬ç«‹åŒåˆ†å¸ƒï¼Œä¹Ÿç ´åäº†æ—¶é—´å¾ªåºï¼Œå¸¦æ¥æœªæ¥å‡½æ•°çš„é—®é¢˜ã€‚
#ä½¿ç”¨KæŠ˜æ¥è¯„ä¼°æ•ˆæœæ¯”è¾ƒé€‚å®œ

'''
6.3 Build an ensemble of estimators, where the base estimator is a decision tree.
(a) How is this ensemble different from an RF?
(b) Using sklearn, produce a bagging classifier that behaves like an RF. What
parameters did you have to set up, and how?

'''
#éšæœºæ£®æ— = Bagging + å†³ç­–æ ‘ + éšæœºç‰¹å¾å­é›†ï¼ˆfeature subsamplingï¼‰
#a éšæœºæ£®æ—åœ¨æ¯æ£µå†³ç­–æ ‘çš„è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥äº†éšæœºç‰¹å¾å­é›†é€‰æ‹©ï¼ˆfeature subsamplingï¼‰ï¼Œè€Œæ™®é€šçš„å†³ç­–æ ‘é›†æˆï¼ˆå¦‚Baggingï¼‰é€šå¸¸ä½¿ç”¨æ‰€æœ‰ç‰¹å¾è¿›è¡Œè®­ç»ƒã€‚è¿™ç§éšæœºç‰¹å¾é€‰æ‹©å¢åŠ äº†æ ‘ä¹‹é—´çš„å·®å¼‚æ€§ï¼Œä»è€Œæé«˜äº†æ•´ä½“æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

#b
# å®šä¹‰åŸºç¡€å†³ç­–æ ‘
base_tree = DecisionTreeClassifier(
    criterion='gini',           # åˆ†è£‚æ ‡å‡†ï¼ŒRF é»˜è®¤ä¸º 'gini'
    splitter='best',            # æœ€ä½³åˆ†è£‚æ–¹å¼
    random_state=None           # è®©æ¯æ£µæ ‘éšæœºåŒ–
)

# æ„å»ºä¸€ä¸ªâ€œç±»éšæœºæ£®æ—â€çš„ Bagging åˆ†ç±»å™¨
bagging_rf_clone = BaggingClassifier(
    base_estimator=base_tree,
    n_estimators=100,                     # æ ‘çš„æ•°é‡
    max_samples=1.0,                      # ä½¿ç”¨ 100% çš„æ ·æœ¬ï¼ˆæœ‰æ”¾å›ï¼‰
    max_features=0.5,                     # â­ å…³é”®ï¼šæ¯æ¬¡åˆ†è£‚æ—¶éšæœºé€‰æ‹© 50% çš„ç‰¹å¾
    bootstrap=True,                       # å¯¹æ ·æœ¬è¿›è¡Œæœ‰æ”¾å›æŠ½æ ·
    bootstrap_features=False,             # ä¸å¯¹ç‰¹å¾æŠ½æ ·ï¼ˆç”± max_features æ§åˆ¶ï¼‰
    oob_score=True,                       # å¯ç”¨è¢‹å¤–è¯¯å·®è¯„ä¼°
    random_state=42
)


'''
6.4 Consider the relation between an RF, the number of trees it is composed of, and the number of features utilized:
(a) Could you envision a relation between the minimum number of trees needed in an RF and the number of features utilized?
(b) Could the number of trees be too small for the number of features used?
(c) Could the number of trees be too high for the number of observations available?
'''
#ç‰¹å¾æ•°é‡å¢åŠ ï¼Œæ‰€éœ€è¦æ ‘æ•°é‡ä¹Ÿä¼šå¢åŠ ã€‚æ ‘æ•°é‡è¿‡å°‘æ—¶ä¼šå‡ºç°é‡è¦ç‰¹å¾æœªå……åˆ†å­¦ä¹ ï¼Œæ¨¡å‹æ€§èƒ½æœªè¾¾ä¸Šé™ï¼›æœªèƒ½å¹³å‡æ‰å™ªéŸ³ç­‰é—®é¢˜
#æ ‘ä¸ä¼šè¿‡å¤šï¼Œç†è®ºä¸Šä¸ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆã€‚ä½†æ˜¯æ ‘å¤ªå¤šå®¹æ˜“å¯¼è‡´è®­ç»ƒæ—¶é—´è¿‡é•¿ï¼Œé¢„æµ‹å»¶è¿Ÿç­‰æ€§èƒ½ä¸ç¡¬ä»¶ä¸Šçš„é—®é¢˜ã€‚


'''
6.5 How is out-of-bag accuracy different from stratified k-fold (with shuffling) cross validation accuracy?
'''

#OOB å‡†ç¡®ç‡æ˜¯ä¸€ç§é«˜æ•ˆã€ä¸“ç”¨çš„è¯„ä¼°æ–¹æ³•ï¼Œé€‚ç”¨äº bagging é›†æˆæ¨¡å‹ï¼›è€Œåˆ†å±‚ k æŠ˜äº¤å‰éªŒè¯æ˜¯ä¸€ç§é€šç”¨ã€ä¸¥è°¨çš„æ ‡å‡†æ–¹æ³•ï¼Œé€‚ç”¨äºæ‰€æœ‰æ¨¡å‹
#OOBä¸éœ€è¦é¢å¤–çš„è®­ç»ƒï¼Œä½†æ˜¯ç±»åˆ«å¹³è¡¡ä¿éšœæ˜¯æ²¡æœ‰çš„ï¼Œæ˜¯æœ‰åçš„ï¼Œè€Œä¸”åªèƒ½åœ¨éšæœºæ£®æ—é‡Œä½¿ç”¨ã€‚é€‚åˆåšå¿«é€Ÿç­›é€‰
#k æŠ˜éœ€è¦è€—è´¹æ—¶é—´è®­ç»ƒï¼Œä½†æ˜¯æ•ˆæœè¾ƒå¥½ï¼Œæ‰€æœ‰æ¨¡å‹éƒ½å¯ä»¥ä½¿ç”¨ã€‚é€‚åˆåšæœ€ç»ˆç¡®è®¤ã€‚


'''
ç¬¬å…­ç« æ€»ç»“ä¸è¦ç‚¹ï¼š
1.baggingé€‚ç”¨äºå¤„ç†è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå› ä¸ºç»“æœæ˜¯å¤šä¸ªæ ‘æŠ•ç¥¨å†³å®šï¼Œä¼šé™ä½æ‹Ÿåˆåº¦ã€‚boostç”¨äºè§£å†³æ¬ æ‹Ÿåˆé—®é¢˜ï¼Œå› ä¸ºè®­ç»ƒäº‹ä¼šæŠŠæ‹Ÿåˆåº¦ä½çš„ä¼°è®¡å™¨ä¸¢å¼ƒï¼Œä¼šå¢åŠ æ‹Ÿåˆåº¦ã€‚
é‡‘èæ•°æ®é€‚ç”¨äºbaggingï¼Œè¿‡æ‹Ÿåˆçš„åæœå¾€å¾€æ˜¯ç¾éš¾æ€§çš„ã€‚è€Œä¸”ä¿¡å™ªæ¯”ä½ï¼Œå¾ˆå®¹æ˜“è¿‡æ‹Ÿåˆ
'''

#%%
#ç¬¬ä¸ƒç«  äº¤å‰éªŒè¯

'''
 7.1 Why is shuffling a dataset before conducting k-fold CV generally a bad idea in
 finance? Whatis the purpose of shuffling? Why does shuffling defeat the purpose of k-fold CV in financial datasets?

'''
#æ‰“ä¹±æ•°æ®ä¼šç ´åè®°å¿†æ€§ï¼Œä½¿å¾—æ¨¡å‹æ•ˆæœå˜å·®ã€‚è€Œä¸”é‡‘é¢æ•°æ®ä¹Ÿä¸æ»¡è¶³ç‹¬ç«‹åŒåˆ†å¸ƒçš„ç‰¹æ€§ï¼Œk-FOLDçš„å‡è®¾ä¸èƒ½æ»¡è¶³ã€‚
#æ‰“ä¹±æ˜¯è®©æ•°æ®é›†åˆ†å¸ƒæ›´å‡åŒ€ï¼Œè€Œæé«˜æ³›åŒ–èƒ½åŠ›

'''
 7.2 Take a pair of matrices (X,y), representing observed features and labels. These
 could be one of the datasets derived from the exercises in Chapter 3.
(a) Derive the performance from a 10-foldCV of a RFclassifier on (X,y),without shuffling.
 (b) Derive the performance from a 10-fold CVofanRFon(X,y),with shuffling.
 (c) Why are both results so different?
 (d) How does shuffling leak information?
'''
#a ä¸æ‰“ä¹±æ•°æ®é›†è¿›è¡Œ10æŠ˜äº¤å‰éªŒè¯
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import KFold, cross_val_score
from sklearn.datasets import make_classification

import yfinance as yf
import talib 

# ----------------------------
# 1. å‡†å¤‡æ•°æ® (X, y)
# ----------------------------

ticker = "AAPL"
data = yf.download(ticker, start="2020-01-01", end="2023-12-31")
ohlcv = data[['Open', 'High', 'Low', 'Close', 'Volume']].copy()
ohlcv.columns = ['open', 'high', 'low', 'close', 'volume'] # talib ä¹ æƒ¯å°å†™

df = ohlcv.copy()

# --- åŸºç¡€ä»·æ ¼æ¯”ç‡ ---
df['close_to_open'] = df['close'] / df['open']
df['high_to_low'] = df['high'] / df['low']

# --- ç§»åŠ¨å¹³å‡çº¿ ---
window_short = 20
window_long = 50
df[f'ma_{window_short}'] = talib.SMA(df['close'], timeperiod=window_short)
df[f'ma_{window_long}'] = talib.SMA(df['close'], timeperiod=window_long)
df['ma_ratio'] = df[f'ma_{window_short}'] / df[f'ma_{window_long}']

# --- æŒ‡æ•°ç§»åŠ¨å¹³å‡çº¿ ---
df['ema_12'] = talib.EMA(df['close'], timeperiod=12)
df['ema_26'] = talib.EMA(df['close'], timeperiod=26)
df['ema_diff'] = df['ema_12'] - df['ema_26']

# --- MACD (Moving Average Convergence Divergence) ---
macd, macd_signal, macd_hist = talib.MACD(df['close'])
df['macd'] = macd
df['macd_signal'] = macd_signal
# df['macd_hist'] = macd_hist # å¯é€‰

# --- å¸ƒæ—å¸¦ (Bollinger Bands) ---
upperband, middleband, lowerband = talib.BBANDS(df['close'], timeperiod=20)
df['bb_upper'] = upperband
df['bb_lower'] = lowerband
df['bb_width'] = (upperband - lowerband) / middleband # å¸ƒæ—å¸¦å®½åº¦

# --- ç›¸å¯¹å¼ºå¼±æŒ‡æ•° RSI ---
df['rsi'] = talib.RSI(df['close'], timeperiod=14)

# --- éšæœºæŒ‡æ ‡ Stochastic Oscillator ---
slowk, slowd = talib.STOCH(df['high'], df['low'], df['close'])
df['stoch_k'] = slowk
df['stoch_d'] = slowd

# --- æ³¢åŠ¨ç‡ (ä½¿ç”¨ ATR - Average True Range) ---
df['atr'] = talib.ATR(df['high'], df['low'], df['close'], timeperiod=14)

# --- æˆäº¤é‡æŒ‡æ ‡ ---
df['volume_sma'] = talib.SMA(df['volume'].astype(float), timeperiod=20)
df['volume_ratio'] = df['volume'] / df['volume_sma']

# --- åˆ é™¤å›  talib è®¡ç®—äº§ç”Ÿçš„ NaN ---
df.dropna(inplace=True)

# --- å®šä¹‰ç‰¹å¾åˆ— ---
# é€‰æ‹©æœ€ç»ˆç”¨äºè®­ç»ƒçš„ç‰¹å¾ (å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´)
feature_cols = [
    'close_to_open', 'high_to_low',
    'ma_ratio', 'ema_diff',
    'macd', 'macd_signal',
    'bb_width',
    'rsi',
    'stoch_k', 'stoch_d',
    'atr',
    'volume_ratio'
]
X = df[feature_cols].copy()

horizon = 5
df['future_close'] = df['close'].shift(-horizon) # æœªæ¥ horizon å¤©çš„æ”¶ç›˜ä»·
df['future_return'] = (df['future_close'] - df['close']) / df['close'] # æœªæ¥æ”¶ç›Š
df['label'] = (df['future_return'] > 0).astype(int) # 1: ä¸Šæ¶¨, 0: ä¸‹è·Œæˆ–æŒå¹³

#ç”Ÿæˆäº‹ä»¶ä»¥åŠå¯¹åº”çš„metalabel
#è®¡ç®—æ ‡å‡†å·®  è¿™é‡Œä¹Ÿå¯ä»¥ç”¨æ»‘åŠ¨çš„æ ‡å‡†å·®æ¥è®¡ç®—
diff_std=df['future_return'].std()

#æ”¶ç›˜ä»·å˜åŠ¨è¶…è¿‡1ä¸ªæ ‡å‡†å·®çš„æŠ“å‡ºæ¥å½“äº‹ä»¶
event_diff = cumsum_events1(df['close'], limit = diff_std) 

#ä½¿ç”¨ä¸‰é‡éšœç¢æ³•å½¢æˆæ ‡ç­¾
# å®šä¹‰å‚æ•°
ptSl = [1, 1]  # å¯¹ç§°æ°´å¹³ barriers
std = getDailyVol(df['close'], span0=100)
std = pd.DataFrame(std).rename(columns={'close': 'daily_vol'})
# å°†ç»“æœåˆå¹¶å›åŸDataFrame
df = df.join(std)
numDays=15
t1 = df['close'].index.searchsorted(df['close'].index + pd.Timedelta(days=numDays))
t1 = t1[t1 < df.shape[0]]  # ç¡®ä¿ä¸è¶…å‡ºèŒƒå›´
t1 = pd.Series(df['close'].index[t1], index=df['close'].index[:t1.shape[0]])
t1.name = 't1'
# å°†t1åˆ—æ·»åŠ åˆ°DataFrame
df = df.join(t1)
df_event=df.loc[event_diff]
df_label=generate_metalabels(df_event, df['close'], df_event['daily_vol'], ptSl)

# --- å¯¹é½ X å’Œ yï¼Œåˆ é™¤åŒ…å« NaN çš„è¡Œ ---
# æ³¨æ„ï¼šç”±äºæœªæ¥æ”¶ç›Šçš„ shift(-horizon)ï¼Œæœ€å horizon è¡Œçš„ future_* ä¼šæ˜¯ NaN
# ä»¥åŠ talib è®¡ç®—å¼•å…¥çš„ NaN
df_final = df_label.dropna(subset=['label']) # dropna on label also handles feature NaNs from alignment

# ç¡®ä¿ X å’Œ y ç´¢å¼•å¯¹é½
X_final = X.loc[df_final.index]
y_final = df_final['metallabel']

# # è½¬æ¢ä¸º DataFrameï¼ˆå¯é€‰ï¼Œä¾¿äºæŸ¥çœ‹ï¼‰
# X = pd.DataFrame(X_final)
# y = pd.Series(y_final)

# 2. è®¾ç½® 10 æŠ˜äº¤å‰éªŒè¯ï¼ˆä¸æ´—ç‰Œï¼‰

cv = KFold(n_splits=10, shuffle=False, random_state=None)  # shuffle=False æ˜¯å…³é”®ï¼

model = RandomForestClassifier(n_estimators=100, random_state=42)

# 4. æ‰§è¡Œäº¤å‰éªŒè¯å¹¶è·å–æ€§èƒ½
scores = cross_val_score(
    estimator=model,
    X=X_final,
    y=y_final,
    cv=cv,
    scoring='accuracy',  # å¯æ”¹ä¸º 'roc_auc', 'f1' ç­‰
    n_jobs=-1            # å¹¶è¡ŒåŠ é€Ÿ
)

# ----------------------------
# 5. è¾“å‡ºç»“æœ
# ----------------------------
print("10-Fold CV Accuracy Scores (no shuffling):")
print(scores)
print(f"\nMean Accuracy: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})")

#b æ‰“ä¹±æ•°æ®é›†è¿›è¡Œ10æŠ˜äº¤å‰éªŒè¯
cv = KFold(n_splits=10, shuffle=True, random_state=32)  # shuffle=True æ˜¯å…³é”®ï¼
model = RandomForestClassifier(n_estimators=100, random_state=42)

# æ‰§è¡Œäº¤å‰éªŒè¯å¹¶è·å–æ€§èƒ½
scores = cross_val_score(
    estimator=model,
    X=X_final,
    y=y_final,
    cv=cv,
    scoring='accuracy',  # å¯æ”¹ä¸º 'roc_auc', 'f1' ç­‰
    n_jobs=-1            # å¹¶è¡ŒåŠ é€Ÿ
)

print("10-Fold CV Accuracy Scores (with shuffling):")
print(scores)
print(f"\nMean Accuracy: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})")

#c1 æ‰“ä¹±åæ•°æ®çš„Mean Accuracyä¸‹é™äº†ï¼Œä¸ºä»€ä¹ˆï¼Ÿä»0.92é™ä½åˆ°0.91ï¼Œä¸æ˜¯å¾ˆæ˜æ˜¾çš„å˜åŒ–ã€‚è¿™ä¸ªå˜åŒ–æ„Ÿè§‰ç”¨è®­ç»ƒè¯¯å·®å°±èƒ½è¯´çš„è¿‡å»â€”â€”â€”â€”è¿™æ˜¯ä¸€å¼€å§‹ä½¿ç”¨sklearnè‡ªå¸¦çš„æ•°æ®é›†

#c2 ä½¿ç”¨è‚¡ç¥¨æ•°æ®åå‘ç°æ‰“ä¹±åæ•°æ®Mean Accuracyä¸Šå‡äº†40%ï¼Œä»0.5åˆ°0.7ï¼Œéå¸¸å¤¸å¼ ã€‚è¿™æ˜¯ä¸ºä»€ä¹ˆï¼Ÿå‡ºç°äº†ä¿¡æ¯æ³„éœ²ã€‚

#c3 ä½¿ç”¨è‚¡ç¥¨æ•°æ®+metalabelï¼Œæ‰“ä¹±åMean Accuracyä»0.5198ä¸‹é™åˆ°äº† 0.4942ï¼Œä¸ºä»€ä¹ˆ?ä¸æ˜¯åº”è¯¥å‡ºç°æœªæ¥å‡½æ•°å¯¼è‡´å‡†ç¡®ç‡ä¸Šå‡ä¹ˆï¼Ÿè¿˜æ˜¯è¯´æ•°æ®çš„è®°å¿†æ€§è¢«ç ´åå¯¼è‡´å‡†ç¡®ç‡ä¸‹é™äº†ï¼Ÿä¸å¯¹ï¼Œæ‰“ä¹±åæ˜¯ä¿¡æ¯æ³„éœ²ï¼Œå¯¼è‡´å‡†ç¡®ç‡ä¸Šå‡æ‰å¯¹ã€‚æ•°æ®å¤„ç†å¯èƒ½å“ªé‡Œå‡ºé—®é¢˜äº†ã€‚-------ä¿®æ”¹äº†random_state=36åï¼Œæ‰“ä¹±åMean Accuracyä»0.4997 ä¸Šå‡åˆ°äº†0.5198 ï¼Œç¬¦åˆé€¾æœŸâ€”â€”â€”â€”ä¿®æ”¹ä¸º30ååˆä¸é¢„æœŸç›¸åäº†,æ”¹ä¸º26ååˆç¬¦åˆäº†ï¼Œçœ‹æ¥æ˜¯æ ·æœ¬æ•°æ®åˆ†å¸ƒå½±å“å‡†ç¡®æ€§ï¼Ÿ

#d æ‰“ä¹±ä¼šæ³„éœ²ä¿¡æ¯ï¼Œå› ä¸ºæ—¶é—´çš„é¡ºåºä¸ä¸€æ ·äº†ï¼Œå¯¼è‡´æœªæ¥å‡½æ•°çš„ä¿¡æ¯æ³„éœ²ã€‚

'''
 7.3 Take the same pair of matrices (X,y) you used in exercise 2.
 (a) Derive the performance from a 10-fold purged CV of an RF on (X,y), with
 1%embargo.
 (b) Why is the performance lower?
 (c) Why is this result more realistic
'''

#a with 1%embargo  æ¸…é™¤å’Œç¦æ­¢éƒ½éœ€è¦æ‰‹åŠ¨å†™ä»£ç å®ç°ï¼Œæ²¡æœ‰ç°æˆçš„pythonåº“
#éœ€è¦åœ¨è¿™é‡Œå†™æˆæ¸…é™¤å’Œç¦æ­¢çš„å‡½æ•°ï¼Œä»¥åŠåº”ç”¨äº†è¿™æ ·æ¸…æ´—æ•°æ®çš„k-fold CV
#æ¸…é™¤+ç¦æ­¢æ•°æ®å¤„ç† æµ‹è¯•é›†å¯ä»¥åœ¨ä¸­é—´ä½†æ˜¯å‰åéƒ½å»æ‰é‡å ï¼ˆæ¸…é™¤ï¼‰ï¼Œåé¢æ—¶é—´çš„æ•°æ®è¿˜è¦å»æ‰ç¦æ­¢
#ä¸‹é¢çš„PurgedKFoldå’ŒcvScoreæ˜¯å‡ºè‡ªä¹¦æœ¬åŸæ–‡ï¼Œç›´æ¥ç”¨å°±è¡Œã€‚
from sklearn.model_selection._split import _BaseKFold
class PurgedKFold(_BaseKFold): 
    '''
        Extend KFold to work with labels that span intervals 
        The train is purged of observations overlapping test-label intervals 
        Test set is assumed contiguous (shuffle=False), w/o training examples in between
    ''' 
    def __init__(self,n_splits=3,t1=None,pctEmbargo=0.): 
            if not isinstance(t1,pd.Series): 
                raise ValueError('Label Through Dates must be a pandas series') 
            super(PurgedKFold,self).__init__(n_splits,shuffle=False,random_state=None) 
            self.t1=t1 
            self.pctEmbargo=pctEmbargo 

    def split(self,X,y=None,groups=None): 
        if (X.index==self.t1.index).sum()!=len(self.t1): 
            raise ValueError('X and ThruDateValues must have the same index') 
        indices=np.arange(X.shape[0]) 
        mbrg=int(X.shape[0]*self.pctEmbargo) 
        test_starts=[(i[0],i[-1]+1) for i in np.array_split(np.arange(X.shape[0]),self.n_splits)] 
        for i,j in test_starts: 
            t0=self.t1.index[i] # start of test set
            test_indices=indices[i:j] 
            maxT1Idx=self.t1.index.searchsorted(self.t1.iloc[test_indices].max()) 
            train_indices=self.t1.index.searchsorted(self.t1[self.t1<=t0].index) 
            train_indices=np.concatenate((train_indices,indices[maxT1Idx+mbrg:]))  #å°†æµ‹è¯•é›†åé¢çš„ç¦è¿æœŸï¼ˆmbrgï¼‰åé¢çš„ä¸€éƒ¨åˆ†æ•°æ®ä¹ŸåŒ…å«åˆ°è®­ç»ƒé›†ä¸­
            yield train_indices,test_indices

def cvScore(clf,X,y,sample_weight,scoring='neg_log_loss',t1=None,cv=None,cvGen=None,pctEmbargo=0.01):
    '''
    sample_weightæ˜¯ä¸xyå¯¹åº”çš„äº‹ä»¶çº§åˆ«çš„æƒé‡ï¼Œè€Œä¸æ˜¯barçº§åˆ«çš„æƒé‡ï¼Œåœ¨ä½¿ç”¨ç¬¬å››ç« çš„é‡å äº‹ä»¶ä¿®æ­£æƒé‡åçš„è¾“å…¥ï¼Œéœ€è¦åœ¨cvScoreä¸­ä¼ é€’


    '''
    
    if scoring not in ['neg_log_loss','accuracy']:
        raise Exception('wrong scoring method.')
    
    from sklearn.metrics import log_loss,accuracy_score
    # from clfSequential import PurgedKFold #å°±æ˜¯ä¸Šè¿°çš„å‡½æ•°

    if cvGen is None:
        cvGen=PurgedKFold(n_splits=cv,t1=t1,pctEmbargo=pctEmbargo) # purged
    score=[]
    for train,test in cvGen.split(X=X):
        fit=clf.fit(X=X.iloc[train,:],y=y.iloc[train],sample_weight=sample_weight.iloc[train].values)
        if scoring=='neg_log_loss':
            prob=fit.predict_proba(X.iloc[test,:])
            score_=-log_loss(y.iloc[test],prob,sample_weight=sample_weight.iloc[test].values,labels=clf.classes_)
        else:
            pred=fit.predict(X.iloc[test,:])
            score_=accuracy_score(y.iloc[test],pred,sample_weight=sample_weight.iloc[test].values)
        score.append(score_)
    return np.array(score)

# ç”Ÿæˆæ ·æœ¬æƒé‡ æ ¹æ®df_finalè¿™ä¸ªäº‹ä»¶ç”Ÿæˆï¼Œè§‚å¯Ÿæ¥çœ‹æ˜¯æœ‰é‡å çš„
sample_weight = pd.Series(np.ones(len(X_final)), index=X_final.index)
event_uniqueness = mpSampleTW(event=df_final, close=df['close'])
# bar_uniqueness = calculate_bar_uniqueness(events=df_final, uniqueness=event_uniqueness, bar_timestamps=df['close'].index)

model = RandomForestClassifier(n_estimators=100, random_state=42)

#æ‰§è¡Œäº¤å‰éªŒè¯å¹¶è·å–æ€§èƒ½
pcv_score=cvScore(model,X_final,y_final,sample_weight=event_uniqueness,scoring='accuracy',t1=df_final['t1'],cv=10,pctEmbargo=0.01)
print("10-Fold CV Accuracy Scores (with purging and embargo):")
print(pcv_score)
print(f"\nMean Accuracy: {pcv_score.mean():.4f} (+/- {pcv_score.std() * 2:.4f})")

#b #c
#7.4aMean Accuracy åœ¨random_state=42 æ˜¯0.4804ï¼Œæ¯”è¾ƒä½  random_state=26 æ˜¯ 0.4807
#random_state=42  7.3açš„å‡†ç¡®ç‡ä¸º0.5198   ,random_state=26 æ˜¯ 0.4997
#random_state=42  7.3bæ‰“ä¹±åçš„7.3bæ˜¯0.5204 ,random_state=26 æ˜¯ 0.5117 
#æ˜æ˜¾çœ‹åˆ°åœ¨æ¸…é™¤+ç¦æ­¢åæ•°æ®çš„ç¨³å®šæ€§ä¸Šå‡äº†ï¼Œæ”¶æ•°æ®åˆ‡åˆ†çš„æ³¢åŠ¨å‡å°‘ï¼Œæ›´åŠ åæ˜ çœŸå®æƒ…å†µ

#å»¶ä¼¸ï¼šæ˜¯å¦éœ€è¦shuffleï¼Ÿâ€”â€”ä¸è¡Œï¼Œæ¸…é™¤å’Œç¦æ­¢éƒ½æ˜¯é»˜è®¤æŒ‰ç…§æ—¶é—´å¾ªåºçš„ã€‚

'''
 7.4 In this chapter we have focused on one reason why k-fold CV fails in financial
 applications, namely the fact that some information from the testing set leaks into
 the training set. Can you think of a second reason for CVâ€™s failure?
'''



#æ—¶é—´åºåˆ—çš„éç‹¬ç«‹åŒåˆ†å¸ƒæ€§ï¼Œå…·æœ‰æ—¶é—´ä¾èµ–


'''
 7.5 Suppose you try one thousand configurations of the same investment strategy,
 and perform a CV on each of them. Some results are guaranteed to look good,
 just by sheer luck. If you only publish those positive results, and hide the rest,
 your audience will not be able to deduce that these results are false positives, a
 statistical fluke. This phenomenon is called â€œselection bias.â€
 (a) Can you imagine one procedure to prevent this?
 (b) What if we split the dataset in three sets: training, validation, and testing?
 The validation set is used to evaluate the trained parameters, and the testing
 is run only on the one configuration chosen in the validation phase. In what
 case does this procedure still fail?
 (c) What is the key to avoiding selection bias
'''

#è¿™æ ·çš„æƒ…å†µå°±åƒæ˜¯åœ¨freqtradeåšå‚æ•°è°ƒèŠ‚è¿›è¡Œå›æµ‹ï¼Œç„¶åå¾ˆå®¹æ˜“è¿‡æ‹Ÿåˆã€‚ä»Šå¹´å°±ä¸­æªäº†ï¼Œè¿‡æ‹Ÿåˆè€Œä¸è‡ªçŸ¥ã€‚
#a å¯èƒ½çš„é¿å…æ–¹æ³•ï¼š
#æ‹‰é•¿æ—¶é—´å›æµ‹ï¼Œçœ‹é•¿æœŸçš„å¹´åŒ–ï¼Œæœ€å¤§å›æ’¤ï¼Œå¤æ™®ã€‚çœ‹åˆ‡ç‰‡æ¯å¹´çš„æ”¶ç›Šæƒ…å†µ ï¼ˆæ—¶é—´ï¼‰
#å°èµ„é‡‘å…ˆè¯•ç›˜ï¼Œçœ‹çœ‹æ˜¯å¦ç¬¦åˆå›æµ‹é¢„æœŸ  ï¼Œæˆ–è€…è®¾ç½®éªŒè¯é›†ï¼Œè¿›è¡Œæ ·æœ¬å¤–äºŒæ¬¡éªŒè¯ã€‚ ï¼ˆæ ·æœ¬å¤–ï¼‰
#ç»æµé€»è¾‘ï¼Œç­–ç•¥åº”æœ‰åˆç†é‡‘èç†è®ºæ”¯æ’‘ï¼Œè€Œéçº¯æ•°æ®æ‹Ÿåˆ è¿™æ ·æ˜¯æœ‰é•¿æœŸä¿è¯çš„å¿…è¦æ¡ä»¶ã€‚
#åœ¨åç»­ç¬¬åç« ä¼šæ·±å…¥çš„æ¢è®¨è¿™äº›é—®é¢˜ã€‚

#b
#æœªæ¥å‡½æ•°ï¼Œä¿¡æ¯æ³„éœ²ï¼Œæ•°æ®ç‰›ç†Šåˆ†å¸ƒä¸å‡ï¼Œæ ·æœ¬é‡å ï¼Œæ•°æ®é‡å¤ªå°‘ï¼ˆæµ‹è¯•é›†ä¸å…·ä»£è¡¨æ€§ï¼‰

#c 
#æ•°æ®åˆ’åˆ†å¿…é¡»ä¸¥æ ¼æŒ‰æ—¶é—´é¡ºåºï¼šè®­ç»ƒé›† â†’ éªŒè¯é›† â†’ æµ‹è¯•é›†ï¼Œç¦æ­¢éšæœºæ‰“ä¹±ï¼›åœ¨äº‹ä»¶é©±åŠ¨æ ‡ç­¾ä¸­ï¼Œè¿˜éœ€ä½¿ç”¨ æ¸…é™¤ï¼ˆPurgingï¼‰å’Œç¦è¿ï¼ˆEmbargoï¼‰ æŠ€æœ¯ï¼Œé˜²æ­¢æ ·æœ¬é—´çš„æ—¶é—´é‡å æ±¡æŸ“ã€‚  
#åœ¨ä¸Šä¸€ä¸ªçš„åŸºç¡€ä¸Šï¼Œæµ‹è¯•é›†åªèƒ½ä½¿ç”¨ä¸€æ¬¡ï¼Œä½œä¸ºæœ€ç»ˆçš„è£åˆ¤


'''
ç¬¬ä¸ƒç« è¦ç‚¹ä¸æ€»ç»“ï¼š
1.kæŠ˜å äº¤å‰éªŒè¯åœ¨é‡‘èé¢†åŸŸï¼Œä¸ç®¡æ˜¯æ¨¡å‹å¼€å‘è¿˜æ˜¯å›æµ‹ï¼Œéƒ½æ˜¯å¤±è´¥çš„ã€‚å› ä¸ºKæŠ˜æ˜¯è¦æ±‚æ•°æ®ç‹¬ç«‹åŒåˆ†å¸ƒçš„ï¼Œé‡‘èæ•°æ®ä¸ç¬¦åˆè¦æ±‚ã€‚ç¬¬5ç« è®²åˆ°é‡‘èæ•°æ®æ˜¯å…·æœ‰è®°å¿†æ€§çš„ï¼Œç›´æ¥ç¡¬æ€§åˆ’åˆ†æ•°æ®ä¼šå¯¼è‡´ä¿¡æ¯æ³„éœ²â€”â€”æµ‹è¯•é›†çš„éƒ¨åˆ†ä¿¡æ¯åœ¨è®­ç»ƒé›†ä¸­ï¼Œè¿™å°±å¯¼è‡´è¿‡æ‹Ÿåˆ
ç¬¬äºŒä¸ªåŸå› æ˜¯ï¼Œåœ¨æ¨¡å‹å¼€å‘è¿‡ç¨‹ä¸­æµ‹è¯•é›†è¢«å¤šæ¬¡ä½¿ç”¨ï¼Œä»è€Œå¯¼è‡´å¤šæ¬¡æ£€éªŒå’Œé€‰æ‹©åå·®ã€‚ï¼ˆè¿™ä¸ªæš‚æœªç†è§£ï¼Œåé¢ä¼šè®²åˆ°ï¼‰
2.å¤„ç†ä¸€ï¼ˆPurgingï¼‰ï¼šæ ¹æ®æµ‹è¯•é›†çš„æ—¶é—´åŒºé—´ï¼Œå‰”é™¤æ‰€æœ‰ä¸æµ‹è¯•åŒºé—´å­˜åœ¨æ—¶é—´é‡å çš„è®­ç»ƒæ ·æœ¬ï¼Œä»¥é˜²æ­¢â€œæœªæ¥ä¿¡æ¯æ³„éœ²â€ï¼ˆlook-ahead biasï¼‰ã€‚
3.å¤„ç†äºŒï¼ˆ Embargoï¼‰ï¼š
åŸå› ï¼šå‰”é™¤å¯èƒ½çš„æœªæ¥å‡½æ•°ã€‚

ä¾‹å­ï¼š
feature = MA_20 / MA_50
åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
æµ‹è¯•é›†æ—¶é—´ï¼š2023-07-01 è‡³ 2023-07-31
ä½ å·²ç»åšäº† Purgingï¼šç¡®ä¿æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„æ ‡ç­¾ç»“æŸæ—¶é—´ < 2023-07-01
ä¾‹å¦‚ï¼Œæœ€åä¸€ä¸ªè®­ç»ƒæ ·æœ¬çš„è§‚å¯Ÿæ—¥æ˜¯ 2023-06-30ï¼Œå…¶æ ‡ç­¾è¦†ç›– 2023-07-01 åˆ° 2023-07-05 â†’ âŒ è¢« Purging å‰”é™¤
æ‰€ä»¥ä½ ä¿ç•™çš„æœ€åä¸€ä¸ªè®­ç»ƒæ ·æœ¬æ˜¯ 2023-06-23ï¼ˆæ ‡ç­¾è¦†ç›– 6/26â€“6/30ï¼Œå®Œå…¨åœ¨7æœˆå‰ï¼‰
âœ… çœ‹ä¼¼å®‰å…¨ï¼

 ä½† Embargo è¦è§£å†³çš„é—®é¢˜å‡ºç°äº†ï¼
è€ƒè™‘ä¸€ä¸ªè®­ç»ƒæ ·æœ¬ï¼šè§‚å¯Ÿæ—¥ = 2023-06-23

å®ƒçš„æ ‡ç­¾ï¼šåŸºäº 2023-06-26 åˆ° 2023-06-30 çš„ä»·æ ¼ â†’ âœ… åœ¨æµ‹è¯•å‰ï¼Œæ²¡é—®é¢˜
å®ƒçš„ç‰¹å¾ MA_20 / MA_50ï¼šéœ€è¦ 2023-04-14 åˆ° 2023-06-23 çš„ä»·æ ¼æ•°æ® â†’ âœ… çœ‹èµ·æ¥ä¹Ÿæ²¡é—®é¢˜ï¼Ÿ
âš ï¸ å…³é”®æ¥äº†ï¼š

æµ‹è¯•é›†ä» 2023-07-01 å¼€å§‹
è€Œä½ çš„è®­ç»ƒæ ·æœ¬ç”¨åˆ°äº† 2023-06-23 çš„ä»·æ ¼ â€”â€” è¿™æ˜¯æµ‹è¯•å‰æœ€åä¸€ä¸ªäº¤æ˜“æ—¥
åœ¨å®ç›˜ä¸­ï¼Œ2023-06-23 çš„ä»·æ ¼åœ¨ 2023-06-23 æ”¶ç›˜åæ‰ç¡®å®šï¼Œè€Œä½ è¦åœ¨ 2023-06-23 ç›˜ä¸­æˆ–ä¹‹å‰ åšå‡ºé¢„æµ‹

å¤„ç†äºŒç»“è®ºï¼šåœ¨åŠ ä¸€ä¸ªå°çª—å£è¿›è¡ŒEmbargoå³å¯é˜²æ­¢æœªæ¥å‡½æ•°ã€‚

#å…¶å®è¿˜æœ‰ä¸ªå°ç–‘æƒ‘ï¼Œå¦‚æœäº‹ä»¶çš„é•¿åº¦æ˜¯5ä¸ªbarï¼Œä½†æ˜¯ç”¨åˆ°äº†æ¯”å¦‚52ä¸ªbarçš„å‡å€¼æŒ‡æ ‡ï¼Œè¿™ä¸ªæ—¶å€™æ¸…é™¤+ç¦æ­¢èƒ½å¤Ÿé˜²æ­¢ä¿¡æ¯æ³„éœ²å—â€”â€”â€”â€”è¿™å°±æ˜¯ç¦æ­¢æ‰€å…¶å·¦å³çš„åœºæ™¯ã€‚åœ¨è®­ç»ƒé›†1ï¼ˆé—´éš”1ï¼‰ æµ‹è¯•é›† ï¼ˆé—´éš”2ï¼‰è®­ç»ƒé›†2  è¿™æ ·çš„åœºæ™¯ä¸­ï¼Œä¸¥æ ¼æ¥è¯´ç¦æ­¢æœŸéœ€è¦å¤§äºç­‰äºï¼ˆæµ‹è¯•é›†+è®­ç»ƒé›†2ï¼‰ä¸­é—´çš„é—´éš”2ï¼Œè¿™æ ·æµ‹è¯•é›†çš„ä¿¡æ¯æ‰ä¸ä¼šæ³„éœ²åˆ°è®­ç»ƒé›†2.è€Œä¸”ç”±äºæ˜¯ä½¿ç”¨emaè¿™æ ·çš„æƒé‡æ–¹å¼ï¼Œç¦æ­¢å»æ‰äº†é è¿‘è®­ç»ƒé›†2çš„é—´éš”2æ•°æ®ï¼Œä¹Ÿèƒ½å¤Ÿæœ‰æ•ˆé˜²æ­¢ä¿¡æ¯æ³„éœ²ã€‚è‡³äºåœ¨é—´éš”1çš„ç‰¹å¾æ•°æ®ï¼Œæ˜¯ä¸å­˜åœ¨ä¿¡æ¯æ³„éœ²çš„ï¼Œæµ‹è¯•é›†ä½¿ç”¨äº†éƒ¨åˆ†è®­ç»ƒé›†çš„ä¿¡æ¯æ˜¯å®Œå…¨OKçš„ã€‚â€”â€”ä¸ºäº†é˜²æ­¢æ•°æ®ç¼ºå¤±è¿‡å¤šï¼Œåªä½¿ç”¨ç¦æ­¢å°±å¤Ÿäº†ï¼Œæ•°æ®é‡è¶³å¤Ÿ+éœ€è¦ä¸¥æ ¼é˜²æ­¢ä¿¡æ¯æ³„éœ²æ‰éœ€è¦å¢å¤§ç¦æ­¢æœŸã€‚â€”â€”ä¸ºäº†æé«˜ç¦æ­¢çš„æœ‰æ•ˆæ€§ï¼Œç§»åŠ¨å¹³å‡è¿™äº›æŒ‡æ ‡è®¡ç®—è¦ä½¿ç”¨emaè¿™æ ·çš„è¶Šè¿‘æƒé‡è¶Šå¤§çš„æŒ‡æ ‡ã€‚


4.ç»“è®ºï¼šæ¯æ¬¡è¿›è¡Œè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„åˆ’åˆ†æ—¶å¿…é¡»è¦ä½¿ç”¨æ¸…é™¤å’Œç¦æ­¢æ¥åˆ’åˆ†ã€‚è¿™é‡Œæœ‰æ–°å¢çš„k-foldç±»æ¥æ›¿ä»£sklearnè‡ªå¸¦çš„åˆ’åˆ†æ–¹æ³•ã€‚
'''

#%%

#ç¬¬å…«ç«  ç‰¹å¾

'''
 8.1 Using the code presented in Section 8.6:
 (a) Generate a dataset (X,y).
 (b) Apply a PCA transformation on X, which we denote Ì‡ X.
 (c) Compute MDI, MDA, and SFI feature importance on ( Ì‡ X,y), where the base
 estimator is RF.
 (d) Do the three methods agree on what features are important? Why?
'''

#a
import pandas as pd
from sklearn.datasets import make_classification
import datetime

# def getTestData(n_features=40, n_informative=10, n_redundant=10, n_samples=10000):
#     """
#     ç”Ÿæˆä¸€ä¸ªå¸¦æ—¶é—´ç´¢å¼•çš„åˆæˆåˆ†ç±»æ•°æ®é›†ï¼Œç”¨äºé‡‘èæœºå™¨å­¦ä¹ å®éªŒï¼ˆå¦‚ AFML æ¡†æ¶ï¼‰ã€‚

#     å‚æ•°:
#         n_features: æ€»ç‰¹å¾æ•°é‡ï¼ˆé»˜è®¤ 40ï¼‰
#         n_informative: æœ‰ä¿¡æ¯é‡çš„ç‰¹å¾æ•°é‡ï¼ˆçœŸæ­£ä¸æ ‡ç­¾ç›¸å…³çš„ç‰¹å¾ï¼Œé»˜è®¤ 10ï¼‰
#         n_redundant: å†—ä½™ç‰¹å¾æ•°é‡ï¼ˆç”±æœ‰ä¿¡æ¯é‡ç‰¹å¾çº¿æ€§ç»„åˆç”Ÿæˆï¼Œé»˜è®¤ 10ï¼‰
#         n_samples: æ ·æœ¬æ•°é‡ï¼ˆé»˜è®¤ 10000ï¼‰

#     è¿”å›:
#         trnsX (pd.DataFrame): ç‰¹å¾çŸ©é˜µï¼Œç´¢å¼•ä¸ºå·¥ä½œæ—¥æ—¥æœŸ
#         cont (pd.DataFrame): åŒ…å«ä»¥ä¸‹åˆ—ï¼š
#             - 'bin': äºŒåˆ†ç±»æ ‡ç­¾ï¼ˆ0 æˆ– 1ï¼‰
#             - 'w': æ ·æœ¬æƒé‡ï¼ˆåˆå§‹ä¸ºå‡åŒ€æƒé‡ï¼‰
#             - 't1': äº‹ä»¶ç»“æŸæ—¶é—´ï¼ˆæ­¤å¤„è®¾ä¸ºä¸æ ·æœ¬æ—¶é—´ç›¸åŒï¼Œæ¨¡æ‹Ÿç¬æ—¶äº‹ä»¶ï¼‰
#     """
#     # ä½¿ç”¨ sklearn ç”Ÿæˆåˆæˆåˆ†ç±»æ•°æ®
#     trnsX, cont = make_classification(
#         n_samples=n_samples,          # æ ·æœ¬æ•°
#         n_features=n_features,        # æ€»ç‰¹å¾æ•°
#         n_informative=n_informative,  # çœŸå®æœ‰æ•ˆç‰¹å¾æ•°
#         n_redundant=n_redundant,      # å†—ä½™ç‰¹å¾æ•°ï¼ˆç”±æœ‰æ•ˆç‰¹å¾æ´¾ç”Ÿï¼‰
#         random_state=0,               # éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°
#         shuffle=False                 # ä¸æ‰“ä¹±é¡ºåºï¼Œä¿ç•™â€œæ—¶é—´â€ç»“æ„ï¼ˆå°½ç®¡æ•°æ®æœ¬èº«æ— æ—¶åºä¾èµ–ï¼‰
#     )
    
#     # åˆ›å»ºä»¥ä»Šå¤©ä¸ºç»ˆç‚¹çš„å·¥ä½œæ—¥æ—¥æœŸç´¢å¼•ï¼ˆå…± n_samples ä¸ªäº¤æ˜“æ—¥ï¼‰
#     end_date = pd.Timestamp.today().normalize()  # è·å–ä»Šå¤©çš„æ—¥æœŸï¼ˆå»æ‰æ—¶åˆ†ç§’ï¼‰
#     date_index = pd.date_range(
#         end=end_date,
#         periods=n_samples,
#         # freq=pd.tseries.offsets.BDay()  # å·¥ä½œæ—¥é¢‘ç‡ï¼ˆè·³è¿‡å‘¨æœ«å’ŒèŠ‚å‡æ—¥ï¼‰
#         freq='H'  # æŒ‰å°æ—¶é¢‘ç‡ç”Ÿæˆæ—¶é—´æˆ³
#     )
    
#     # è½¬æ¢ä¸º pandas DataFrame å’Œ Seriesï¼Œå¹¶è®¾ç½®æ—¶é—´ç´¢å¼•
#     trnsX = pd.DataFrame(trnsX, index=date_index)
#     cont = pd.Series(cont, index=date_index).to_frame('bin')  # 'bin' è¡¨ç¤ºäºŒå…ƒæ ‡ç­¾
    
#     # æ„é€ ç‰¹å¾åˆ—åï¼š
#     # - I_0, I_1, ... : æœ‰ä¿¡æ¯é‡çš„ç‰¹å¾ï¼ˆInformativeï¼‰
#     # - R_0, R_1, ... : å†—ä½™ç‰¹å¾ï¼ˆRedundantï¼‰
#     # - N_0, N_1, ... : å™ªå£°ç‰¹å¾ï¼ˆNoiseï¼Œæ—¢ä¸ç›¸å…³ä¹Ÿä¸å†—ä½™ï¼‰
#     informative_cols = [f'I_{i}' for i in range(n_informative)]
#     redundant_cols = [f'R_{i}' for i in range(n_redundant)]
#     noise_cols = [f'N_{i}' for i in range(n_features - n_informative - n_redundant)]
#     trnsX.columns = informative_cols + redundant_cols + noise_cols
    
#     # æ·»åŠ æ ·æœ¬æƒé‡ï¼šåˆå§‹è®¾ä¸ºå‡åŒ€æƒé‡ï¼ˆåç»­å¯æ›¿æ¢ä¸ºåŸºäºå”¯ä¸€æ€§çš„æƒé‡ï¼‰
#     cont['w'] = 1.0 / cont.shape[0]
    
#     # æ·»åŠ  t1 åˆ—ï¼ˆäº‹ä»¶ç»“æŸæ—¶é—´ï¼‰ï¼š
#     # åœ¨çœŸå® triple-barrier æ ‡ç­¾ä¸­ï¼Œt1 æ˜¯æœªæ¥æŸä¸ªæ—¶é—´ç‚¹ï¼›
#     # æ­¤å¤„ä¸ºç®€åŒ–ï¼Œè®¾ä¸ºå½“å‰æ ·æœ¬æ—¶é—´ï¼ˆå³æ¯ä¸ªäº‹ä»¶åªè¦†ç›–ä¸€ä¸ª barï¼‰
#     cont['t1'] = cont.index.copy()
    
#     return trnsX, cont
def getTestData(n_features=40, n_informative=10, n_redundant=10, n_samples=10000, target_oob_range=(0.5, 0.8)):
    """
    ç”Ÿæˆä¸€ä¸ªå¸¦æ—¶é—´ç´¢å¼•çš„åˆæˆåˆ†ç±»æ•°æ®é›†ï¼Œç”¨äºé‡‘èæœºå™¨å­¦ä¹ å®éªŒï¼ˆå¦‚ AFML æ¡†æ¶ï¼‰ã€‚
    æ”¹è¿›ç‰ˆæœ¬ï¼šç”Ÿæˆçš„æ•°æ®æ›´æ¥è¿‘çœŸå®é‡‘èæ•°æ®ï¼ŒOOBåˆ†æ•°åœ¨50%-80%èŒƒå›´å†…ã€‚

    å‚æ•°:
        n_features: æ€»ç‰¹å¾æ•°é‡ï¼ˆé»˜è®¤ 40ï¼‰
        n_informative: æœ‰ä¿¡æ¯é‡çš„ç‰¹å¾æ•°é‡ï¼ˆçœŸæ­£ä¸æ ‡ç­¾ç›¸å…³çš„ç‰¹å¾ï¼Œé»˜è®¤ 10ï¼‰
        n_redundant: å†—ä½™ç‰¹å¾æ•°é‡ï¼ˆç”±æœ‰ä¿¡æ¯é‡ç‰¹å¾çº¿æ€§ç»„åˆç”Ÿæˆï¼Œé»˜è®¤ 10ï¼‰
        n_samples: æ ·æœ¬æ•°é‡ï¼ˆé»˜è®¤ 10000ï¼‰
        target_oob_range: ç›®æ ‡OOBåˆ†æ•°èŒƒå›´ï¼ˆé»˜è®¤ (0.6, 0.8)ï¼‰

    è¿”å›:
        trnsX (pd.DataFrame): ç‰¹å¾çŸ©é˜µï¼Œç´¢å¼•ä¸ºå·¥ä½œæ—¥æ—¥æœŸ
        cont (pd.DataFrame): åŒ…å«ä»¥ä¸‹åˆ—ï¼š
            - 'bin': äºŒåˆ†ç±»æ ‡ç­¾ï¼ˆ0 æˆ– 1ï¼‰
            - 'w': æ ·æœ¬æƒé‡ï¼ˆåˆå§‹ä¸ºå‡åŒ€æƒé‡ï¼‰
            - 't1': äº‹ä»¶ç»“æŸæ—¶é—´ï¼ˆæ­¤å¤„è®¾ä¸ºä¸æ ·æœ¬æ—¶é—´ç›¸åŒï¼Œæ¨¡æ‹Ÿç¬æ—¶äº‹ä»¶ï¼‰
    """
    import numpy as np
    from sklearn.datasets import make_classification
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import train_test_split
    
    # è°ƒæ•´å‚æ•°ä»¥é™ä½æ¨¡å‹æ€§èƒ½ï¼Œä½¿å…¶æ›´æ¥è¿‘çœŸå®é‡‘èæ•°æ®
    # 1. é™ä½ç‰¹å¾ä¸æ ‡ç­¾çš„ç›¸å…³æ€§
    # 2. å¢åŠ ç±»åˆ«ä¸å¹³è¡¡
    # 3. å¢åŠ å™ªå£°
    
    # åˆå§‹å°è¯•ä½¿ç”¨é»˜è®¤å‚æ•°
    flip_y = 0.05  # 5%çš„æ ‡ç­¾éšæœºç¿»è½¬
    class_sep = 0.8 # ç±»åˆ«åˆ†ç¦»åº¦ï¼Œé»˜è®¤ä¸º1.0ï¼Œé™ä½æ­¤å€¼ä½¿ç±»åˆ«æ›´éš¾åŒºåˆ†
    
    # è¿­ä»£è°ƒæ•´å‚æ•°ç›´åˆ°è¾¾åˆ°ç›®æ ‡OOBèŒƒå›´
    max_attempts = 5
    for attempt in range(max_attempts):
        # ä½¿ç”¨ sklearn ç”Ÿæˆåˆæˆåˆ†ç±»æ•°æ®
        trnsX, cont = make_classification(
            n_samples=n_samples,          # æ ·æœ¬æ•°
            n_features=n_features,        # æ€»ç‰¹å¾æ•°
            n_informative=n_informative,  # çœŸå®æœ‰æ•ˆç‰¹å¾æ•°
            n_redundant=n_redundant,      # å†—ä½™ç‰¹å¾æ•°ï¼ˆç”±æœ‰æ•ˆç‰¹å¾æ´¾ç”Ÿï¼‰
            flip_y=flip_y,                # éšæœºç¿»è½¬çš„æ ‡ç­¾æ¯”ä¾‹
            class_sep=class_sep,          # ç±»åˆ«åˆ†ç¦»åº¦
            random_state=42+attempt,      # æ¯æ¬¡å°è¯•ä½¿ç”¨ä¸åŒçš„éšæœºç§å­
            shuffle=False                 # ä¸æ‰“ä¹±é¡ºåºï¼Œä¿ç•™"æ—¶é—´"ç»“æ„
        )
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„éšæœºæ£®æ—æ¥æµ‹è¯•OOBåˆ†æ•°
        rf_test = RandomForestClassifier(
            n_estimators=50,
            max_depth=10,
            n_jobs=-1,
            oob_score=True,
            random_state=42
        )
        
        # æ‹Ÿåˆæ¨¡å‹
        rf_test.fit(trnsX, cont)
        oob_score = rf_test.oob_score_
        
        print(f"å°è¯• {attempt+1}: flip_y={flip_y}, class_sep={class_sep}, OOB={oob_score:.4f}")
        
        # å¦‚æœOOBåˆ†æ•°åœ¨ç›®æ ‡èŒƒå›´å†…ï¼Œè·³å‡ºå¾ªç¯
        if target_oob_range[0] <= oob_score <= target_oob_range[1]:
            break
            
        # è°ƒæ•´å‚æ•°
        if oob_score > target_oob_range[1]:  # OOBå¤ªé«˜ï¼Œå¢åŠ éš¾åº¦
            flip_y = min(0.2, flip_y + 0.03)  # å¢åŠ æ ‡ç­¾å™ªå£°
            class_sep = max(0.5, class_sep - 0.1)  # é™ä½ç±»åˆ«åˆ†ç¦»åº¦
        else:  # OOBå¤ªä½ï¼Œé™ä½éš¾åº¦
            flip_y = max(0.01, flip_y - 0.02)  # å‡å°‘æ ‡ç­¾å™ªå£°
            class_sep = min(2.0, class_sep + 0.1)  # å¢åŠ ç±»åˆ«åˆ†ç¦»åº¦
    
    # åˆ›å»ºä»¥ä»Šå¤©ä¸ºç»ˆç‚¹çš„å·¥ä½œæ—¥æ—¥æœŸç´¢å¼•ï¼ˆå…± n_samples ä¸ªäº¤æ˜“æ—¥ï¼‰
    end_date = pd.Timestamp.today().normalize()  # è·å–ä»Šå¤©çš„æ—¥æœŸï¼ˆå»æ‰æ—¶åˆ†ç§’ï¼‰
    date_index = pd.date_range(
        end=end_date,
        periods=n_samples,
        # freq=pd.tseries.offsets.BDay()  # å·¥ä½œæ—¥é¢‘ç‡ï¼ˆè·³è¿‡å‘¨æœ«å’ŒèŠ‚å‡æ—¥ï¼‰
        freq='h'  # æŒ‰å°æ—¶é¢‘ç‡ç”Ÿæˆæ—¶é—´æˆ³
    )
    
    # è½¬æ¢ä¸º pandas DataFrame å’Œ Seriesï¼Œå¹¶è®¾ç½®æ—¶é—´ç´¢å¼•
    trnsX = pd.DataFrame(trnsX, index=date_index)
    cont = pd.Series(cont, index=date_index).to_frame('bin')  # 'bin' è¡¨ç¤ºäºŒå…ƒæ ‡ç­¾
    
    # æ„é€ ç‰¹å¾åˆ—åï¼š
    # - I_0, I_1, ... : æœ‰ä¿¡æ¯é‡çš„ç‰¹å¾ï¼ˆInformativeï¼‰
    # - R_0, R_1, ... : å†—ä½™ç‰¹å¾ï¼ˆRedundantï¼‰
    # - N_0, N_1, ... : å™ªå£°ç‰¹å¾ï¼ˆNoiseï¼Œæ—¢ä¸ç›¸å…³ä¹Ÿä¸å†—ä½™ï¼‰
    informative_cols = [f'I_{i}' for i in range(n_informative)]
    redundant_cols = [f'R_{i}' for i in range(n_redundant)]
    noise_cols = [f'N_{i}' for i in range(n_features - n_informative - n_redundant)]
    trnsX.columns = informative_cols + redundant_cols + noise_cols
    
    # æ·»åŠ æ ·æœ¬æƒé‡ï¼šåˆå§‹è®¾ä¸ºå‡åŒ€æƒé‡ï¼ˆåç»­å¯æ›¿æ¢ä¸ºåŸºäºå”¯ä¸€æ€§çš„æƒé‡ï¼‰
    cont['w'] = 1.0 / cont.shape[0]
    
    # æ·»åŠ  t1 åˆ—ï¼ˆäº‹ä»¶ç»“æŸæ—¶é—´ï¼‰ï¼š
    # åœ¨çœŸå® triple-barrier æ ‡ç­¾ä¸­ï¼Œt1 æ˜¯æœªæ¥æŸä¸ªæ—¶é—´ç‚¹ï¼›
    # æ­¤å¤„ä¸ºç®€åŒ–ï¼Œè®¾ä¸ºå½“å‰æ ·æœ¬æ—¶é—´ï¼ˆå³æ¯ä¸ªäº‹ä»¶åªè¦†ç›–ä¸€ä¸ª barï¼‰
    cont['t1'] = cont.index.copy()
    
    print(f"æœ€ç»ˆOOBåˆ†æ•°: {oob_score:.4f}")
    
    return trnsX, cont

trnsX,cont=getTestData()


#b åº”ç”¨pcaé™ç»´
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

def apply_pca_to_features(trnsX, n_components=None, variance_threshold=0.95):
    """
    å¯¹ç‰¹å¾çŸ©é˜µ trnsX åº”ç”¨ PCA é™ç»´ã€‚

    å‚æ•°:
        trnsX (pd.DataFrame): åŸå§‹ç‰¹å¾çŸ©é˜µï¼Œç´¢å¼•ä¸ºæ—¶é—´
        n_components (int or None): æŒ‡å®šä¿ç•™çš„ä¸»æˆåˆ†æ•°é‡ã€‚
            - è‹¥ä¸º Noneï¼Œåˆ™æ ¹æ® variance_threshold è‡ªåŠ¨é€‰æ‹©
        variance_threshold (float): ç´¯ç§¯æ–¹å·®è§£é‡Šæ¯”ä¾‹é˜ˆå€¼ï¼ˆä»…å½“ n_components=None æ—¶ç”Ÿæ•ˆï¼‰

    è¿”å›:
        trnsX_pca (pd.DataFrame): é™ç»´åçš„ç‰¹å¾çŸ©é˜µï¼Œåˆ—åä¸º 'PC_0', 'PC_1', ...
    """
    # 1. æ ‡å‡†åŒ–ï¼šPCA å¯¹é‡çº²æ•æ„Ÿï¼Œå¿…é¡»å…ˆæ ‡å‡†åŒ–ï¼ˆå‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1ï¼‰
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(trnsX)
    
    # 2. åˆå§‹åŒ– PCA
    if n_components is None:
        # è‡ªåŠ¨é€‰æ‹©èƒ½è§£é‡Šè‡³å°‘ variance_threshold æ–¹å·®çš„æœ€å°‘ä¸»æˆåˆ†
        pca = PCA(n_components=variance_threshold)
    else:
        pca = PCA(n_components=n_components)
    
    # 3. æ‹Ÿåˆå¹¶è½¬æ¢
    X_pca = pca.fit_transform(X_scaled)
    
    # 4. æ„é€ æ–°çš„åˆ—åï¼šPC_0, PC_1, ...
    n_final = X_pca.shape[1]
    pca_columns = [f'PC_{i}' for i in range(n_final)]
    
    # 5. è½¬æ¢ä¸º DataFrameï¼Œä¿ç•™åŸå§‹æ—¶é—´ç´¢å¼•
    trnsX_pca = pd.DataFrame(X_pca, index=trnsX.index, columns=pca_columns)
    
    # ï¼ˆå¯é€‰ï¼‰æ‰“å°ä¿¡æ¯
    print(f"åŸå§‹ç‰¹å¾æ•°: {trnsX.shape[1]}")
    print(f"é™ç»´åç‰¹å¾æ•°: {n_final}")
    print(f"ç´¯ç§¯è§£é‡Šæ–¹å·®æ¯”ä¾‹: {pca.explained_variance_ratio_.sum():.4f}")
    if n_components is None:
        print(f"è‡ªåŠ¨é€‰æ‹©ä¸»æˆåˆ†æ•°é‡ä»¥ä¿ç•™ â‰¥{variance_threshold:.0%} çš„æ–¹å·®")
    
    return trnsX_pca, pca, scaler  # è¿”å›å˜æ¢å™¨ä»¥ä¾¿åç»­ç”¨äºæ–°æ•°æ®


# 2. åº”ç”¨ PCAï¼ˆä¿ç•™ 95% æ–¹å·®ï¼‰
trnsX_pca, pca_model, scaler_model = apply_pca_to_features(
    trnsX, 
    variance_threshold=0.95
)

# ï¼ˆå¯é€‰ï¼‰æŸ¥çœ‹å„ä¸»æˆåˆ†è§£é‡Šçš„æ–¹å·®æ¯”ä¾‹
print("\nå„ä¸»æˆåˆ†è§£é‡Šçš„æ–¹å·®æ¯”ä¾‹:")
print(pca_model.explained_variance_ratio_)


#c åŸºäºéšæœºæ£®æ—æ¨¡å‹è®¡ç®—MDI, MDA, and SFI
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, log_loss

def featImpMDI(fit, featNames):
    """
    åŸºäºæ ·æœ¬å†…ï¼ˆIn-Sampleï¼‰å¹³å‡ä¸çº¯åº¦å‡å°‘ï¼ˆMDIï¼‰è®¡ç®—ç‰¹å¾é‡è¦æ€§ã€‚
    
    æ³¨æ„ï¼šé€‚ç”¨äºæ¯æ£µæ ‘åªä½¿ç”¨ä¸€ä¸ªç‰¹å¾çš„ Bagging æ¨¡å‹ï¼ˆå¦‚ AFML æ¨èè®¾ç½®ï¼‰ï¼Œ
          æ­¤æ—¶æ¯æ£µæ ‘çš„ feature_importances_ ä¸­åªæœ‰ä¸€ä¸ªéé›¶å€¼ï¼Œå…¶ä½™ä¸º 0ã€‚
    
    å‚æ•°:
        fit: å·²è®­ç»ƒçš„ BaggingClassifier æˆ–ç±»ä¼¼é›†æˆæ¨¡å‹ï¼ˆéœ€æœ‰ .estimators_ï¼‰
        featNames: ç‰¹å¾åç§°åˆ—è¡¨ï¼ˆä¸è¾“å…¥ç‰¹å¾é¡ºåºä¸€è‡´ï¼‰
    
    è¿”å›:
        pd.DataFrame: åŒ…å« 'mean' å’Œ 'std' ä¸¤åˆ—ï¼Œå·²å½’ä¸€åŒ–ï¼ˆmean åˆ—æ€»å’Œä¸º 1ï¼‰
    """
    # æå–æ¯æ£µæ ‘çš„ç‰¹å¾é‡è¦æ€§ï¼ˆæ¯æ£µæ ‘æ˜¯ä¸€ä¸ªæ•°ç»„ï¼‰
    # å½“ max_features=1 æ—¶ï¼Œæ¯æ£µæ ‘åªæœ‰ä¸€ä¸ªç‰¹å¾çš„é‡è¦æ€§éé›¶ï¼Œå…¶ä½™ä¸º 0
    df0 = {i: tree.feature_importances_ for i, tree in enumerate(fit.estimators_)}
    
    # è½¬ä¸º DataFrameï¼šè¡Œ=æ ‘ï¼Œåˆ—=ç‰¹å¾
    df0 = pd.DataFrame.from_dict(df0, orient='index')
    df0.columns = featNames
    
    # å°† 0 æ›¿æ¢ä¸º NaNï¼Œä»¥ä¾¿åç»­ç»Ÿè®¡å¿½ç•¥æœªè¢«ä½¿ç”¨çš„ç‰¹å¾
    # ï¼ˆå› ä¸ºæ¯æ£µæ ‘åªç”¨ä¸€ä¸ªç‰¹å¾ï¼Œå…¶ä»–éƒ½æ˜¯ 0ï¼Œä¸ä»£è¡¨â€œä¸é‡è¦â€ï¼Œè€Œæ˜¯â€œæœªä½¿ç”¨â€ï¼‰
    df0 = df0.replace(0, np.nan)
    
    # è®¡ç®—å‡å€¼å’Œæ ‡å‡†è¯¯ï¼ˆStandard Error = std / sqrt(n)ï¼‰
    mean_imp = df0.mean()
    std_err = df0.std() / np.sqrt(df0.count())  # ä½¿ç”¨ count() é¿å… NaN å½±å“
    
    # æ„é€ ç»“æœ DataFrame
    imp = pd.DataFrame({
        'mean': mean_imp,
        'std': std_err
    })
    
    # å½’ä¸€åŒ–ï¼šä½¿ mean åˆ—çš„æ€»å’Œä¸º 1ï¼ˆä¾¿äºè§£é‡Šä¸ºç›¸å¯¹é‡è¦æ€§ï¼‰
    imp['mean'] /= imp['mean'].sum()
    
    return imp

def featImpMDA(clf, X, y, cv, sample_weight, t1, pctEmbargo, scoring='neg_log_loss'):
    """
    åŸºäºæ ·æœ¬å¤–ï¼ˆOOSï¼‰æ€§èƒ½ä¸‹é™è®¡ç®— MDAï¼ˆMean Decrease Accuracyï¼‰ç‰¹å¾é‡è¦æ€§ã€‚
    
    å‚æ•°:
        clf: å·²å®šä¹‰ä½†æœªè®­ç»ƒçš„åˆ†ç±»å™¨ï¼ˆéœ€æ”¯æŒ fit/predict/predict_probaï¼‰
        X: ç‰¹å¾ DataFrame
        y: æ ‡ç­¾ Series
        cv: CV æŠ˜æ•°
        sample_weight: æ ·æœ¬æƒé‡ Series
        t1: äº‹ä»¶ç»“æŸæ—¶é—´ Seriesï¼ˆç”¨äº PurgedKFoldï¼‰
        pctEmbargo: Embargo æ¯”ä¾‹ï¼ˆé˜²æ­¢ä¿¡æ¯æ³„éœ²ï¼‰
        scoring: è¯„åˆ†æ–¹å¼ï¼Œæ”¯æŒ 'neg_log_loss' æˆ– 'accuracy'
    
    è¿”å›:
        imp (pd.DataFrame): å„ç‰¹å¾çš„ MDA é‡è¦æ€§ï¼ˆmean å’Œ stdï¼‰
        oos_score (float): åŸå§‹ OOS å¹³å‡å¾—åˆ†
    """
    if scoring not in ['neg_log_loss', 'accuracy']:
        raise ValueError("scoring å¿…é¡»æ˜¯ 'neg_log_loss' æˆ– 'accuracy'")

    # from crossValidation import PurgedKFold  # ç¡®ä¿è¯¥æ¨¡å—å·²å®ç°ä¸”å…¼å®¹ Python 3

    # åˆå§‹åŒ– Purged K-Fold äº¤å‰éªŒè¯ç”Ÿæˆå™¨
    cvGen = PurgedKFold(n_splits=cv, t1=t1, pctEmbargo=pctEmbargo)

    # å­˜å‚¨åŸå§‹ OOS åˆ†æ•°ï¼ˆæ¯æŠ˜ä¸€ä¸ªå€¼ï¼‰
    scr0 = pd.Series(dtype=float)
    # å­˜å‚¨æ‰“ä¹±æ¯ä¸ªç‰¹å¾åçš„ OOS åˆ†æ•°ï¼ˆæ¯æŠ˜ Ã— æ¯ç‰¹å¾ï¼‰
    scr1 = pd.DataFrame(columns=X.columns, dtype=float)

    # æ‰§è¡Œäº¤å‰éªŒè¯
    for i, (train, test) in enumerate(cvGen.split(X=X)):
        # è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ’åˆ†
        X0, y0, w0 = X.iloc[train, :], y.iloc[train], sample_weight.iloc[train]
        X1, y1, w1 = X.iloc[test, :], y.iloc[test], sample_weight.iloc[test]

        # åœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆæ¨¡å‹
        fit = clf.fit(X=X0, y=y0, sample_weight=w0.values)

        # è®¡ç®—åŸå§‹ OOS åˆ†æ•°
        if scoring == 'neg_log_loss':
            prob = fit.predict_proba(X1)
            score_orig = -log_loss(y1, prob, sample_weight=w1.values, labels=clf.classes_)
        else:  # accuracy
            pred = fit.predict(X1)
            score_orig = accuracy_score(y1, pred, sample_weight=w1.values)
        scr0.loc[i] = score_orig

        # å¯¹æ¯ä¸ªç‰¹å¾è¿›è¡Œæ‰“ä¹±æµ‹è¯•
        for j in X.columns:
            # æ·±æ‹·è´æµ‹è¯•é›†ç‰¹å¾
            X1_permuted = X1.copy()
            # æ‰“ä¹±ç¬¬ j åˆ—ï¼ˆæ³¨æ„ï¼šå¿…é¡»æ“ä½œ .values ä»¥é¿å… pandas è­¦å‘Šï¼‰
            shuffled_values = X1_permuted[j].values.copy()
            np.random.shuffle(shuffled_values)
            X1_permuted[j] = shuffled_values

            # ç”¨æ‰“ä¹±åçš„æ•°æ®é¢„æµ‹
            if scoring == 'neg_log_loss':
                prob_perm = fit.predict_proba(X1_permuted)
                score_perm = -log_loss(y1, prob_perm, sample_weight=w1.values, labels=clf.classes_)
            else:
                pred_perm = fit.predict(X1_permuted)
                score_perm = accuracy_score(y1, pred_perm, sample_weight=w1.values)
            
            scr1.loc[i, j] = score_perm

    # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„é‡è¦æ€§ï¼šåŸå§‹åˆ†æ•° - æ‰“ä¹±ååˆ†æ•°ï¼ˆè¶Šå¤§è¶Šé‡è¦ï¼‰
    # æ³¨æ„ï¼šscr0 æ˜¯ Series (n_splits,), scr1 æ˜¯ DataFrame (n_splits, n_features)
    imp = scr0.values[:, None] - scr1.values  # å¹¿æ’­ç›¸å‡
    imp = pd.DataFrame(imp, index=scr1.index, columns=scr1.columns)

    # å½’ä¸€åŒ–ï¼ˆç›¸å¯¹ä¸‹é™æ¯”ä¾‹ï¼‰
    if scoring == 'neg_log_loss':
        # é¿å…é™¤é›¶ï¼šä½¿ç”¨æ‰“ä¹±åçš„åˆ†æ•°ä½œä¸ºåˆ†æ¯ï¼ˆAFML åŸå§‹åšæ³•ï¼‰
        imp = imp / (-scr1 + 1e-10)  # åŠ å°é‡é˜²æ­¢é™¤é›¶
    else:
        # accuracy: æœ€å¤§å¯èƒ½ä¸‹é™æ˜¯ (1 - æ‰“ä¹±åå‡†ç¡®ç‡)
        imp = imp / (1.0 - scr1 + 1e-10)

    # è®¡ç®—å‡å€¼å’Œæ ‡å‡†è¯¯ï¼ˆStandard Error = std / sqrt(n)ï¼‰
    mean_imp = imp.mean()
    std_err = imp.std() / np.sqrt(imp.shape[0])

    # æ„é€ ç»“æœ DataFrame
    result_imp = pd.DataFrame({
        'mean': mean_imp,
        'std': std_err
    })

    return result_imp, scr0.mean()

def auxFeatImpSFI(featNames, clf, trnsX, cont, scoring, cvGen):
    """
    è®¡ç®—å•ç‰¹å¾é‡è¦æ€§ï¼ˆSFIï¼‰ï¼šå¯¹æ¯ä¸ªç‰¹å¾å•ç‹¬è®­ç»ƒæ¨¡å‹ï¼Œè¯„ä¼°å…¶ OOS é¢„æµ‹èƒ½åŠ›ã€‚
    
    å‚æ•°:
        featNames: è¦è¯„ä¼°çš„ç‰¹å¾åç§°åˆ—è¡¨ï¼ˆæˆ–ç´¢å¼•ï¼‰
        clf: åˆ†ç±»å™¨ï¼ˆæ¯æ¬¡ä¼šç”¨å•ç‰¹å¾é‡æ–°è®­ç»ƒï¼‰
        trnsX: å®Œæ•´ç‰¹å¾ DataFrame
        cont: åŒ…å« 'bin'ï¼ˆæ ‡ç­¾ï¼‰å’Œ 'w'ï¼ˆæ ·æœ¬æƒé‡ï¼‰çš„ DataFrame
        scoring: è¯„åˆ†æŒ‡æ ‡ï¼ˆå¦‚ 'accuracy', 'neg_log_loss'ï¼‰
        cvGen: äº¤å‰éªŒè¯ç”Ÿæˆå™¨ï¼ˆå¦‚ PurgedKFoldï¼‰
    
    è¿”å›:
        pd.DataFrame: æ¯è¡Œä¸€ä¸ªç‰¹å¾ï¼ŒåŒ…å« 'mean'ï¼ˆå¹³å‡å¾—åˆ†ï¼‰å’Œ 'std'ï¼ˆæ ‡å‡†è¯¯ï¼‰
    """
    # åˆå§‹åŒ–ç»“æœ DataFrame
    imp = pd.DataFrame(index=featNames, columns=['mean', 'std'], dtype=float)

    from sklearn.base import clone

    

    for featName in featNames:
        clf_copy = clone(clf) # å…‹éš†åˆ†ç±»å™¨æ¨¡æ¿ï¼ˆä¿ç•™å‚æ•°ï¼Œä½†æœªè®­ç»ƒï¼‰
        # åªä½¿ç”¨å½“å‰ç‰¹å¾è¿›è¡Œäº¤å‰éªŒè¯è¯„åˆ†
        scores = cvScore(
            clf_copy,
            X=trnsX[[featName]],          # æ³¨æ„ï¼šåŒæ‹¬å·ä¿æŒ DataFrame ç»“æ„
            y=cont['bin'],
            sample_weight=cont['w'],
            scoring=scoring,
            cvGen=cvGen
        )
        
        # è®¡ç®—å‡å€¼å’Œæ ‡å‡†è¯¯ï¼ˆStandard Error = std / sqrt(n)ï¼‰
        mean_score = scores.mean()
        std_err = scores.std() / (len(scores) ** 0.5) if len(scores) > 1 else 0.0
        
        imp.loc[featName, 'mean'] = mean_score
        imp.loc[featName, 'std'] = std_err
        # print(f"ç‰¹å¾ {featName}: å¹³å‡åˆ†={mean_score:.4f}, æ ‡å‡†è¯¯={std_err:.4f}, åˆ†æ•°={scores}")
    return imp

def featImportance(trnsX, cont, clf=None, n_estimators=1000, cv=10, max_samples=1.,
                   numThreads=24, pctEmbargo=0.01, scoring='accuracy', method='MDI',
                   minWLeaf=0., random_state=38):
    """
    è®¡ç®—ç‰¹å¾é‡è¦æ€§ï¼ˆæ”¯æŒä¼ å…¥è‡ªå®šä¹‰åˆ†ç±»å™¨ï¼‰ï¼Œå¦‚æœæœªæä¾›åˆ†ç±»å™¨ï¼Œåˆ™åˆ›å»º AFML æ¨èçš„â€œæ— åâ€Bagging æ¨¡å‹


    å‚æ•°:
        trnsX (pd.DataFrame): ç‰¹å¾çŸ©é˜µ
        cont (pd.DataFrame): åŒ…å«æ ‡ç­¾å’Œæƒé‡çš„ DataFrameï¼Œå¿…é¡»åŒ…å« 'bin'ï¼ˆæ ‡ç­¾ï¼‰å’Œ 'w'ï¼ˆæ ·æœ¬æƒé‡ï¼‰åˆ—
        clf: å¯é€‰ï¼Œè‡ªå®šä¹‰åˆ†ç±»å™¨å®ä¾‹ï¼›è‹¥ä¸º Noneï¼Œåˆ™ä½¿ç”¨ AFML æ¨èçš„ BaggingClassifier
        n_estimators (int): Bagging ä¸­åŸºå­¦ä¹ å™¨çš„æ•°é‡
        cv (int): äº¤å‰éªŒè¯æŠ˜æ•°
        max_samples (float): Bagging ä¸­æ¯ä¸ªåŸºå­¦ä¹ å™¨ä½¿ç”¨çš„æ ·æœ¬æ¯”ä¾‹
        numThreads (int): å¹¶è¡Œçº¿ç¨‹æ•°
        pctEmbargo (float): ç¦æ­¢æœŸæ¯”ä¾‹
        scoring (str): è¯„åˆ†æ–¹æ³•ï¼Œæ”¯æŒ 'neg_log_loss' æˆ– 'accuracy'
        method (str): ç‰¹å¾é‡è¦æ€§è®¡ç®—æ–¹æ³•ï¼Œæ”¯æŒ 'MDI' æˆ– 'MDA'
        minWLeaf (float): å†³ç­–æ ‘å¶èŠ‚ç‚¹çš„æœ€å°æƒé‡åˆ†æ•°
        random_state (int): éšæœºç§å­
    """
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.ensemble import BaggingClassifier
    from sklearn.base import clone
    # from crossValidation import PurgedKFold

    # å¦‚æœæœªæä¾›åˆ†ç±»å™¨ï¼Œåˆ™åˆ›å»º AFML æ¨èçš„â€œæ— åâ€Bagging æ¨¡å‹
    if clf is None:
        tree = DecisionTreeClassifier(
            criterion='entropy',
            max_features=1,
            class_weight='balanced',
            min_weight_fraction_leaf=minWLeaf,
        )
        clf = BaggingClassifier(
            estimator=tree,
            n_estimators=n_estimators,
            max_features=1.0,
            max_samples=max_samples,
            oob_score=True,
            n_jobs=(-1 if numThreads > 1 else 1)
        )

    clf = clone(clf) #é‡ç½®ä¸ºæœªè®­ç»ƒçŠ¶æ€
     # === DEBUG START ===
    assert not hasattr(clf, 'estimators_'), "ERROR: clf å·²è®­ç»ƒï¼å¿…é¡»ä¼ æœªè®­ç»ƒæ¨¡æ¿"
    print("âœ… è¾“å…¥æ¨¡å‹æœªè®­ç»ƒ")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¸¸æ•°ç‰¹å¾
    const_cols = trnsX.columns[trnsX.nunique() <= 1]
    if len(const_cols) > 0:
        print(f"âš ï¸ è­¦å‘Šï¼šå­˜åœ¨å¸¸æ•°ç‰¹å¾ {list(const_cols)}ï¼Œå°†å¯¼è‡´ MDA=0")
    # === DEBUG END ===

    # å…‹éš†åˆ†ç±»å™¨æ¨¡æ¿ï¼ˆä¿ç•™å‚æ•°ï¼Œä½†æœªè®­ç»ƒï¼‰
    clf_oob = clone(clf)

    # æ‹Ÿåˆæ¨¡å‹ è¿™é‡Œä¼šå¯¼è‡´clfå‚æ•°è¢«è®­ç»ƒåˆ°ï¼Œä¸‹æ–¹è®¡ç®—ä¼ å…¥çš„æ¨¡å‹å¿…é¡»æ˜¯æœªè®­ç»ƒçš„,æ‰€ä»¥è¦å¦èµ·ä¸€ä¸ªåŒæ ·åˆå§‹é…ç½®çš„æ¨¡å‹
    fit = clf_oob.fit(X=trnsX, y=cont['bin'], sample_weight=cont['w'].values)
    oob = fit.oob_score_

    # å‡†å¤‡ CV ç”Ÿæˆå™¨
    cvGen = PurgedKFold(n_splits=cv, t1=cont['t1'], pctEmbargo=pctEmbargo)

    if method == 'MDI':
        imp = featImpMDI(fit, featNames=trnsX.columns)
        oos = cvScore(clf, X=trnsX, y=cont['bin'], sample_weight=cont['w'],scoring=scoring,t1=cont['t1'], cvGen=cvGen).mean()
    elif method == 'MDA':
        imp, oos = featImpMDA(clf, X=trnsX, y=cont['bin'], cv=cv,sample_weight=cont['w'], t1=cont['t1'],pctEmbargo=pctEmbargo,scoring=scoring)
    elif method == 'SFI':
        oos = cvScore(clf, X=trnsX, y=cont['bin'], sample_weight=cont['w'],scoring=scoring,t1=cont['t1'], cvGen=cvGen).mean()
        
        # SFI ä½¿ç”¨å¹¶è¡Œè®¡ç®—æ¯ä¸ªç‰¹å¾çš„é‡è¦æ€§
        clf.n_jobs = 1  # å°†å¹¶è¡Œäº¤ç»™ mpPandasObjï¼Œè€Œé sklearn
        #å¦‚æœæ²¡ä¼ å…¥clf ,auxFeatImpSFI ç»“æœè²Œä¼¼åªè·Ÿrandom_stateæœ‰å…³ï¼Œä½¿ç”¨å†³ç­–æ ‘å’Œbaggingé›†æˆçš„è¯ï¼Œæ•°æ®ä¸€ç‚¹ä½œç”¨éƒ½æ²¡æœ‰
        imp = mpPandasObj(
            func=auxFeatImpSFI,
            pdObj=('featNames', trnsX.columns),
            numThreads=numThreads,
            clf=clf,
            trnsX=trnsX,
            cont=cont,
            scoring=scoring,
            cvGen=cvGen
        )
    else:
        raise ValueError("method å¿…é¡»æ˜¯ 'MDI', 'MDA' æˆ– 'SFI'")
    print(method,' finishï¼')
    return imp, oob, oos


#éœ€è¦å¯¹æ­£äº¤å¤„ç†åçš„ç‰¹å¾è¿›è¡Œç‰¹å¾é‡è¦æ€§è®¡ç®—
# åˆå§‹åŒ–éšæœºæ£®æ—åˆ†ç±»å™¨
rf = RandomForestClassifier(
    n_estimators=50,        # æ ‘çš„æ•°é‡
    max_depth=10,             # æ ‘çš„æœ€å¤§æ·±åº¦
    # random_state=0,           # éšæœºç§å­
    # max_features=1,    # å…³é”®è®¾ç½®ï¼Œé˜²æ­¢é®è”½æ•ˆåº”  æ ‘æ¨¡å‹å¼ºåˆ¶max_features â‰¥ 1
    n_jobs=-1,                # ä½¿ç”¨æ‰€æœ‰ CPU æ ¸å¿ƒå¹¶è¡Œè®¡ç®—
    oob_score=True
)

MDI_imp,MDI_oob,MDI_oos=featImportance(trnsX_pca, cont, clf=rf, n_estimators=50, cv=3, max_samples=1.,numThreads=24, pctEmbargo=0.01, scoring='accuracy', method='MDI',minWLeaf=0., random_state=42)
MDA_imp,MDA_oob,MDA_oos=featImportance(trnsX_pca, cont, clf=rf, n_estimators=50, cv=10, max_samples=1.,numThreads=24, pctEmbargo=0.01, scoring='accuracy', method='MDA',minWLeaf=0., random_state=42)
SFI_imp,SFI_oob,SFI_oos=featImportance(trnsX_pca, cont, clf=rf, n_estimators=50, cv=3, max_samples=1., numThreads=24, pctEmbargo=0.01, scoring='accuracy', method='SFI',minWLeaf=0., random_state=42)
# print(MDA_oob,MDA_oos) 
# åˆå¹¶ä¸‰ç§æ–¹æ³•çš„ç»“æœï¼ˆç¡®ä¿ç´¢å¼•å¯¹é½ï¼‰
imp_df = pd.DataFrame({
    'MDI': MDI_imp['mean'] if isinstance(MDI_imp, pd.DataFrame) else MDI_imp,
    'MDA': MDA_imp['mean'] if isinstance(MDA_imp, pd.DataFrame) else MDA_imp,
    'SFI': SFI_imp['mean'] if isinstance(SFI_imp, pd.DataFrame) else SFI_imp
})
imp_df_sorted = imp_df.sort_values(by='MDA', ascending=False)
print(imp_df_sorted.round(4))
print("\nğŸ“ˆ OOS æ€§èƒ½:")
print(f"MDI OOS: {MDI_oos:.4f} | MDA OOS: {MDA_oos:.4f} | SFI OOS: {SFI_oos:.4f}")


#ä½¿ç”¨æœªæ­£äº¤çš„ç‰¹å¾è®­ç»ƒï¼Œå¯¹æ¯”å™ªéŸ³ç‰¹å¾éƒ½å¤„äºä½å€¼ï¼Œmdaæ•ˆæœä¸é”™ã€‚SFIå·®è·éƒ½ä¸å¤§ï¼Œæœ‰éš¾éš¾åŒºåˆ†
MDI_imp2,MDI_oob2,MDI_oos2=featImportance(trnsX, cont, clf=rf, n_estimators=50, cv=3, max_samples=1.,numThreads=24, pctEmbargo=0.01, scoring='accuracy', method='MDI',minWLeaf=0., random_state=42)
MDA_imp2,MDA_oob2,MDA_oos2=featImportance(trnsX, cont, clf=rf, n_estimators=50, cv=3, max_samples=1.,numThreads=24, pctEmbargo=0.01, scoring='accuracy', method='MDA',minWLeaf=0., random_state=42)
SFI_imp2,SFI_oob2,SFI_oos2=featImportance(trnsX, cont, clf=rf, n_estimators=50, cv=3, max_samples=1., numThreads=24, pctEmbargo=0.01, scoring='accuracy', method='SFI',minWLeaf=0., random_state=42)

# åˆå¹¶ä¸‰ç§æ–¹æ³•çš„ç»“æœï¼ˆç¡®ä¿ç´¢å¼•å¯¹é½ï¼‰
imp_df2 = pd.DataFrame({
    'MDI': MDI_imp2['mean'] if isinstance(MDI_imp2, pd.DataFrame) else MDI_imp2,
    'MDA': MDA_imp2['mean'] if isinstance(MDA_imp2, pd.DataFrame) else MDA_imp2,
    'SFI': SFI_imp2['mean'] if isinstance(SFI_imp2, pd.DataFrame) else SFI_imp2
})
imp_df_sorted2 = imp_df2.sort_values(by='MDA', ascending=False)
print(imp_df_sorted2.round(4))
print("\nğŸ“ˆ OOS æ€§èƒ½:")
print(f"MDI OOS: {MDI_oos2:.4f} | MDA OOS: {MDA_oos2:.4f} | SFI OOS: {SFI_oos2:.4f}")

#ç»“è®ºä»¥mdaä¸ºä¸»ï¼Œsfiä¸ºè¾…ç­›é€‰ç‰¹å¾é‡è¦æ€§ã€‚


'''
 8.2 From exercise 1, generate a new dataset (Ìˆ X,y), where Ìˆ X is a feature union of X
 and Ì‡ X.
 (a) Compute MDI, MDA, and SFI feature importance on (Ìˆ X,y), where the base
 estimator is RF.
 (b) Do the three methods agree on the important features? Why?
'''

import random

# è®¾ç½®éšæœºç§å­ï¼ˆå¯é€‰ï¼Œç”¨äºå¤ç°ï¼‰
random.seed(42)

# ä» trnsX ä¸­éšæœºæŠ½å– n1 ä¸ªç‰¹å¾
n1 = 10  # ä¾‹å¦‚æŠ½å– 10 ä¸ªåŸå§‹ç‰¹å¾
selected_from_trnsX = random.sample(list(trnsX.columns), k=min(n1, trnsX.shape[1]))

# ä» trnsX_pca ä¸­éšæœºæŠ½å– n2 ä¸ªç‰¹å¾
n2 = 10   # ä¾‹å¦‚æŠ½å– 5 ä¸ª PCA ç‰¹å¾
selected_from_trnsX_pca = random.sample(list(trnsX_pca.columns), k=min(n2, trnsX_pca.shape[1]))

# æŒ‰é€‰å®šçš„åˆ—æå–å­é›†ï¼Œå¹¶æ¨ªå‘æ‹¼æ¥ï¼ˆç¡®ä¿è¡Œç´¢å¼•å¯¹é½ï¼‰
trnsX_union = pd.concat([
    trnsX[selected_from_trnsX],
    trnsX_pca[selected_from_trnsX_pca]
], axis=1)

print("æ–°æ•°æ®é›† trnsX_union çš„å½¢çŠ¶:", trnsX_union.shape)
print("åŒ…å«çš„åˆ—:", list(trnsX_union.columns))

#ä½¿ç”¨åˆæˆçš„ç‰¹å¾è®­ç»ƒ
MDI_imp3,MDI_oob3,MDI_oos3=featImportance(trnsX_union, cont, clf=rf, n_estimators=50, cv=3, max_samples=1.,numThreads=24, pctEmbargo=0.01, scoring='accuracy', method='MDI',minWLeaf=0., random_state=42)
MDA_imp3,MDA_oob3,MDA_oos3=featImportance(trnsX_union, cont, clf=rf, n_estimators=50, cv=3, max_samples=1.,numThreads=24, pctEmbargo=0.01, scoring='accuracy', method='MDA',minWLeaf=0., random_state=42)
SFI_imp3,SFI_oob3,SFI_oos3=featImportance(trnsX_union, cont, clf=rf, n_estimators=50, cv=3, max_samples=1., numThreads=24, pctEmbargo=0.01, scoring='accuracy', method='SFI',minWLeaf=0., random_state=42)

# åˆå¹¶ä¸‰ç§æ–¹æ³•çš„ç»“æœï¼ˆç¡®ä¿ç´¢å¼•å¯¹é½ï¼‰
imp_df3 = pd.DataFrame({
    'MDI': MDI_imp3['mean'] if isinstance(MDI_imp3, pd.DataFrame) else MDI_imp3,
    'MDA': MDA_imp3['mean'] if isinstance(MDA_imp3, pd.DataFrame) else MDA_imp3,
    'SFI': SFI_imp3['mean'] if isinstance(SFI_imp3, pd.DataFrame) else SFI_imp3
})
imp_df_sorted3 = imp_df3.sort_values(by='MDA', ascending=False)
print(imp_df_sorted3.round(4))
print("\nğŸ“ˆ OOS æ€§èƒ½:")
print(f"MDI OOS: {MDI_oos3:.4f} | MDA OOS: {MDA_oos3:.4f} | SFI OOS: {SFI_oos3:.4f}")


#ç»“æœï¼š1.MDAå’ŒMDIéƒ½ä¼šæœ‰è·³è·ƒä¸‹è·Œçš„ç‰¹å¾ï¼Œå€’åºæ’åˆ—ï¼Œå·®ä¸€è¡Œçš„å·®åˆ«å¤§çº¦æœ‰3å€è¿™æ ·ï¼Œåæ­£å·®äº†å¥½å‡ å€çš„ã€‚åªä½¿ç”¨è·³è·ƒä¸‹é™å‰çš„æ•°æ®å³å¯ã€‚
# 2.ç»è¿‡ä¸¤ä¸ªæ¨¡å‹çš„ç»“æœå¯¹æ¯”ï¼Œåœ¨æ··åˆæ¨¡å‹MDAä¸­è¡¨ç°è¾ƒå¥½çš„ï¼Œåœ¨åŸæ¨¡å‹ä¹Ÿæ ‡å‡†è¾ƒå¥½ã€‚ä½†æ˜¯æœ‰MDAè¯¯æ€çš„ç‰¹å¾ï¼Œåœ¨MDIä¸­æ²¡æœ‰ï¼Œä½†æ˜¯é”™æ€çš„éƒ½æ˜¯å†—ä½™å­—æ®µã€‚ç”¨MDAæ ¡å‡†MDIï¼ˆä¾‹å¦‚ï¼šMDIæ’åå‰10ä½†MDAä¸æ˜¾è‘— â†’ åˆ é™¤ï¼‰ï¼Œè¿™æ ·æ›´ä¸¥æ ¼ï¼Œæ‰¾åˆ°çš„é‡è¦æ€§æ›´æœ‰æ•ˆã€‚



'''
 8.3 Take the results from exercise 2:
 (a) Drop the most important features according to each method, resulting in a
 features matrix âƒ› X.
 (b) Compute MDI, MDA, and SFI feature importance on (âƒ› X,y), where the base
 estimator is RF.
 (c) Do you appreciate significant changes in the rankings of important features,
 relative to the results from exercise 2?
'''

#å‡è®¾æˆ‘ä»¬ä»ä¸Šä¸€æ­¥çš„ç»“æœä¸­é€‰æ‹©æ¯ç§æ–¹æ³•å¾—åˆ†æœ€é«˜çš„ç‰¹å¾è¿›è¡Œåˆ é™¤

selected_features = imp_df_sorted3.MDA.nlargest(1).index.tolist() + imp_df_sorted3.MDI.nlargest(1).index.tolist() + imp_df_sorted3.SFI.nlargest(1).index.tolist()

# ä» trnsX_union ä¸­åˆ é™¤é€‰ä¸­çš„ç‰¹å¾
trnsX_union_dropped = trnsX_union.drop(columns=selected_features)

print("åˆ é™¤åçš„ trnsX_union å½¢çŠ¶:", trnsX_union_dropped.shape)
print("åˆ é™¤çš„ç‰¹å¾:", selected_features)

#è®¡ç®—å¾—åˆ†
MDI_imp4,MDI_oob4,MDI_oos4=featImportance(trnsX_union_dropped, cont, clf=rf, n_estimators=50, cv=3, max_samples=1.,numThreads=24, pctEmbargo=0.01, scoring='accuracy', method='MDI',minWLeaf=0., random_state=42)
MDA_imp4,MDA_oob4,MDA_oos4=featImportance(trnsX_union_dropped, cont, clf=rf, n_estimators=50, cv=3, max_samples=1.,numThreads=24, pctEmbargo=0.01, scoring='accuracy', method='MDA',minWLeaf=0., random_state=42)
SFI_imp4,SFI_oob4,SFI_oos4=featImportance(trnsX_union_dropped, cont, clf=rf, n_estimators=50, cv=3, max_samples=1., numThreads=24, pctEmbargo=0.01, scoring='accuracy', method='SFI',minWLeaf=0., random_state=42)

# åˆå¹¶ä¸‰ç§æ–¹æ³•çš„ç»“æœï¼ˆç¡®ä¿ç´¢å¼•å¯¹é½ï¼‰
imp_df4 = pd.DataFrame({
    'MDI': MDI_imp4['mean'] if isinstance(MDI_imp4, pd.DataFrame) else MDI_imp4,
    'MDA': MDA_imp4['mean'] if isinstance(MDA_imp4, pd.DataFrame) else MDA_imp4,
    'SFI': SFI_imp4['mean'] if isinstance(SFI_imp4, pd.DataFrame) else SFI_imp4
})
imp_df_sorted4 = imp_df4.sort_values(by='MDA', ascending=False)
print(imp_df_sorted4.round(4))
print("\nğŸ“ˆ OOS æ€§èƒ½:")
print(f"MDI OOS: {MDI_oos4:.4f} | MDA OOS: {MDA_oos4:.4f} | SFI OOS: {SFI_oos4:.4f}")


#c
#ç‰¹å¾R_7åœ¨å®éªŒ2é‡Œé‡è¦æ€§æ¥è¿‘äº0ï¼Œä½†æ˜¯åœ¨å®éªŒ3é‡Œé‡è¦æ€§ä¸€è·ƒæˆä¸ºç¬¬ä¸€ä½ã€‚å…¶ä½™è·³è·ƒå‰çš„ç‰¹å¾ä»æ—§ä¿ç•™åœ¨è·³è·ƒå‰çš„ä½ç½®ï¼Œåªæ˜¯é¡ºåºç•¥æœ‰æ”¹å˜ã€‚
#å»æ‰é‡è¦ç‰¹å¾å¯ä»¥å‡å°‘æ›¿ä»£æ•ˆåº”ï¼Œä½†æ˜¯ä¼šå¯¼è‡´oobï¼Œoosé™ä½ï¼Œæ•´ä½“çš„æ¨¡å‹æ€§èƒ½ä¹Ÿä¼šä¸‹é™ã€‚æ‰€ä»¥æ›´å¤šçš„æ˜¯è€ƒè™‘PCAæ–¹æ³•æ¥é™ä½æ›¿ä»£æ•ˆç›Šï¼Œä»ç»“æœä¸Šçœ‹ï¼Œæ•ˆæœå·®ä¸å¤šï¼Œè€Œä¸éœ€è¦äººå·¥ç­›é€‰å“ªäº›æ˜¯é‡è¦ç‰¹å¾ã€‚


'''
 8.4 Using the code presented in Section 8.6:
 (a) Generate a dataset (X,y) of 1E6 observations, where 5 features are informa
tive, 5 are redundant and 10 are noise.
 (b) Split (X,y) into 10 datasets {(Xi,yi)}i=1,â€¦,10, each of 1E5 observations.
 (c) Compute the parallelized feature importance (Section 8.5), on each of the 10
 datasets, {(Xi, yi)}i=1,â€¦,10.
 (d) Compute the stacked feature importance on the combined dataset (X,y).
 (e) What causes the discrepancy between the two? Which one is more reliable?
 '''

#a
trnsX,cont=getTestData(n_features=20, n_informative=5, n_redundant=5, n_samples=1000000)

#b
# å°†æ•°æ®é›†æ‹†åˆ†ä¸º 10 ä¸ªå­é›†
trnsX_subsets = np.array_split(trnsX, 10)
cont_subsets = np.array_split(cont, 10)
#å°†cont_subsetsçš„wè¿™åˆ—çš„æƒé‡ä¿®æ­£ä¸º1/len(cont_subset)
cont_subsets = [cont_subset.assign(w=1/len(cont_subset)) for cont_subset in cont_subsets]
# MDA_imp_subset1, _oob, _oos = featImportance(trnsX_subsets[1], cont_subsets[1],n_estimators=100, cv=10, pctEmbargo=0.01, scoring='accuracy', method='MDA')

#c å¹¶è¡Œè®¡ç®—æ¯ä¸ªå­é›†çš„ç‰¹å¾é‡è¦æ€§ åªä½¿ç”¨MDA 
MDA_imp_subsets = []
MDA_imp_oobs=[]
MDA_imp_ooss=[]
for trnsX_subset, cont_subset in zip(trnsX_subsets, cont_subsets):
    MDA_imp_subset, MDA_imp_oob, MDA_imp_oos = featImportance(trnsX_subset, cont_subset, clf=None,n_estimators=100, cv=10, pctEmbargo=0.01, scoring='neg_log_loss', method='MDA')
    MDA_imp_subsets.append(MDA_imp_subset)
    MDA_imp_oobs.append(MDA_imp_oob)
    MDA_imp_ooss.append(MDA_imp_oos)




# è®¡ç®—æ¯ä¸ªå­é›†çš„é‡è¦æ€§å‡å€¼
MDA_imp_mean = pd.concat(MDA_imp_subsets).groupby(level=0).mean()
MDA_imp_oobs_mean = np.mean(MDA_imp_oobs)
MDA_imp_ooss = np.mean(MDA_imp_ooss)
print(MDA_imp_mean,'oob:',MDA_imp_oobs_mean,'oos:',MDA_imp_ooss)

#d åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šè®¡ç®—å †å çš„ç‰¹å¾é‡è¦æ€§
MDA_imp_full, MDA_imp_oob_full, MDA_imp_oos_full = featImportance(trnsX, cont,  clf=None,n_estimators=100, cv=10, pctEmbargo=0.01, scoring='neg_log_loss', method='MDA')
print(MDA_imp_full,'oob:',MDA_imp_oob_full,'oos:',MDA_imp_oos_full)
#e åŒºåˆ«
#ç‰¹å¾é‡è¦æ€§åœ¨å †å å’Œå¹¶è¡Œä¸Šéƒ½èƒ½å¤Ÿå°†I,R,Nè¿›è¡ŒåŒºåˆ†ï¼Œirä¸Næœ‰æ˜æ˜¾çš„åŒºåˆ«ã€‚

##å †å oob: 0.814751 oos: -0.4946819712192533
#å¹¶è¡Œ oob: 0.9005750000000001 oos: -0.3266109903063382
#åœ¨OOBå’ŒOOSä¸Šå¹¶è¡Œçš„æ•ˆæœæ˜æ˜¾åå¤§ï¼Œæ˜¯æœ‰åçš„ï¼Œæ‰€ä»¥é¢„æµ‹èƒ½åŠ›ä¸Šåº”è¯¥ä½¿ç”¨å †å æ›´åæ˜ çœŸå®æ•ˆæœ
#æ­¤å¤–ï¼Œå †å çš„é²æ£’æ€§æ›´å¥½ï¼Œå› ä¸ºè®­ç»ƒçš„æ•°æ®é›†æ›´å¤§ï¼Œæ›´å°‘å—æç«¯å€¼å½±å“ã€‚ç¼ºç‚¹æ˜¯éœ€è¦çš„ç¡¬ä»¶æ¡ä»¶è¦æ›´é«˜

'''
 8.5 Repeat all MDI calculations from exercises 1â€“4, but this time allow for masking
 effects. That means, do not set max_features=int(1) in Snippet 8.2. How do
 results differ as a consequence of this change? Why?
'''

#å¤ªè´¹æ—¶äº†ï¼Œå°±ä¸é‡è·‘äº†
#é®è”½æ•ˆåº”å®ä¾‹å¦‚ä¸‹ã€‚æ‰€ä»¥ä¸€èˆ¬æƒ…å†µä¸‹è¦ä½¿ç”¨featImportance(trnsX, cont,  clf=Noneï¼‰é‡Œé¢çš„clf=Noneï¼Œè¿›è¡Œå‰”é™¤é®è”½æ•ˆåº”
'''
ç‰¹å¾		æ ‡å‡† RFï¼ˆæœ‰æ©è”½ï¼‰MDA	max_features=1ï¼ˆæ— æ©è”½ï¼‰MDA
MA5		0.12	                         0.11
MA10	0.01ï¼ˆè¢«æ©è”½ï¼‰	                0.10
'''






'''
ç¬¬å…«ç« æ€»ç»“ï¼š
1.è¦è®¤è¯†åˆ°å›æµ‹æ˜¯éªŒè¯æ‰‹æ®µè€Œä¸æ˜¯æ¢ç´¢å‘ç°çœŸç†çš„æ–¹æ³•ï¼Œç‰¹å¾é‡è¦æ€§æ‰æ˜¯æ¢ç´¢çš„å·¥å…·ã€‚
2.å¦‚ä½•è¯„ä¼°ç‰¹å¾é‡è¦æ€§ï¼ŸPCAæ­£äº¤é™ç»´+MDIï¼ˆéšæœºæ£®æ—çš„feature_importï¼Œä¼šæœ‰æ›¿ä»£æ•ˆåº”ï¼Œæ‰€ä»¥å¿…é¡»ä½¿ç”¨pcaï¼‰/MDAï¼ˆå¹³å‡å‡†ç¡®æ€§ä¸‹é™,ä½¿ç”¨è¢‹å¤–æ ·æœ¬ï¼Œä¸é™äºéšæœºæ£®æ—æ¨¡å‹ï¼Œæ›´æ³›ç”¨ï¼Œä¹Ÿä¼šæœ‰æ›¿ä»£æ•ˆåº”ï¼‰/SFIï¼ˆå•ä¸ªç‰¹å¾çš„é‡è¦æ€§ï¼Œå¯èƒ½ä¼šä¸¢å¤±ç‰¹å¾äº¤äº’æ•ˆåº”ï¼‰
3.ä¸ºäº†é¿å…ä¹¦é‡Œåˆ—å‡ºçš„éƒ¨åˆ†ç¼ºé™·ï¼ˆsection 8.2  8.3 é®è”½æ•ˆåº”ï¼‰ï¼Œéœ€è¦ä½¿ç”¨æ”¹è¿›åçš„æ¨¡å‹ï¼ˆmax_features=1,ä¸”æ˜¯å†³ç­–æ ‘åˆ†ç±»å™¨ï¼‰+æ”¹è¿›åçš„ç‰¹å¾é‡è¦æ€§è®¡ç®—å‡½æ•°ï¼Œè€Œä¸æ˜¯ä½¿ç”¨sklearnè‡ªå¸¦çš„ã€‚ æ­¤å¤–æ•°æ®çš„å¤„ç†ä¹Ÿéœ€è¦æ‰§è¡Œæ¸…æ¥šå’Œç¦è¿ï¼Œäº‹ä»¶ç­‰å†…å®¹ï¼Œè¿™äº›å†…å®¹æ˜¯ç´¯ç§¯ä¸Šå»çš„ï¼Œè€Œä¸æ˜¯å‰²è£‚å­˜åœ¨çš„ã€‚â€”â€”â€”â€”â€”â€”æ‰€ä»¥è®¡ç®—MDAæ—¶ï¼Œå°±ä¸è¦ä½¿ç”¨ä¼ å…¥çš„clfäº†ï¼Œä½¿ç”¨featImportanceé‡Œé¢è®¾ç½®çš„é›†æˆå†³ç­–æ ‘æ¨¡å‹!!!!! è¿™ä¸€æ­¥å¾ˆå…³é”®
4.å¯¹ç¬¬ä¸‰ç‚¹çš„è¡ç”Ÿï¼šç”±äºä½¿ç”¨max_features=1çš„æ ‘æ¨¡å‹ä¼šå¯¼è‡´æ‰€æœ‰ç‰¹å¾çš„SFIéƒ½ç›¸åŒï¼Œè€ŒMDAï¼ŒSFIæ˜¯å¯ä»¥åº”ç”¨äºæ‰€æœ‰åˆ†ç±»å™¨çš„ï¼Œæ‰€ä»¥ä¼ å…¥ä¸€ä¸ªclfæ¨¡å‹è¿›å»ï¼Œé˜²æ­¢SFIè®¡ç®—å¤±æ•ˆ
5.MDIåªèƒ½ç”¨äºéšæœºæ£®æ—æ¨¡å‹ï¼Œç”±äºé®è”½æ•ˆåº”ï¼Œæ›¿ä»£æ•ˆç›Šç­‰ï¼Œå¯¼è‡´åå·®ï¼Œæ‰€ä»¥ä»…é€‚ç”¨äºåˆç­›ï¼Œåº”è¯¥ä»¥MDAä½œä¸ºåŸºå‡†ï¼ŒSFIè¿›è¡Œè¾…åŠ©ä¸ºä½³ã€‚
6.MDAå’ŒMDIéƒ½ä¼šæœ‰è·³è·ƒä¸‹è·Œçš„ç‰¹å¾ï¼Œå€’åºæ’åˆ—ï¼Œå·®ä¸€è¡Œçš„å·®åˆ«å¤§çº¦æœ‰3å€è¿™æ ·ï¼Œåæ­£å·®äº†å¥½å‡ å€çš„ã€‚åªä½¿ç”¨è·³è·ƒä¸‹é™å‰çš„æ•°æ®å³å¯ã€‚
7.ç»è¿‡ä¸¤ä¸ªæ¨¡å‹ï¼ˆtrnsX_unionï¼‰çš„ç»“æœå¯¹æ¯”ï¼Œåœ¨æ··åˆæ¨¡å‹MDAä¸­è¡¨ç°è¾ƒå¥½çš„ï¼Œåœ¨åŸæ¨¡å‹ä¹Ÿæ ‡å‡†è¾ƒå¥½ã€‚ä½†æ˜¯æœ‰MDAè¯¯æ€çš„ç‰¹å¾ï¼Œåœ¨MDIä¸­æ²¡æœ‰ï¼Œä½†æ˜¯é”™æ€çš„éƒ½æ˜¯å†—ä½™å­—æ®µã€‚ç”¨MDAæ ¡å‡†MDIï¼ˆä¾‹å¦‚ï¼šMDIæ’åå‰10ä½†MDAä¸æ˜¾è‘— â†’ åˆ é™¤ï¼‰ï¼Œè¿™æ ·æ›´ä¸¥æ ¼ï¼Œæ‰¾åˆ°çš„é‡è¦æ€§æ›´æœ‰æ•ˆã€‚
8.åœ¨MDAçš„è®¡ç®—ä¸­ï¼Œå¦‚æœæ¨¡å‹çš„å‡†ç¡®ç‡è¾¾åˆ°95%ç­‰ä»¥ä¸Šï¼Œä¼šå¯¼è‡´MDAæ— æ³•è®¡ç®—ï¼Œç‰¹å¾é‡è¦æ€§ï¼Œæ‰€æœ‰çš„ç‰¹å¾é‡è¦æ€§å‡å€¼å’Œæ–¹å·®éƒ½ä¼šåªå¾—åˆ°0ã€‚è¿™ä¸ªæ—¶å€™è¦æ”¹ç”¨neg_log_lossä½œä¸ºscoringæŒ‡æ ‡ã€‚å½“ç„¶ï¼Œåœ¨å®é™…æ•°æ®ä¸­ï¼Œè¾¾åˆ°70%ä»¥ä¸Šå‡†ç¡®ç‡éƒ½éå¸¸å°‘è§äº†ã€‚
'''

#%%

#ç¬¬ä¹ç«  äº¤å‰éªŒè¯çš„è¶…å‚ä¼˜åŒ–
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV, StratifiedKFold,RandomizedSearchCV
from sklearn.ensemble import BaggingClassifier

'''
 9.1 Using the function getTestData from Chapter 8, form a synthetic dataset of
 10,000 observations with 10 features, where 5 are informative and 5 are noise.
 (a) Use GridSearchCV on 10-fold CV to find the C, gamma optimal hyper
parameters on a SVC with RBF kernel, where param_grid={'C':[1E
2,1E-1,1,10,100],'gamma':[1E-2,1E-1,1,10,100]} and the scor
ing function is neg_log_loss.
 (b) How many nodes are there in the grid?
 (c) How many fits did it take to find the optimal solution?
 (d) How long did it take to find this solution?
 (e) How can you access the optimal result?
 (f) What is the CV score of the optimal parameter combination?
 (g) How can you pass sample weights to the SVC?
'''
#a
class MyPipeline(Pipeline):
    def fit(self,X,y,sample_weight=None,**fit_params):
        if sample_weight is not None:
            fit_params[self.steps[-1][0]+'__sample_weight']=sample_weight
        return super(MyPipeline,self).fit(X,y,**fit_params)

#é€‚åº”AFMLä½“ç³»çš„è¶…å‚ä¼˜åŒ–
def clfHyperFit(feat, lbl, t1, pipe_clf, param_grid, cv=3, bagging=[0, None, 1.], 
                rndSearchIter=0, n_jobs=-1, pctEmbargo=0.01, **fit_params):
    '''
    æ³¨æ„ï¼š
    1.æ ·æœ¬æƒé‡å¿…é¡»è¦åœ¨**fit_paramsé‡Œé¢ä¼ å…¥ï¼Œè€Œä¸”è¦ä»¥{Pipeline[-1][0]}__sample_weightçš„å½¢å¼ä¼ å…¥,Pipeline[-1][0]å°±æ˜¯æ„é€ çš„Pipelineæœ€ç»ˆçš„åˆ†ç±»å™¨åç§°ã€‚   â€”â€”â€”â€”è¿™æ ·æ ·æœ¬æƒé‡å·²ç»è¢«æ­£ç¡®çš„ä¼ å…¥äº†ç½‘æ ¼æœç´¢å’Œæ¨¡å‹é‡Œ


    '''
    # 1) è®¾ç½®è¯„åˆ†æ ‡å‡†
    # if set(lbl.values) == {0, 1}: 
    #     scoring = 'f1'  # F1åˆ†æ•°ç”¨äºäºŒåˆ†ç±»é—®é¢˜
    # else:
    #     scoring = 'neg_log_loss'  # å¯¹æ•°æŸå¤±ï¼Œç”¨äºå¤šåˆ†ç±»é—®é¢˜
    scoring = 'neg_log_loss'  # å¯¹9.1çš„ä¸´æ—¶ä½¿ç”¨
    # scoring = 'accuracy'

    inner_cv = PurgedKFold(n_splits=cv, t1=t1, pctEmbargo=pctEmbargo)  

    # 3) è¶…å‚æ•°æœç´¢
    if rndSearchIter == 0:
        # ä½¿ç”¨ç½‘æ ¼æœç´¢
        gs = GridSearchCV(estimator=pipe_clf, param_grid=param_grid, scoring=scoring, 
                          cv=inner_cv, n_jobs=n_jobs, ) #iid=False å·²å¼ƒç”¨ï¼Œæ–°ç‰ˆçš„sklearné»˜è®¤å°±æ˜¯False
    else:
        # ä½¿ç”¨éšæœºæœç´¢
        gs = RandomizedSearchCV(estimator=pipe_clf, param_distributions=param_grid, 
                                scoring=scoring, cv=inner_cv, n_jobs=n_jobs, 
                                n_iter=rndSearchIter)

    # è®­ç»ƒå¹¶è·å¾—æœ€ä½³æ¨¡å‹
    gs = gs.fit(feat, lbl, **fit_params)  #æ ·æœ¬æƒé‡é€šè¿‡{Pipeline[-1][0]}__sample_weight åœ¨å‚æ•°é‡Œä¼ å…¥äº†ï¼Œ å› ä¸ºæ˜¯ä½¿ç”¨åŒä¸‹åˆ’çº¿è¿›è¡Œä¼ å…¥çš„æ ·æœ¬æƒé‡å‚æ•°ï¼Œpipelineèƒ½å¤Ÿè°ƒç”¨è¿™ä¸ªå‚æ•°

    best_estimator = gs.best_estimator_ 
    print("æœ€ä¼˜ CV å¾—åˆ†:", gs.best_score_)
    # 4) å¦‚æœéœ€è¦ï¼Œä½¿ç”¨ Bagging é›†æˆæ–¹æ³•
    if bagging[1] > 0:
        gs = BaggingClassifier(base_estimator=MyPipeline(gs.steps), 
                               n_estimators=int(bagging[0]), max_samples=float(bagging[1]), 
                               max_features=float(bagging[2]), n_jobs=n_jobs)
        
        # åœ¨ Bagging ä¸­è®­ç»ƒæ¨¡å‹
        gs = gs.fit(feat, lbl, sample_weight=fit_params[gs.base_estimator.steps[-1][0]+'__sample_weight'])
        final_model  = Pipeline([('bag', gs)])
    else:
        final_model = best_estimator
    # 5) è¿”å›æœ€ç»ˆçš„æ¨¡å‹,æœç´¢çš„ç»“æœ
    return final_model,gs

trnsX,cont=getTestData(n_features=10,n_informative=5,n_redundant=0,n_samples=10000)

# 2. æ„å»º Pipelineï¼šå¿…é¡»åŒ…å«æ ‡å‡†åŒ–ï¼ˆSVC å¯¹å°ºåº¦æåº¦æ•æ„Ÿï¼ï¼‰
pipe = Pipeline([
    ('scaler', StandardScaler()),          
    ('svc', SVC(kernel='rbf', probability=True, random_state=42))  # probability=True æ‰èƒ½ç”¨ log_loss
])

# 3. å®šä¹‰è¶…å‚æ•°ç½‘æ ¼
param_grid = {
    'svc__C': [1e-2, 1e-1, 1, 10, 100],      # æ³¨æ„ï¼šä½ å†™çš„æ˜¯ 1E2=100ï¼Œä½†é€šå¸¸ä»æ›´å°å¼€å§‹
    'svc__gamma': [1e-2, 1e-1, 1, 10, 100]
}

#ç½‘æ ¼æœç´¢
print('å¼€å§‹æ—¶é—´ï¼š',dt.datetime.now() )
svc_model_GS,svc_GS_gs=clfHyperFit(feat=trnsX, lbl=cont['bin'], t1=cont['t1'], pipe_clf=pipe, param_grid=param_grid, cv=10, bagging=[0, 0, 1.], rndSearchIter=0,pctEmbargo=0.01, **{'svc__sample_weight':cont['w']})
print('ç»“æŸæ—¶é—´ï¼š',dt.datetime.now() )


#b ç½‘æ ¼æœç´¢çš„èŠ‚ç‚¹æ•°é‡
n_nodes = len(param_grid['svc__C']) * len(param_grid['svc__gamma'])
 
#c ç½‘æ ¼æœç´¢çš„æ‹Ÿåˆæ¬¡æ•°  èŠ‚ç‚¹æ•°é‡*cv
n_fits = n_nodes * 10

#d èŠ±è´¹æ—¶é—´ æ¯è®°å½•ï¼Œå¤§çº¦1å°æ—¶å§

#e è®¿é—®æœ€ä¼˜ç»“æœ
best_params =svc_model_GS.named_steps['svc'].get_params() 

#f æœ€ä¼˜å‚æ•°ç»„åˆçš„CVåˆ†æ•° å‡½æ•°æ²¡æœ‰è¿”å›ï¼Ÿ
best_score = svc_model_GS.best_score_



'''
 9.2 Using the same dataset from exercise 1,
 (a) Use RandomizedSearchCV on 10-fold CV to find the C,
 gamma optimal hyper-parameters on an SVC with RBF kernel,
 where
 param_distributions={'C':logUniform(a=1E-2,b=
 1E2),'gamma':logUniform(a=1E-2,b=1E2)},n_iter=25 and
 neg_log_loss is the scoring function.
 (b) How long did it take to find this solution?
 (c) Is the optimal parameter combination similar to the one found in exercise 1?
 (d) What is the CV score of the optimal parameter combination? How does it
 compare to the CV score from exercise 1?
 '''

#a
from scipy.stats import loguniform
import numpy as np,pandas as pd,matplotlib.pyplot as mpl
from scipy.stats import rv_continuous,kstest

# # å®˜æ–¹
# dist1 = loguniform(0.01, 100)

#å·®å¼‚ä¸å¤§ï¼Œç›´æ¥ç”¨å®˜æ–¹çš„å°±è¡Œï¼Œæ›´æ–¹ä¾¿
# class logUniform_gen(rv_continuous):
# # random numbers log-uniformly distributed between 1 and e
#     def _cdf(self,x):
#         return np.log(x/self.a)/np.log(self.b/self.a)
# def logUniform(a=1,b=np.exp(1)):
#     return logUniform_gen(a=a,b=b,name='logUniform')

# dist2 = logUniform_gen(a=0.01, b=100, name='logUniform')



# æ„å»º Pipelineï¼šå¿…é¡»åŒ…å«æ ‡å‡†åŒ–ï¼ˆSVC å¯¹å°ºåº¦æåº¦æ•æ„Ÿï¼ï¼‰
pipe = Pipeline([
    ('scaler', StandardScaler()),          
    ('svc', SVC(kernel='rbf', probability=True, random_state=42))  # probability=True æ‰èƒ½ç”¨ log_loss
])

#å®šä¹‰è¶…å‚æ•°ç½‘æ ¼
param_grid = {
    'svc__C': loguniform(a=1E-2,b=1E2),    
    'svc__gamma': loguniform(a=1E-2,b=1E2)
}


print('å¼€å§‹æ—¶é—´ï¼š',dt.datetime.now() )
svc_model_RS,svc_RS_gs=clfHyperFit(feat=trnsX, lbl=cont['bin'], t1=cont['t1'], pipe_clf=pipe, param_grid=param_grid, cv=10, bagging=[0, 0, 1.], rndSearchIter=25,pctEmbargo=0.01, **{'svc__sample_weight':cont['w']})
print('ç»“æŸæ—¶é—´',dt.datetime.now() )

#b èŠ±è´¹æ—¶é—´åªæœ‰25åˆ†é’Ÿï¼Œæ˜æ˜¾å˜å¿«äº†

#c 'C': 0.20446553804452852,'gamma': 0.1902583355975827  åœ¨ä¸Šä¸€ä»½é‡Œé¢'C': 1ï¼Œ'gamma': 0.1,æ•°æ®æ˜¯æ¥è¿‘çš„ï¼Œä½†æ˜¯ä¸æ˜¯ä¸€æ ·
best_params =svc_model_RS.named_steps['svc'].get_params() 

#d æœ€ä¼˜ CV å¾—åˆ†: -0.6300694759525192 ï¼Œåœ¨ä¸Šä¸€ä»½ç»ƒä¹  -0.6569474181788084.æ¯”ä¹‹å‰çš„å¿«ï¼Œè€Œä¸”å¾—åˆ†æ›´é«˜


'''
 9.3 From exercise 1,
 (a) Compute the Sharperatio of the resulting in-sample forecasts, from point 1.a
 (see Chapter 14 for a definition of Sharpe ratio).
 (b) Repeat point 1.a, this time with accuracy as the scoring function. Compute
 the in-sample forecasts derived from the hyper-tuned parameters.
 (c) What scoring method leads to higher (in-sample) Sharpe ratio?
'''

#è®¡ç®—æ ·æœ¬å†…çš„å¤æ™®  è¿™ä¸æ˜¯æ ‡å‡†å¤æ™®ï¼Œåªæ˜¯ç±»ä¼¼å¤æ™®çš„ä¸€ä¸ªå‡å€¼/æ–¹å·®å¾—åˆ†ç¨³å®šæ€§ å‚è€ƒæ€§ä¸é«˜
def in_sample_sharpe_ratio(clf):
    sharpe_ratio = []
    for i in np.arange(len(clf.cv_results_['mean_test_score'])):
        if clf.cv_results_['mean_test_score'][i] < 0:
            sharpe_ratio.append(-1 * clf.cv_results_['mean_test_score'][i]/ clf.cv_results_['std_test_score'][i])
        else:
            sharpe_ratio.append(clf.cv_results_['mean_test_score'][i]/ clf.cv_results_['std_test_score'][i])
    print("IS Best Score Sharpe Ratio: {0:.6f}".format(sharpe_ratio[clf.best_index_]))
    print("Best IS Sharpe ratio: {0:.6f}\nLowest IS Sharpe Ratio: {1:.6f}\nMean Sharpe Ratio: {2:.6f}".format(max(sharpe_ratio), min(sharpe_ratio), np.mean(sharpe_ratio)))

in_sample_sharpe_ratio(svc_GS_gs)
#IS Best Score Sharpe Ratio: 8.743683
# Best IS Sharpe ratio: 24.950627
# Lowest IS Sharpe Ratio: 1.285766
# Mean Sharpe Ratio: 13.120860


#b accuracy 
in_sample_sharpe_ratio(svc_GS_gs)
# IS Best Score Sharpe Ratio: 4.153074
# Best IS Sharpe ratio: 4.153074
# Lowest IS Sharpe Ratio: 0.881693
# Mean Sharpe Ratio: 1.033248

#c neg_log_losså¾—åˆ°çš„ç»“æœæ›´å¥½ï¼Œæ›´ç¨³å®šã€‚è¿™æ˜¯ä¸ºå•¥å‘¢ï¼Ÿ

'''
 9.4 From exercise 2,
 (a) Compute the Sharpe ratio of the resulting in-sample forecasts, from point
 2.a.
 (b) Repeat point 2.a, this time with accuracy as the scoring function. Compute
 the in-sample forecasts derived from the hyper-tuned parameters.
 (c) What scoring method leads to higher (in-sample) Sharpe ratio?
 '''

# a neg_log_loss
in_sample_sharpe_ratio(svc_RS_gs)
# IS Best Score Sharpe Ratio: 6.121556
# Best IS Sharpe ratio: 19.323110
# Lowest IS Sharpe Ratio: 1.344132
# Mean Sharpe Ratio: 11.597977

#b accuracy 
in_sample_sharpe_ratio(svc_RS_gs)
# IS Best Score Sharpe Ratio: 0.881693
# Best IS Sharpe ratio: 0.881693
# Lowest IS Sharpe Ratio: 0.881693
# Mean Sharpe Ratio: 0.881693

#c neg_log_losså¾—åˆ°çš„ç»“æœæ›´å¥½ï¼Œæ›´ç¨³å®šã€‚






'''
 9.5 Read the definition of log loss, L[Y,P].
 (a) Why is the scoring function neg_log_loss defined as the negative log loss,
 âˆ’L[Y,P]?
 (b) What would be the outcome of maximizing the log loss, rather than the neg
ative log loss?
'''
#(a) å› ä¸ºsklearnçš„è¶…å‚ä¼˜åŒ–å‡½æ•°éƒ½æ˜¯æœ€å¤§åŒ–è¯„åˆ†å‡½æ•°ï¼Œæ‰€ä»¥ä¸ºäº†æœ€å°åŒ–log_lossï¼Œéœ€è¦å–è´Ÿå€¼ï¼Œä½¿å¾—æœ€å°åŒ–log_lossç­‰ä»·äºæœ€å¤§åŒ–-neg_log_lossã€‚

# æœ€å¤§åŒ–negative log loss æ”¹ä¸ºæœ€å¤§åŒ–log lossä¼šå¯¼è‡´æ¨¡å‹æ•…æ„å­¦å


'''
 9.6 Consider an investment strategy that sizes its bets equally, regardless of the fore
castâ€™s confidence. In this case, what is a more appropriate scoring function for
 hyper-parameter tuning, accuracy or cross-entropy loss?
'''


#å‡†ç¡®ç‡æ¯”è¾ƒåˆé€‚ï¼Œä¸ç®¡ç½®ä¿¡åº¦é«˜ä½ï¼Œåæ­£æ¯æ¬¡ä¸‹æ³¨éƒ½ä¸€æ ·ã€‚åªè€ƒè™‘è¿™æ¬¡çš„ä¿¡æ¯æ˜¯å¦æ­£ç¡®ã€‚F1-score ä¹Ÿæ˜¯è€ƒè™‘çš„ä¸€ä¸ªæ–¹å‘ï¼Œå‘å‡ºçš„ä¿¡å·å¯èƒ½æ˜¯ä¸å¹³è¡¡çš„ï¼Œæ‰€ä»¥F1-score æ›´å¥½ã€‚
#è¿›ä¸€æ­¥ï¼Œå¯¹äºè¿™æ ·çš„ç­‰é¢ä¸‹æ³¨ï¼Œå¯ä»¥è‡ªå®šä¹‰ scorerï¼šscore = mean(returns[y_pred == y_true] - returns[y_pred != y_true])  å…³æ³¨é•¿æœŸæœŸæœ›æ”¶ç›Š




'''
ç¬¬ä¹ç« æ€»ç»“ï¼š
1.ä»‹ç»äº†ä¼˜åŒ–åçš„è¶…å‚ä¼˜åŒ–å‡½æ•°ï¼Œèƒ½å¤Ÿé€‚é…ä¸Šé¢æåˆ°çš„æ¸…é™¤å’Œç¦æ­¢ã€‚æå‡ºä½¿ç”¨éšæœºè¶…å‚ï¼Œæˆ‘è§‰å¾—æ¯”ç½‘æ ¼æœç´¢å¥½ã€‚
2.æœ¬ç« æ ¸å¿ƒã€‚ä½¿ç”¨meta-labelingæ—¶ä½¿ç”¨f1è¯„åˆ†è¿›è¡Œä¼°è®¡ï¼Œè€Œå…¶ä»–æƒ…å†µè¦ä½¿ç”¨neg_log_lossè¿›è¡Œè¯„åˆ†ã€‚ä½¿ç”¨accuracyè¿›è¡Œè¯„åˆ†ä¼šå®¹æ˜“å¯¼è‡´é¢„æµ‹é”™è¯¯è€ŒäºæŸã€‚å› ä¸ºæŠ•èµ„çš„æ”¶ç›Šæ¥æºäºå¯¹é«˜ç½®ä¿¡çš„æ­£ç¡®é¢„æµ‹ï¼Œè€Œå‡†ç¡®ç‡å¯¹äºé«˜ç½®ä¿¡åº¦ä¸ä½ç½®ä¿¡åº¦çš„é¢„æµ‹æ˜¯æ²¡æœ‰åŒºåˆ†çš„ï¼Œéƒ½æ˜¯1-0ï¼Œè¿™æ ·ã€‚è€Œä½¿ç”¨ï¼ï¼ï¼ï¼ï¼
3.æ ¹æ®ç¬¬äºŒç‚¹çš„è¡ç”Ÿï¼šä»“ä½ï¼Œé£é™©åº¦ä¾èµ–äºç½®ä¿¡åº¦ã€‚æ¯”å¦‚é¢„æµ‹ä¸Šæ¶¨æ¦‚ç‡ä¸º0.6æ—¶ä»“ä½ä¸º1Wï¼Œæ¦‚ç‡ä¸º0.8æ—¶ä»“ä½ä¸º5Wï¼Œè¿™æ ·çš„ä¸€ä¸ªåŸºäºç½®ä¿¡åº¦çš„åŠ¨æ€è°ƒæ•´ã€‚
4.å¯¹äºmeta-labelingæ—¶å¯ä»¥ä½¿ç”¨f1æˆ–è€…accuracyè¿›è¡Œè¯„åˆ†,meta-labelingæ ‡ç­¾å·²ä»£è¡¨â€œç»æµç»“æœâ€,æ˜¯ç›´æ¥å†³ç­–åšè¿˜æ˜¯ä¸åšï¼Œæ— éœ€æ¦‚ç‡æ ¡å‡†ï¼Œè€Œä¸”ä¸æ˜¯ä½¿ç”¨åœ¨ä¸»æ¨¡å‹ä¸Šçš„ã€‚æ‰€ä»¥ä»…æœ‰è¿™ä¸ªä¾‹å¤–ã€‚
'''



#%%

#ç¬¬åç«  ä¸‹æ³¨å¤§å°  æ ¹æ®æœºå™¨å­¦ä¹ ç»“æœè°ƒæ•´ä¸‹æ³¨çš„å¤§å°








'''
ç¬¬åç« æ€»ç»“ï¼š
1.

'''


